

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>OpenCV-Python Tutorials 1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  

  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  
        <link rel="index" title="Index"
              href="genindex.html"/>
        <link rel="search" title="Search" href="search.html"/>
    <link rel="top" title="None" href="index.html#document-index"/> 

  
  <script src="_static/js/modernizr.min.js"></script>


<!-- RTD Extra Head -->

<!-- 
Always link to the latest version, as canonical.
http://docs.readthedocs.org/en/latest/canonical.html
-->
<link rel="canonical" href="http://opencv-python-tutroals.readthedocs.io/en/latest/" />

<link rel="stylesheet" href="https://media.readthedocs.org/css/readthedocs-doc-embed.css" type="text/css" />

<script type="text/javascript" src="_static/readthedocs-data.js"></script>

<!-- Add page-specific data, which must exist in the page js, not global -->
<script type="text/javascript">
READTHEDOCS_DATA['page'] = 'index'
</script>

<script type="text/javascript" src="_static/readthedocs-dynamic-include.js"></script>

<!-- end RTD <extrahead> --></head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html#document-index" class="icon icon-home"> OpenCV-Python Tutorials
          

          
            
            <img src="_static/opencv-logo-white.png" class="logo" />
          
          </a>

          
            
            
            
              <div class="version">
                latest
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
            
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-py_tutorials/py_tutorials">OpenCV-Python Tutorials</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-py_tutorials/py_setup/py_table_of_contents_setup/py_table_of_contents_setup">Introduction to OpenCV</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-py_tutorials/py_gui/py_table_of_contents_gui/py_table_of_contents_gui">Gui Features in OpenCV</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-py_tutorials/py_core/py_table_of_contents_core/py_table_of_contents_core">Core Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-py_tutorials/py_imgproc/py_table_of_contents_imgproc/py_table_of_contents_imgproc">Image Processing in OpenCV</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-py_tutorials/py_feature2d/py_table_of_contents_feature2d/py_table_of_contents_feature2d">Feature Detection and Description</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-py_tutorials/py_video/py_table_of_contents_video/py_table_of_contents_video">Video Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-py_tutorials/py_calib3d/py_table_of_contents_calib3d/py_table_of_contents_calib3d">Camera Calibration and 3D Reconstruction</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-py_tutorials/py_ml/py_table_of_contents_ml/py_table_of_contents_ml">Machine Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-py_tutorials/py_photo/py_table_of_contents_photo/py_table_of_contents_photo">Computational Photography</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-py_tutorials/py_objdetect/py_table_of_contents_objdetect/py_table_of_contents_objdetect">Object Detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-py_tutorials/py_bindings/py_table_of_contents_bindings/py_table_of_contents_bindings">OpenCV-Python Bindings</a></li>
</ul>
</li>
</ul>
</div>
            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html#document-index">OpenCV-Python Tutorials</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html#document-index">Docs</a> &raquo;</li>
        
      <li>OpenCV-Python Tutorials 1 documentation</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/abidrahmank/OpenCV2-Python-Tutorials/blob/master/source/index.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="welcome-to-opencv-python-tutorials-s-documentation">
<h1>Welcome to OpenCV-Python Tutorials&#8217;s documentation!<a class="headerlink" href="#welcome-to-opencv-python-tutorials-s-documentation" title="Permalink to this headline">¶</a></h1>
<p>Contents:</p>
<div class="toctree-wrapper compound">
<span id="document-py_tutorials/py_tutorials"></span><div class="section" id="opencv-python-tutorials">
<h2>OpenCV-Python Tutorials<a class="headerlink" href="#opencv-python-tutorials" title="Permalink to this headline">¶</a></h2>
<ul>
<li><p class="first"><a class="reference internal" href="index.html#py-table-of-content-setup"><span class="std std-ref">Introduction to OpenCV</span></a></p>
<table border="1" class="toctableopencv docutils">
<colgroup>
<col width="17%" />
<col width="83%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/intro.png"><img alt="Introduction Icon" src="_images/intro.png" style="width: 80pt; height: 80pt;" /></a></td>
<td>Learn how to setup OpenCV-Python on your computer!</td>
</tr>
</tbody>
</table>
</li>
<li><p class="first"><a class="reference internal" href="index.html#py-table-of-content-gui"><span class="std std-ref">Gui Features in OpenCV</span></a></p>
<table border="1" class="toctableopencv docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/gui.jpg"><img alt="gui Icon" src="_images/gui.jpg" style="width: 80pt; height: 80pt;" /></a></td>
<td>Here you will learn how to display and save images and videos, control mouse events and create trackbar.</td>
</tr>
</tbody>
</table>
</li>
<li><p class="first"><a class="reference internal" href="index.html#py-table-of-content-core"><span class="std std-ref">Core Operations</span></a></p>
<table border="1" class="toctableopencv docutils">
<colgroup>
<col width="7%" />
<col width="93%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/core.jpg"><img alt="core Icon" src="_images/core.jpg" style="width: 80pt; height: 80pt;" /></a></td>
<td>In this section you will learn basic operations on image like pixel editing, geometric transformations, code optimization, some mathematical tools etc.</td>
</tr>
</tbody>
</table>
</li>
<li><p class="first"><a class="reference internal" href="index.html#py-table-of-content-imgproc"><span class="std std-ref">Image Processing in OpenCV</span></a></p>
<table border="1" class="toctableopencv docutils">
<colgroup>
<col width="12%" />
<col width="88%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/imgproc.jpg"><img alt="imgproc Icon" src="_images/imgproc.jpg" style="width: 80pt; height: 80pt;" /></a></td>
<td>In this section you will learn different image processing functions inside OpenCV.</td>
</tr>
</tbody>
</table>
</li>
<li><p class="first"><a class="reference internal" href="index.html#py-table-of-content-feature2d"><span class="std std-ref">Feature Detection and Description</span></a></p>
<table border="1" class="toctableopencv docutils">
<colgroup>
<col width="14%" />
<col width="86%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/featureicon.jpg"><img alt="imgproc Icon" src="_images/featureicon.jpg" style="width: 80pt; height: 80pt;" /></a></td>
<td>In this section you will learn about feature detectors and descriptors</td>
</tr>
</tbody>
</table>
</li>
<li><p class="first"><a class="reference internal" href="index.html#py-table-of-content-video"><span class="std std-ref">Video Analysis</span></a></p>
<table border="1" class="toctableopencv docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/videoicon.jpg"><img alt="imgproc Icon" src="_images/videoicon.jpg" style="width: 80pt; height: 80pt;" /></a></td>
<td>In this section you will learn different techniques to work with videos like object tracking etc.</td>
</tr>
</tbody>
</table>
</li>
<li><p class="first"><a class="reference internal" href="index.html#py-table-of-content-calib"><span class="std std-ref">Camera Calibration and 3D Reconstruction</span></a></p>
<table border="1" class="toctableopencv docutils">
<colgroup>
<col width="13%" />
<col width="87%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/calib3d_icon.jpg"><img alt="Calib Icon" src="_images/calib3d_icon.jpg" style="width: 80pt; height: 80pt;" /></a></td>
<td>In this section we will learn about camera calibration, stereo imaging etc.</td>
</tr>
</tbody>
</table>
</li>
<li><p class="first"><a class="reference internal" href="index.html#py-table-of-content-ml"><span class="std std-ref">Machine Learning</span></a></p>
<table border="1" class="toctableopencv docutils">
<colgroup>
<col width="12%" />
<col width="88%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/MachineLearnings.jpg"><img alt="ML Icon" src="_images/MachineLearnings.jpg" style="width: 80pt; height: 80pt;" /></a></td>
<td>In this section you will learn different image processing functions inside OpenCV.</td>
</tr>
</tbody>
</table>
</li>
<li><p class="first"><a class="reference internal" href="index.html#py-table-of-content-photo"><span class="std std-ref">Computational Photography</span></a></p>
<table border="1" class="toctableopencv docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/photoicon.jpg"><img alt="ML Icon" src="_images/photoicon.jpg" style="width: 80pt; height: 80pt;" /></a></td>
<td>In this section you will learn different computational photography techniques like image denoising etc.</td>
</tr>
</tbody>
</table>
</li>
<li><p class="first"><a class="reference internal" href="index.html#py-table-of-content-objdetection"><span class="std std-ref">Object Detection</span></a></p>
<table border="1" class="toctableopencv docutils">
<colgroup>
<col width="13%" />
<col width="88%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/obj_icon.jpg"><img alt="OD Icon" src="_images/obj_icon.jpg" style="width: 80pt; height: 80pt;" /></a></td>
<td>In this section you will object detection techniques like face detection etc.</td>
</tr>
</tbody>
</table>
</li>
<li><p class="first"><a class="reference internal" href="index.html#py-table-of-content-bindings"><span class="std std-ref">OpenCV-Python Bindings</span></a></p>
<table border="1" class="toctableopencv docutils">
<colgroup>
<col width="14%" />
<col width="86%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/obj_icon.jpg"><img alt="OD Icon" src="_images/obj_icon.jpg" style="width: 80pt; height: 80pt;" /></a></td>
<td>In this section, we will see how OpenCV-Python bindings are generated</td>
</tr>
</tbody>
</table>
</li>
</ul>
<div class="toctree-wrapper compound">
<span id="document-py_tutorials/py_setup/py_table_of_contents_setup/py_table_of_contents_setup"></span><div class="section" id="introduction-to-opencv">
<span id="py-table-of-content-setup"></span><h3>Introduction to OpenCV<a class="headerlink" href="#introduction-to-opencv" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference internal" href="index.html#intro"><span class="std std-ref">Introduction to OpenCV-Python Tutorials</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="17%" />
<col width="83%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/opencv_logo.jpg"><img alt="Intro_1" src="_images/opencv_logo.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Getting Started with OpenCV-Python</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#install-opencv-python-in-windows"><span class="std std-ref">Install OpenCV-Python in Windows</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="17%" />
<col width="83%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/windows_logo.jpg"><img alt="Install_1" src="_images/windows_logo.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Set Up OpenCV-Python in Windows</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#install-opencv-python-in-fedora"><span class="std std-ref">Install OpenCV-Python in Fedora</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="17%" />
<col width="83%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/fedora_logo.jpg"><img alt="Install_2" src="_images/fedora_logo.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Set Up OpenCV-Python in Fedora</td>
</tr>
</tbody>
</table>
</div></blockquote>
<div class="toctree-wrapper compound">
<span id="document-py_tutorials/py_setup/py_intro/py_intro"></span><div class="section" id="introduction-to-opencv-python-tutorials">
<span id="intro"></span><h4>Introduction to OpenCV-Python Tutorials<a class="headerlink" href="#introduction-to-opencv-python-tutorials" title="Permalink to this headline">¶</a></h4>
<div class="section" id="opencv">
<h5>OpenCV<a class="headerlink" href="#opencv" title="Permalink to this headline">¶</a></h5>
<p>OpenCV was started at Intel in 1999 by <strong>Gary Bradsky</strong> and the first release came out in 2000. <strong>Vadim Pisarevsky</strong> joined Gary Bradsky to manage Intel&#8217;s Russian software OpenCV team. In 2005, OpenCV was used on Stanley, the vehicle who won 2005 DARPA Grand Challenge. Later its active development continued under the support of Willow Garage, with Gary Bradsky and Vadim Pisarevsky leading the project. Right now, OpenCV supports a lot of algorithms related to Computer Vision and Machine Learning and it is expanding day-by-day.</p>
<p>Currently OpenCV supports a wide variety of programming languages like C++, Python, Java etc and is available on different platforms including Windows, Linux, OS X, Android, iOS etc. Also, interfaces based on CUDA and OpenCL are also under active development for high-speed GPU operations.</p>
<p>OpenCV-Python is the Python API of OpenCV. It combines the best qualities of OpenCV C++ API and Python language.</p>
</div>
<div class="section" id="opencv-python">
<h5>OpenCV-Python<a class="headerlink" href="#opencv-python" title="Permalink to this headline">¶</a></h5>
<p>Python is a general purpose programming language started by <strong>Guido van Rossum</strong>, which became very popular in short time mainly because of its simplicity and code readability. It enables the programmer to express his ideas in fewer lines of code without reducing any readability.</p>
<p>Compared to other languages like C/C++, Python is slower. But another important feature of Python is that it can be easily extended with C/C++. This feature helps us to write computationally intensive codes in C/C++ and create a Python wrapper for it so that we can use these wrappers as Python modules. This gives us two advantages: first, our code is as fast as original C/C++ code (since it is the actual C++ code working in background) and second, it is very easy to code in Python. This is how OpenCV-Python works, it is a Python wrapper around original C++ implementation.</p>
<p>And the support of Numpy makes the task more easier. <strong>Numpy</strong> is a highly optimized library for numerical operations. It gives a MATLAB-style syntax. All the OpenCV array structures are converted to-and-from Numpy arrays. So whatever operations you can do in Numpy, you can combine it with OpenCV, which increases number of weapons in your arsenal. Besides that, several other libraries like SciPy, Matplotlib which supports Numpy can be used with this.</p>
<p>So OpenCV-Python is an appropriate tool for fast prototyping of computer vision problems.</p>
</div>
<div class="section" id="opencv-python-tutorials">
<h5>OpenCV-Python Tutorials<a class="headerlink" href="#opencv-python-tutorials" title="Permalink to this headline">¶</a></h5>
<p>OpenCV introduces a new set of tutorials which will guide you through various functions available in OpenCV-Python. <strong>This guide is mainly focused on OpenCV 3.x version</strong> (although most of the tutorials will work with OpenCV 2.x also).</p>
<p>A prior knowledge on Python and Numpy is required before starting because they won&#8217;t be covered in this guide. <strong>Especially, a good knowledge on Numpy is must to write optimized codes in OpenCV-Python.</strong></p>
<p>This tutorial has been started by <em>Abid Rahman K.</em> as part of Google Summer of Code 2013 program, under the guidance of <em>Alexander Mordvintsev</em>.</p>
</div>
<div class="section" id="opencv-needs-you">
<h5>OpenCV Needs You !!!<a class="headerlink" href="#opencv-needs-you" title="Permalink to this headline">¶</a></h5>
<p>Since OpenCV is an open source initiative, all are welcome to make contributions to this library. And it is same for this tutorial also.</p>
<p>So, if you find any mistake in this tutorial (whether it be a small spelling mistake or a big error in code or concepts, whatever), feel free to correct it.</p>
<p>And that will be a good task for freshers who begin to contribute to open source projects. Just fork the OpenCV in github, make necessary corrections and send a pull request to OpenCV. OpenCV developers will check your pull request, give you important feedback and once it passes the approval of the reviewer, it will be merged to OpenCV. Then you become a open source contributor. Similar is the case with other tutorials, documentation etc.</p>
<p>As new modules are added to OpenCV-Python, this tutorial will have to be expanded. So those who knows about particular algorithm can write up a tutorial which includes a basic theory of the algorithm and a code showing basic usage of the algorithm and submit it to OpenCV.</p>
<p>Remember, we <strong>together</strong> can make this project a great success !!!</p>
</div>
<div class="section" id="contributors">
<h5>Contributors<a class="headerlink" href="#contributors" title="Permalink to this headline">¶</a></h5>
<p>Below is the list of contributors who submitted tutorials to OpenCV-Python.</p>
<ol class="arabic simple">
<li>Alexander Mordvintsev (GSoC-2013 mentor)</li>
<li>Abid Rahman K. (GSoC-2013 intern)</li>
</ol>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
<ol class="arabic simple">
<li>A Quick guide to Python - <a class="reference external" href="http://swaroopch.com/notes/python/">A Byte of Python</a></li>
<li><a class="reference external" href="http://wiki.scipy.org/Tentative_NumPy_Tutorial">Basic Numpy Tutorials</a></li>
<li><a class="reference external" href="http://wiki.scipy.org/Numpy_Example_List">Numpy Examples List</a></li>
<li><a class="reference external" href="http://docs.opencv.org/">OpenCV Documentation</a></li>
<li><a class="reference external" href="http://answers.opencv.org/questions/">OpenCV Forum</a></li>
</ol>
</div>
</div>
<span id="document-py_tutorials/py_setup/py_setup_in_windows/py_setup_in_windows"></span><div class="section" id="install-opencv-python-in-windows">
<span id="id1"></span><h4>Install OpenCV-Python in Windows<a class="headerlink" href="#install-opencv-python-in-windows" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goals">
<h5>Goals<a class="headerlink" href="#goals" title="Permalink to this headline">¶</a></h5>
<dl class="docutils">
<dt>In this tutorial</dt>
<dd><ul class="first last simple">
<li>We will learn to setup OpenCV-Python in your Windows system.</li>
</ul>
</dd>
</dl>
<p><em>Below steps are tested in a Windows 7-64 bit machine with Visual Studio 2010 and Visual Studio 2012. The screenshots shows VS2012.</em></p>
</div>
<div class="section" id="installing-opencv-from-prebuilt-binaries">
<h5>Installing OpenCV from prebuilt binaries<a class="headerlink" href="#installing-opencv-from-prebuilt-binaries" title="Permalink to this headline">¶</a></h5>
<ol class="arabic">
<li><p class="first">Below Python packages are to be downloaded and installed to their default locations.</p>
<blockquote>
<div><p>1.1. <a class="reference external" href="http://python.org/ftp/python/2.7.5/python-2.7.5.msi">Python-2.7.x</a>.</p>
<p>1.2. <a class="reference external" href="http://sourceforge.net/projects/numpy/files/NumPy/1.7.1/numpy-1.7.1-win32-superpack-python2.7.exe/download">Numpy</a>.</p>
<p>1.3. <a class="reference external" href="https://downloads.sourceforge.net/project/matplotlib/matplotlib/matplotlib-1.3.0/matplotlib-1.3.0.win32-py2.7.exe">Matplotlib</a> (<em>Matplotlib is optional, but recommended since we use it a lot in our tutorials</em>).</p>
</div></blockquote>
</li>
<li><p class="first">Install all packages into their default locations. Python will be installed to <strong>C:/Python27/</strong>.</p>
</li>
<li><p class="first">After installation, open Python IDLE. Enter <code class="docutils literal"><span class="pre">import</span> <span class="pre">numpy</span></code> and make sure Numpy is working fine.</p>
</li>
<li><p class="first">Download latest OpenCV release from <a class="reference external" href="http://sourceforge.net/projects/opencvlibrary/files/opencv-win/2.4.6/OpenCV-2.4.6.0.exe/download">sourceforge site</a> and double-click to extract it.</p>
</li>
</ol>
<ol class="arabic" start="7">
<li><p class="first">Goto <strong>opencv/build/python/2.7</strong> folder.</p>
</li>
<li><p class="first">Copy <strong>cv2.pyd</strong> to <strong>C:/Python27/lib/site-packeges</strong>.</p>
</li>
<li><p class="first">Open Python IDLE and type following codes in Python terminal.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span> <span class="n">cv2</span><span class="o">.</span><span class="n">__version__</span>
</pre></div>
</div>
</li>
</ol>
<p>If the results are printed out without any errors, congratulations !!! You have installed OpenCV-Python successfully.</p>
</div>
<div class="section" id="building-opencv-from-source">
<h5>Building OpenCV from source<a class="headerlink" href="#building-opencv-from-source" title="Permalink to this headline">¶</a></h5>
<ol class="arabic">
<li><p class="first">Download and install Visual Studio and CMake.</p>
<blockquote>
<div><p>1.1. <a class="reference external" href="http://go.microsoft.com/?linkid=9816768">Visual Studio 2012</a></p>
<p>1.2. <a class="reference external" href="http://www.cmake.org/files/v2.8/cmake-2.8.11.2-win32-x86.exe">CMake</a></p>
</div></blockquote>
</li>
<li><p class="first">Download and install necessary Python packages to their default locations</p>
<blockquote>
<div><p>2.1. <a class="reference external" href="http://python.org/ftp/python/2.7.5/python-2.7.5.msi">Python 2.7.x</a></p>
<p>2.2. <a class="reference external" href="http://sourceforge.net/projects/numpy/files/NumPy/1.7.1/numpy-1.7.1-win32-superpack-python2.7.exe/download">Numpy</a></p>
<p>2.3. <a class="reference external" href="https://downloads.sourceforge.net/project/matplotlib/matplotlib/matplotlib-1.3.0/matplotlib-1.3.0.win32-py2.7.exe">Matplotlib</a> (<em>Matplotlib is optional, but recommended since we use it a lot in our tutorials.</em>)</p>
</div></blockquote>
</li>
</ol>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">In this case, we are using 32-bit binaries of Python packages. But if you want to use OpenCV for x64, 64-bit binaries of Python packages are to be installed. Problem is that, there is no official 64-bit binaries of Numpy. You have to build it on your own. For that, you have to use the same compiler used to build Python. When you start Python IDLE, it shows the compiler details. You can get more <a class="reference external" href="http://stackoverflow.com/q/2676763/1134940">information here</a>. So your system must have the same Visual Studio version and build Numpy from source.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Another method to have 64-bit Python packages is to use ready-made Python distributions from third-parties like <a class="reference external" href="http://www.continuum.io/downloads">Anaconda</a>, <a class="reference external" href="https://www.enthought.com/downloads/">Enthought</a> etc. It will be bigger in size, but will have everything you need. Everything in a single shell. You can also download 32-bit versions also.</p>
</div>
<ol class="arabic" start="3">
<li><p class="first">Make sure Python and Numpy are working fine.</p>
</li>
<li><p class="first">Download OpenCV source. It can be from <a class="reference external" href="http://sourceforge.net/projects/opencvlibrary/">Sourceforge</a> (for official release version) or from <a class="reference external" href="https://github.com/Itseez/opencv">Github</a> (for latest source).</p>
</li>
<li><p class="first">Extract it to a folder, <code class="docutils literal"><span class="pre">opencv</span></code> and create a new folder <code class="docutils literal"><span class="pre">build</span></code> in it.</p>
</li>
<li><p class="first">Open CMake-gui (<em>Start &gt; All Programs &gt; CMake-gui</em>)</p>
</li>
<li><p class="first">Fill the fields as follows (see the image below):</p>
<blockquote>
<div><p>7.1. Click on <strong>Browse Source...</strong> and locate the <code class="docutils literal"><span class="pre">opencv</span></code> folder.</p>
<p>7.2. Click on <strong>Browse Build...</strong> and locate the <code class="docutils literal"><span class="pre">build</span></code> folder we created.</p>
<p>7.3. Click on <strong>Configure</strong>.</p>
<blockquote>
<div><img alt="capture1" class="align-center" src="_images/Capture1.jpg" />
</div></blockquote>
<p>7.4. It will open a new window to select the compiler. Choose appropriate compiler (here, Visual Studio 11) and click <strong>Finish</strong>.</p>
<blockquote>
<div><img alt="capture2" class="align-center" src="_images/Capture2.png" />
</div></blockquote>
<p>7.5. Wait until analysis is finished.</p>
</div></blockquote>
</li>
<li><p class="first">You will see all the fields are marked in red. Click on the <strong>WITH</strong> field to expand it. It decides what extra features you need. So mark appropriate fields. See the below image:</p>
<blockquote>
<div><img alt="capture3" class="align-center" src="_images/Capture3.png" />
</div></blockquote>
</li>
<li><p class="first">Now click on <strong>BUILD</strong> field to expand it. First few fields configure the build method. See the below image:</p>
<blockquote>
<div><img alt="capture5" class="align-center" src="_images/Capture5.png" />
</div></blockquote>
</li>
<li><p class="first">Remaining fields specify what modules are to be built. Since GPU modules are not yet supported by OpenCV-Python, you can completely avoid it to save time (But if you work with them, keep it there). See the image below:</p>
<img alt="capture6" class="align-center" src="_images/Capture6.png" />
</li>
<li><p class="first">Now click on <strong>ENABLE</strong> field to expand it. Make sure <strong>ENABLE_SOLUTION_FOLDERS</strong> is unchecked (Solution folders are not supported by Visual Studio Express edition). See the image below:</p>
<img alt="capture7" class="align-center" src="_images/Capture7.png" />
</li>
<li><p class="first">Also make sure that in the <strong>PYTHON</strong> field, everything is filled. (Ignore PYTHON_DEBUG_LIBRARY). See image below:</p>
<img alt="capture80" class="align-center" src="_images/Capture80.png" />
</li>
<li><p class="first">Finally click the <strong>Generate</strong> button.</p>
</li>
<li><p class="first">Now go to our <strong>opencv/build</strong> folder. There you will find <strong>OpenCV.sln</strong> file. Open it with Visual Studio.</p>
</li>
<li><p class="first">Check build mode as <strong>Release</strong> instead of <strong>Debug</strong>.</p>
</li>
<li><p class="first">In the solution explorer, right-click on the <strong>Solution</strong> (or <strong>ALL_BUILD</strong>) and build it. It will take some time to finish.</p>
</li>
<li><p class="first">Again, right-click on <strong>INSTALL</strong> and build it. Now OpenCV-Python will be installed.</p>
<img alt="capture8" class="align-center" src="_images/Capture8.png" />
</li>
<li><p class="first">Open Python IDLE and enter <code class="docutils literal"><span class="pre">import</span> <span class="pre">cv2</span></code>. If no error, it is installed correctly.</p>
</li>
</ol>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">We have installed with no other support like TBB, Eigen, Qt, Documentation etc. It would be difficult to explain it here. A more detailed video will be added soon or you can just hack around.</p>
</div>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
<ol class="arabic simple">
<li>If you have a windows machine, compile the OpenCV from source. Do all kinds of hacks. If you meet any problem, visit OpenCV forum and explain your problem.</li>
</ol>
</div>
</div>
<span id="document-py_tutorials/py_setup/py_setup_in_fedora/py_setup_in_fedora"></span><div class="section" id="install-opencv-python-in-fedora">
<span id="id1"></span><h4>Install OpenCV-Python in Fedora<a class="headerlink" href="#install-opencv-python-in-fedora" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goals">
<h5>Goals<a class="headerlink" href="#goals" title="Permalink to this headline">¶</a></h5>
<dl class="docutils">
<dt>In this tutorial</dt>
<dd><ul class="first last simple">
<li>We will learn to setup OpenCV-Python in your Fedora system. Below steps are tested for Fedora 18 (64-bit) and Fedora 19 (32-bit).</li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="introduction">
<h5>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h5>
<p>OpenCV-Python can be installed in Fedora in two ways, 1) Install from pre-built binaries available in fedora repositories,  2) Compile from the source. In this section, we will see both.</p>
<p>Another important thing is the additional libraries required. OpenCV-Python requires only <strong>Numpy</strong> (in addition to other dependencies, which we will see later). But in this tutorials, we also use <strong>Matplotlib</strong> for some easy and nice plotting purposes (which I feel much better compared to OpenCV). Matplotlib is optional, but highly recommended. Similarly we will also see <strong>IPython</strong>, an Interactive Python Terminal, which is also highly recommended.</p>
</div>
<div class="section" id="installing-opencv-python-from-pre-built-binaries">
<h5>Installing OpenCV-Python from Pre-built Binaries<a class="headerlink" href="#installing-opencv-python-from-pre-built-binaries" title="Permalink to this headline">¶</a></h5>
<p>Install all packages with following command in terminal as root.</p>
<blockquote>
<div><div class="highlight-bash"><div class="highlight"><pre><span></span>$ yum install numpy opencv*
</pre></div>
</div>
</div></blockquote>
<p>Open Python IDLE (or IPython) and type following codes in Python terminal.</p>
<blockquote>
<div><div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="n">cv2</span><span class="o">.</span><span class="n">__version__</span>
</pre></div>
</div>
</div></blockquote>
<p>If the results are printed out without any errors, congratulations !!! You have installed OpenCV-Python successfully.</p>
<p>It is quite easy. But there is a problem with this. Yum repositories may not contain the latest version of OpenCV always. For example, at the time of writing this tutorial, yum repository contains 2.4.5 while latest OpenCV version is 2.4.6. With respect to Python API, latest version will always contain much better support. Also, there may be chance of problems with camera support, video playback etc depending upon the drivers, ffmpeg, gstreamer packages present etc.</p>
<p>So my personnel preference is next method, i.e. compiling from source. Also at some point of time, if you want to contribute to OpenCV, you will need this.</p>
</div>
<div class="section" id="installing-opencv-from-source">
<h5>Installing OpenCV from source<a class="headerlink" href="#installing-opencv-from-source" title="Permalink to this headline">¶</a></h5>
<p>Compiling from source may seem a little complicated at first, but once you succeeded in it, there is nothing complicated.</p>
<p>First we will install some dependencies. Some are compulsory, some are optional. Optional dependencies, you can leave if you don&#8217;t want.</p>
<div class="section" id="compulsory-dependencies">
<h6>Compulsory Dependencies<a class="headerlink" href="#compulsory-dependencies" title="Permalink to this headline">¶</a></h6>
<p>We need <strong>CMake</strong> to configure the installation, <strong>GCC</strong> for compilation, <strong>Python-devel</strong> and <strong>Numpy</strong> for creating Python extensions etc.</p>
<blockquote>
<div><div class="highlight-bash"><div class="highlight"><pre><span></span>yum install cmake
yum install python-devel numpy
yum install gcc gcc-c++
</pre></div>
</div>
</div></blockquote>
<p>Next we need <strong>GTK</strong> support for GUI features, Camera support (libdc1394, libv4l), Media Support (ffmpeg, gstreamer) etc.</p>
<blockquote>
<div><div class="highlight-bash"><div class="highlight"><pre><span></span>yum install gtk2-devel
yum install libdc1394-devel
yum install libv4l-devel
yum install ffmpeg-devel
yum install gstreamer-plugins-base-devel
</pre></div>
</div>
</div></blockquote>
</div>
<div class="section" id="optional-dependencies">
<h6>Optional Dependencies<a class="headerlink" href="#optional-dependencies" title="Permalink to this headline">¶</a></h6>
<p>Above dependencies are sufficient to install OpenCV in your fedora machine. But depending upon your requirements, you may need some extra dependencies. A list of such optional dependencies are given below. You can either leave it or install it, your call :)</p>
<p>OpenCV comes with supporting files for image formats like PNG, JPEG, JPEG2000, TIFF, WebP etc. But it may be a little old. If you want to get latest libraries, you can install development files for these formats.</p>
<blockquote>
<div><div class="highlight-bash"><div class="highlight"><pre><span></span>yum install libpng-devel
yum install libjpeg-turbo-devel
yum install jasper-devel
yum install openexr-devel
yum install libtiff-devel
yum install libwebp-devel
</pre></div>
</div>
</div></blockquote>
<p>Several OpenCV functions are parallelized with <strong>Intel&#8217;s Threading Building Blocks</strong> (TBB). But if you want to enable it, you need to install TBB first. ( Also while configuring installation with CMake, don&#8217;t forget to pass <code class="docutils literal"><span class="pre">-D</span> <span class="pre">WITH_TBB=ON</span></code>. More details below.)</p>
<blockquote>
<div><div class="highlight-bash"><div class="highlight"><pre><span></span>yum install tbb-devel
</pre></div>
</div>
</div></blockquote>
<p>OpenCV uses another library <strong>Eigen</strong> for optimized mathematical operations. So if you have Eigen installed in your system, you can exploit it. ( Also while configuring installation with CMake, don&#8217;t forget to pass <code class="docutils literal"><span class="pre">-D</span> <span class="pre">WITH_EIGEN=ON</span></code>. More details below.)</p>
<blockquote>
<div><div class="highlight-bash"><div class="highlight"><pre><span></span>yum install eigen3-devel
</pre></div>
</div>
</div></blockquote>
<p>If you want to build <strong>documentation</strong> ( <em>Yes, you can create offline version of OpenCV&#8217;s complete official documentation in your system in HTML with full search facility so that you need not access internet always if any question, and it is quite FAST!!!</em> ), you need to install <strong>Sphinx</strong> (a documentation generation tool) and <strong>pdflatex</strong> (if you want to create a PDF version of it). ( Also while configuring installation with CMake, don&#8217;t forget to pass <code class="docutils literal"><span class="pre">-D</span> <span class="pre">BUILD_DOCS=ON</span></code>. More details below.)</p>
<blockquote>
<div><div class="highlight-bash"><div class="highlight"><pre><span></span>yum install python-sphinx
yum install texlive
</pre></div>
</div>
</div></blockquote>
</div>
<div class="section" id="downloading-opencv">
<h6>Downloading OpenCV<a class="headerlink" href="#downloading-opencv" title="Permalink to this headline">¶</a></h6>
<p>Next we have to download OpenCV. You can download the latest release of OpenCV from <a class="reference external" href="http://sourceforge.net/projects/opencvlibrary/">sourceforge site</a>. Then extract the folder.</p>
<p>Or you can download latest source from OpenCV&#8217;s github repo. (If you want to contribute to OpenCV, choose this. It always keeps your OpenCV up-to-date). For that, you need to install <strong>Git</strong> first.</p>
<blockquote>
<div><div class="highlight-bash"><div class="highlight"><pre><span></span>yum install git
git clone https://github.com/Itseez/opencv.git
</pre></div>
</div>
</div></blockquote>
<p>It will create a folder <code class="docutils literal"><span class="pre">OpenCV</span></code> in home directory (or the directory you specify). The cloning may take some time depending upon your internet connection.</p>
<p>Now open a terminal window and navigate to the downloaded OpenCV folder. Create a new <code class="docutils literal"><span class="pre">build</span></code> folder and navigate to it.</p>
<blockquote>
<div><div class="highlight-bash"><div class="highlight"><pre><span></span>mkdir build
<span class="nb">cd</span> build
</pre></div>
</div>
</div></blockquote>
</div>
<div class="section" id="configuring-and-installing">
<h6>Configuring and Installing<a class="headerlink" href="#configuring-and-installing" title="Permalink to this headline">¶</a></h6>
<p>Now we have installed all the required dependencies, let&#8217;s install OpenCV. Installation has to be configured with CMake. It specifies which modules are to be installed, installation path, which additional libraries to be used, whether documentation and examples to be compiled etc. Below command is normally used for configuration (executed from <code class="docutils literal"><span class="pre">build</span></code> folder).</p>
<blockquote>
<div><div class="highlight-bash"><div class="highlight"><pre><span></span>cmake -D <span class="nv">CMAKE_BUILD_TYPE</span><span class="o">=</span>RELEASE -D <span class="nv">CMAKE_INSTALL_PREFIX</span><span class="o">=</span>/usr/local ..
</pre></div>
</div>
</div></blockquote>
<p>It specifies that build type is &#8220;Release Mode&#8221; and installation path is <code class="docutils literal"><span class="pre">/usr/local</span></code>. Observe the <code class="docutils literal"><span class="pre">-D</span></code> before each option and <code class="docutils literal"><span class="pre">..</span></code> at the end. In short, this is the format:</p>
<blockquote>
<div><div class="highlight-bash"><div class="highlight"><pre><span></span>cmake <span class="o">[</span>-D &lt;flag&gt;<span class="o">]</span> <span class="o">[</span>-D &lt;flag&gt;<span class="o">]</span> ..
</pre></div>
</div>
</div></blockquote>
<p>You can specify as many flags you want, but each flag should be preceded by <code class="docutils literal"><span class="pre">-D</span></code>.</p>
<p>So in this tutorial, we are installing OpenCV with TBB and Eigen support. We also build the documentation, but we exclude Performance tests and building samples. We also disable GPU related modules (since we use OpenCV-Python, we don&#8217;t need GPU related modules. It saves us some time).</p>
<p><em>(All the below commands can be done in a single cmake statement, but it is split here for better understanding.)</em></p>
<ul>
<li><p class="first">Enable TBB and Eigen support:</p>
<blockquote>
<div><div class="highlight-bash"><div class="highlight"><pre><span></span>cmake -D <span class="nv">WITH_TBB</span><span class="o">=</span>ON -D <span class="nv">WITH_EIGEN</span><span class="o">=</span>ON ..
</pre></div>
</div>
</div></blockquote>
</li>
<li><p class="first">Enable documentation and disable tests and samples</p>
<blockquote>
<div><div class="highlight-bash"><div class="highlight"><pre><span></span>cmake -D <span class="nv">BUILD_DOCS</span><span class="o">=</span>ON -D <span class="nv">BUILD_TESTS</span><span class="o">=</span>OFF -D <span class="nv">BUILD_PERF_TESTS</span><span class="o">=</span>OFF -D <span class="nv">BUILD_EXAMPLES</span><span class="o">=</span>OFF ..
</pre></div>
</div>
</div></blockquote>
</li>
<li><p class="first">Disable all GPU related modules.</p>
<blockquote>
<div><div class="highlight-bash"><div class="highlight"><pre><span></span>cmake -D <span class="nv">WITH_OPENCL</span><span class="o">=</span>OFF -D <span class="nv">WITH_CUDA</span><span class="o">=</span>OFF -D <span class="nv">BUILD_opencv_gpu</span><span class="o">=</span>OFF -D <span class="nv">BUILD_opencv_gpuarithm</span><span class="o">=</span>OFF -D <span class="nv">BUILD_opencv_gpubgsegm</span><span class="o">=</span>OFF -D <span class="nv">BUILD_opencv_gpucodec</span><span class="o">=</span>OFF -D <span class="nv">BUILD_opencv_gpufeatures2d</span><span class="o">=</span>OFF -D <span class="nv">BUILD_opencv_gpufilters</span><span class="o">=</span>OFF -D <span class="nv">BUILD_opencv_gpuimgproc</span><span class="o">=</span>OFF -D <span class="nv">BUILD_opencv_gpulegacy</span><span class="o">=</span>OFF -D <span class="nv">BUILD_opencv_gpuoptflow</span><span class="o">=</span>OFF -D <span class="nv">BUILD_opencv_gpustereo</span><span class="o">=</span>OFF -D <span class="nv">BUILD_opencv_gpuwarping</span><span class="o">=</span>OFF ..
</pre></div>
</div>
</div></blockquote>
</li>
<li><p class="first">Set installation path and build type</p>
<blockquote>
<div><div class="highlight-bash"><div class="highlight"><pre><span></span>cmake -D <span class="nv">CMAKE_BUILD_TYPE</span><span class="o">=</span>RELEASE -D <span class="nv">CMAKE_INSTALL_PREFIX</span><span class="o">=</span>/usr/local ..
</pre></div>
</div>
</div></blockquote>
</li>
</ul>
<p>Each time you enter cmake statement, it prints out the resulting configuration setup. In the final setup you got, make sure that following fields are filled (below is the some important parts of configuration I got). These fields should be filled appropriately in your system also. Otherwise some problem has happened. So check if you have correctly performed above steps.</p>
<blockquote>
<div><div class="highlight-bash"><div class="highlight"><pre><span></span>--   GUI:
--     GTK+ <span class="m">2</span>.x:                    YES <span class="o">(</span>ver <span class="m">2</span>.24.19<span class="o">)</span>
--     GThread :                    YES <span class="o">(</span>ver <span class="m">2</span>.36.3<span class="o">)</span>

--   Video I/O:
--     DC1394 <span class="m">2</span>.x:                  YES <span class="o">(</span>ver <span class="m">2</span>.2.0<span class="o">)</span>
--     FFMPEG:                      YES
--       codec:                     YES <span class="o">(</span>ver <span class="m">54</span>.92.100<span class="o">)</span>
--       format:                    YES <span class="o">(</span>ver <span class="m">54</span>.63.104<span class="o">)</span>
--       util:                      YES <span class="o">(</span>ver <span class="m">52</span>.18.100<span class="o">)</span>
--       swscale:                   YES <span class="o">(</span>ver <span class="m">2</span>.2.100<span class="o">)</span>
--       gentoo-style:              YES
--     GStreamer:
--       base:                      YES <span class="o">(</span>ver <span class="m">0</span>.10.36<span class="o">)</span>
--       video:                     YES <span class="o">(</span>ver <span class="m">0</span>.10.36<span class="o">)</span>
--       app:                       YES <span class="o">(</span>ver <span class="m">0</span>.10.36<span class="o">)</span>
--       riff:                      YES <span class="o">(</span>ver <span class="m">0</span>.10.36<span class="o">)</span>
--       pbutils:                   YES <span class="o">(</span>ver <span class="m">0</span>.10.36<span class="o">)</span>

--     V4L/V4L2:                    Using libv4l <span class="o">(</span>ver <span class="m">1</span>.0.0<span class="o">)</span>

--   Other third-party libraries:
--     Use Eigen:                   YES <span class="o">(</span>ver <span class="m">3</span>.1.4<span class="o">)</span>
--     Use TBB:                     YES <span class="o">(</span>ver <span class="m">4</span>.0 interface <span class="m">6004</span><span class="o">)</span>

--   Python:
--     Interpreter:                 /usr/bin/python2 <span class="o">(</span>ver <span class="m">2</span>.7.5<span class="o">)</span>
--     Libraries:                   /lib/libpython2.7.so <span class="o">(</span>ver <span class="m">2</span>.7.5<span class="o">)</span>
--     numpy:                       /usr/lib/python2.7/site-packages/numpy/core/include <span class="o">(</span>ver <span class="m">1</span>.7.1<span class="o">)</span>
--     packages path:               lib/python2.7/site-packages

--   Documentation:
--     Build Documentation:         YES
--     Sphinx:                      /usr/bin/sphinx-build <span class="o">(</span>ver <span class="m">1</span>.1.3<span class="o">)</span>
--     PdfLaTeX compiler:           /usr/bin/pdflatex
--
--   Tests and samples:
--     Tests:                       NO
--     Performance tests:           NO
--     C/C++ Examples:              NO
</pre></div>
</div>
</div></blockquote>
<p>Many other flags and settings are there. It is left for you for further exploration.</p>
<p>Now you build the files using <code class="docutils literal"><span class="pre">make</span></code> command and install it using <code class="docutils literal"><span class="pre">make</span> <span class="pre">install</span></code> command. <code class="docutils literal"><span class="pre">make</span> <span class="pre">install</span></code> should be executed as root.</p>
<blockquote>
<div><div class="highlight-bash"><div class="highlight"><pre><span></span>make
su
make install
</pre></div>
</div>
</div></blockquote>
<p>Installation is over. All files are installed in <code class="docutils literal"><span class="pre">/usr/local/</span></code> folder. But to use it, your Python should be able to find OpenCV module. You have two options for that.</p>
<ol class="arabic">
<li><p class="first"><strong>Move the module to any folder in Python Path</strong> : Python path can be found out by entering <code class="docutils literal"><span class="pre">import</span> <span class="pre">sys;print</span> <span class="pre">sys.path</span></code> in Python terminal. It will print out many locations. Move <code class="docutils literal"><span class="pre">/usr/local/lib/python2.7/site-packages/cv2.so</span></code> to any of this folder. For example,</p>
<blockquote>
<div><div class="highlight-bash"><div class="highlight"><pre><span></span>su mv /usr/local/lib/python2.7/site-packages/cv2.so /usr/lib/python2.7/site-packages
</pre></div>
</div>
</div></blockquote>
</li>
</ol>
<p>But you will have to do this every time you install OpenCV.</p>
<ol class="arabic" start="2">
<li><p class="first"><strong>Add ``/usr/local/lib/python2.7/site-packages`` to the PYTHON_PATH</strong>: It is to be done only once. Just open <code class="docutils literal"><span class="pre">~/.bashrc</span></code> and add following line to it, then log out and come back.</p>
<blockquote>
<div><div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">PYTHONPATH</span><span class="o">=</span><span class="nv">$PYTHONPATH</span>:/usr/local/lib/python2.7/site-packages
</pre></div>
</div>
</div></blockquote>
</li>
</ol>
<p>Thus OpenCV installation is finished. Open a terminal and try <code class="docutils literal"><span class="pre">import</span> <span class="pre">cv2</span></code>.</p>
<p>To build the documentation, just enter following commands:</p>
<blockquote>
<div><div class="highlight-bash"><div class="highlight"><pre><span></span>make docs
make html_docs
</pre></div>
</div>
</div></blockquote>
<p>Then open <code class="docutils literal"><span class="pre">opencv/build/doc/_html/index.html</span></code> and bookmark it in the browser.</p>
</div>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
<ol class="arabic simple">
<li>Compile OpenCV from source in your Fedora machine.</li>
</ol>
</div>
</div>
</div>
</div>
<span id="document-py_tutorials/py_gui/py_table_of_contents_gui/py_table_of_contents_gui"></span><div class="section" id="gui-features-in-opencv">
<span id="py-table-of-content-gui"></span><h3>Gui Features in OpenCV<a class="headerlink" href="#gui-features-in-opencv" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference internal" href="index.html#display-image"><span class="std std-ref">Getting Started with Images</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="17%" />
<col width="83%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/image_display.jpg"><img alt="gui_1" src="_images/image_display.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Learn to load an image, display it and save it back</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#display-video"><span class="std std-ref">Getting Started with Videos</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="13%" />
<col width="87%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/video_display.jpg"><img alt="gui_2" src="_images/video_display.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Learn to play videos, capture videos from Camera and write it as a video</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#drawing-functions"><span class="std std-ref">Drawing Functions in OpenCV</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="14%" />
<col width="86%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/drawing1.jpg"><img alt="gui_5" src="_images/drawing1.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Learn to draw lines, rectangles, ellipses, circles etc with OpenCV</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#mouse-handling"><span class="std std-ref">Mouse as a Paint-Brush</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="17%" />
<col width="83%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/mouse_drawing.jpg"><img alt="gui_3" src="_images/mouse_drawing.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Draw stuffs with your mouse</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#trackbar"><span class="std std-ref">Trackbar as the Color Palette</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="17%" />
<col width="83%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/trackbar.jpg"><img alt="gui_4" src="_images/trackbar.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Create trackbar to control certain parameters</td>
</tr>
</tbody>
</table>
</div></blockquote>
<div class="toctree-wrapper compound">
<span id="document-py_tutorials/py_gui/py_image_display/py_image_display"></span><div class="section" id="getting-started-with-images">
<span id="display-image"></span><h4>Getting Started with Images<a class="headerlink" href="#getting-started-with-images" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goals">
<h5>Goals<a class="headerlink" href="#goals" title="Permalink to this headline">¶</a></h5>
<div class="enumeratevisibleitemswithsquare docutils container">
<ul class="simple">
<li>Here, you will learn how to read an image, how to display it and how to save it back</li>
<li>You will learn these functions : <strong>cv2.imread()</strong>, <strong>cv2.imshow()</strong> , <strong>cv2.imwrite()</strong></li>
<li>Optionally, you will learn how to display images with Matplotlib</li>
</ul>
</div>
</div>
<div class="section" id="using-opencv">
<h5>Using OpenCV<a class="headerlink" href="#using-opencv" title="Permalink to this headline">¶</a></h5>
<div class="section" id="read-an-image">
<h6>Read an image<a class="headerlink" href="#read-an-image" title="Permalink to this headline">¶</a></h6>
<p>Use the function <strong>cv2.imread()</strong> to read an image. The image should be in the working directory or a full path of image should be given.</p>
<p>Second argument is a flag which specifies the way image should be read.</p>
<ul class="simple">
<li>cv2.IMREAD_COLOR : Loads a color image. Any transparency of image will be neglected. It is the default flag.</li>
<li>cv2.IMREAD_GRAYSCALE : Loads image in grayscale mode</li>
<li>cv2.IMREAD_UNCHANGED : Loads image as such including alpha channel</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Instead of these three flags, you can simply pass integers 1, 0 or -1 respectively.</p>
</div>
<p>See the code below:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cv2</span>

<span class="c1"># Load an color image in grayscale</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;messi5.jpg&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Even if the image path is wrong, it won&#8217;t throw any error, but <code class="docutils literal"><span class="pre">print</span> <span class="pre">img</span></code> will give you <code class="docutils literal"><span class="pre">None</span></code></p>
</div>
</div>
<div class="section" id="display-an-image">
<h6>Display an image<a class="headerlink" href="#display-an-image" title="Permalink to this headline">¶</a></h6>
<p>Use the function <strong>cv2.imshow()</strong> to display an image in a window. The window automatically fits to the image size.</p>
<p>First argument is a window name which is a string. second argument is our image. You can create as many windows as you wish, but with different window names.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">cv2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s1">&#39;image&#39;</span><span class="p">,</span><span class="n">img</span><span class="p">)</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>
</pre></div>
</div>
<p>A screenshot of the window will look like this (in Fedora-Gnome machine):</p>
<blockquote>
<div><img alt="Screenshot of Image Window in OpenCV" class="align-center" src="_images/opencv_screenshot.jpg" />
</div></blockquote>
<p><strong>cv2.waitKey()</strong> is a keyboard binding function. Its argument is the time in milliseconds. The function waits for specified milliseconds for any keyboard event. If you press any key in that time, the program continues. If <strong>0</strong> is passed, it waits indefinitely for a key stroke. It can also be set to detect specific key strokes like, if key <cite>a</cite> is pressed etc which we will discuss below.</p>
<p><strong>cv2.destroyAllWindows()</strong> simply destroys all the windows we created. If you want to destroy any specific window, use the function <strong>cv2.destroyWindow()</strong> where you pass the exact window name as the argument.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">There is a special case where you can already create a window and load image to it later. In that case, you can specify whether window is resizable or not. It is done with the function <strong>cv2.namedWindow()</strong>. By default, the flag is <code class="docutils literal"><span class="pre">cv2.WINDOW_AUTOSIZE</span></code>. But if you specify flag to be <code class="docutils literal"><span class="pre">cv2.WINDOW_NORMAL</span></code>, you can resize window. It will be helpful when image is too large in dimension and adding track bar to windows.</p>
</div>
<p>See the code below:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">cv2</span><span class="o">.</span><span class="n">namedWindow</span><span class="p">(</span><span class="s1">&#39;image&#39;</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">WINDOW_NORMAL</span><span class="p">)</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s1">&#39;image&#39;</span><span class="p">,</span><span class="n">img</span><span class="p">)</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="write-an-image">
<h6>Write an image<a class="headerlink" href="#write-an-image" title="Permalink to this headline">¶</a></h6>
<p>Use the function <strong>cv2.imwrite()</strong> to save an image.</p>
<p>First argument is the file name, second argument is the image you want to save.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">cv2</span><span class="o">.</span><span class="n">imwrite</span><span class="p">(</span><span class="s1">&#39;messigray.png&#39;</span><span class="p">,</span><span class="n">img</span><span class="p">)</span>
</pre></div>
</div>
<p>This will save the image in PNG format in the working directory.</p>
</div>
<div class="section" id="sum-it-up">
<h6>Sum it up<a class="headerlink" href="#sum-it-up" title="Permalink to this headline">¶</a></h6>
<p>Below program loads an image in grayscale, displays it, save the image if you press &#8216;s&#8217; and exit, or simply exit without saving if you press <cite>ESC</cite> key.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cv2</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;messi5.jpg&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s1">&#39;image&#39;</span><span class="p">,</span><span class="n">img</span><span class="p">)</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">27</span><span class="p">:</span>         <span class="c1"># wait for ESC key to exit</span>
    <span class="n">cv2</span><span class="o">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>
<span class="k">elif</span> <span class="n">k</span> <span class="o">==</span> <span class="nb">ord</span><span class="p">(</span><span class="s1">&#39;s&#39;</span><span class="p">):</span> <span class="c1"># wait for &#39;s&#39; key to save and exit</span>
    <span class="n">cv2</span><span class="o">.</span><span class="n">imwrite</span><span class="p">(</span><span class="s1">&#39;messigray.png&#39;</span><span class="p">,</span><span class="n">img</span><span class="p">)</span>
    <span class="n">cv2</span><span class="o">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">If you are using a 64-bit machine, you will have to modify <code class="docutils literal"><span class="pre">k</span> <span class="pre">=</span> <span class="pre">cv2.waitKey(0)</span></code> line as follows : <code class="docutils literal"><span class="pre">k</span> <span class="pre">=</span> <span class="pre">cv2.waitKey(0)</span> <span class="pre">&amp;</span> <span class="pre">0xFF</span></code></p>
</div>
</div>
</div>
<div class="section" id="using-matplotlib">
<h5>Using Matplotlib<a class="headerlink" href="#using-matplotlib" title="Permalink to this headline">¶</a></h5>
<p>Matplotlib is a plotting library for Python which gives you wide variety of plotting methods. You will see them in coming articles. Here, you will learn how to display image with Matplotlib. You can zoom images, save it etc using Matplotlib.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;messi5.jpg&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">interpolation</span> <span class="o">=</span> <span class="s1">&#39;bicubic&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([]),</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>  <span class="c1"># to hide tick values on X and Y axis</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>A screen-shot of the window will look like this :</p>
<blockquote>
<div><img alt="Screenshot of Image Window in Matplotlib" class="align-center" src="_images/matplotlib_screenshot.jpg" />
</div></blockquote>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last">Plenty of plotting options are available in Matplotlib. Please refer to Matplotlib docs for more details. Some, we will see on the way.</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Color image loaded by OpenCV is in BGR mode. But Matplotlib displays in RGB mode. So color images will not be displayed correctly in Matplotlib if image is read with OpenCV. Please see the exercises for more details.</p>
</div>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
<ol class="arabic simple">
<li><a class="reference external" href="http://matplotlib.org/api/pyplot_api.html">Matplotlib Plotting Styles and Features</a></li>
</ol>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
<ol class="arabic simple">
<li>There is some problem when you try to load color image in OpenCV and display it in Matplotlib. Read <a class="reference external" href="http://stackoverflow.com/a/15074748/1134940">this discussion</a> and understand it.</li>
</ol>
</div>
</div>
<span id="document-py_tutorials/py_gui/py_video_display/py_video_display"></span><div class="section" id="getting-started-with-videos">
<span id="display-video"></span><h4>Getting Started with Videos<a class="headerlink" href="#getting-started-with-videos" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goal">
<h5>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h5>
<div class="enumeratevisibleitemswithsquare docutils container">
<ul class="simple">
<li>Learn to read video, display video and save video.</li>
<li>Learn to capture from Camera and display it.</li>
<li>You will learn these functions : <strong>cv2.VideoCapture()</strong>, <strong>cv2.VideoWriter()</strong></li>
</ul>
</div>
</div>
<div class="section" id="capture-video-from-camera">
<h5>Capture Video from Camera<a class="headerlink" href="#capture-video-from-camera" title="Permalink to this headline">¶</a></h5>
<p>Often, we have to capture live stream with camera. OpenCV provides a very simple interface to this. Let&#8217;s capture a video from the camera (I am using the in-built webcam of my laptop), convert it into grayscale video and display it. Just a simple task to get started.</p>
<p>To capture a video, you need to create a <strong>VideoCapture</strong> object. Its argument can be either the device index or the name of a video file. Device index is just the number to specify which camera. Normally one camera will be connected (as in my case). So I simply pass 0 (or -1). You can select the second camera by passing 1 and so on. After that, you can capture frame-by-frame. But at the end, don&#8217;t forget to release the capture.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cv2</span>

<span class="n">cap</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">VideoCapture</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="k">while</span><span class="p">(</span><span class="kc">True</span><span class="p">):</span>
    <span class="c1"># Capture frame-by-frame</span>
    <span class="n">ret</span><span class="p">,</span> <span class="n">frame</span> <span class="o">=</span> <span class="n">cap</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>

    <span class="c1"># Our operations on the frame come here</span>
    <span class="n">gray</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2GRAY</span><span class="p">)</span>

    <span class="c1"># Display the resulting frame</span>
    <span class="n">cv2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s1">&#39;frame&#39;</span><span class="p">,</span><span class="n">gray</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">cv2</span><span class="o">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="mh">0xFF</span> <span class="o">==</span> <span class="nb">ord</span><span class="p">(</span><span class="s1">&#39;q&#39;</span><span class="p">):</span>
        <span class="k">break</span>

<span class="c1"># When everything done, release the capture</span>
<span class="n">cap</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>
</pre></div>
</div>
<p><code class="docutils literal"><span class="pre">cap.read()</span></code> returns a bool (True/False). If frame is read correctly, it will be True. So you can check end of the video by checking this return value.</p>
<p>Sometimes, <code class="docutils literal"><span class="pre">cap</span></code> may not have initialized the capture. In that case, this code shows error. You can check whether it is initialized or not by the method <strong>cap.isOpened()</strong>. If it is True, OK. Otherwise open it using <strong>cap.open()</strong>.</p>
<p>You can also access some of the features of this video using <strong>cap.get(propId)</strong> method where propId is a number from 0 to 18. Each number denotes a property of the video (if it is applicable to that video) and full details can be seen here: <a class="reference external" href="http://docs.opencv.org/modules/highgui/doc/reading_and_writing_images_and_video.html#videocapture-get">Property Identifier</a>. Some of these values can be modified using <strong>cap.set(propId, value)</strong>. Value is the new value you want.</p>
<p>For example, I can check the frame width and height by <code class="docutils literal"><span class="pre">cap.get(3)</span></code> and <code class="docutils literal"><span class="pre">cap.get(4)</span></code>. It gives me 640x480 by default. But I want to modify it to 320x240. Just use <code class="docutils literal"><span class="pre">ret</span> <span class="pre">=</span> <span class="pre">cap.set(3,320)</span></code> and <code class="docutils literal"><span class="pre">ret</span> <span class="pre">=</span> <span class="pre">cap.set(4,240)</span></code>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">If you are getting error, make sure camera is working fine using any other camera application (like Cheese in Linux).</p>
</div>
</div>
<div class="section" id="playing-video-from-file">
<h5>Playing Video from file<a class="headerlink" href="#playing-video-from-file" title="Permalink to this headline">¶</a></h5>
<p>It is same as capturing from Camera, just change camera index with video file name. Also while displaying the frame, use appropriate time for <code class="docutils literal"><span class="pre">cv2.waitKey()</span></code>. If it is too less, video will be very fast and if it is too high, video will be slow (Well, that is how you can display videos in slow motion). 25 milliseconds will be OK in normal cases.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cv2</span>

<span class="n">cap</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">VideoCapture</span><span class="p">(</span><span class="s1">&#39;vtest.avi&#39;</span><span class="p">)</span>

<span class="k">while</span><span class="p">(</span><span class="n">cap</span><span class="o">.</span><span class="n">isOpened</span><span class="p">()):</span>
    <span class="n">ret</span><span class="p">,</span> <span class="n">frame</span> <span class="o">=</span> <span class="n">cap</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>

    <span class="n">gray</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2GRAY</span><span class="p">)</span>

    <span class="n">cv2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s1">&#39;frame&#39;</span><span class="p">,</span><span class="n">gray</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">cv2</span><span class="o">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="mh">0xFF</span> <span class="o">==</span> <span class="nb">ord</span><span class="p">(</span><span class="s1">&#39;q&#39;</span><span class="p">):</span>
        <span class="k">break</span>

<span class="n">cap</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Make sure proper versions of ffmpeg or gstreamer is installed. Sometimes, it is a headache to work with Video Capture mostly due to wrong installation of ffmpeg/gstreamer.</p>
</div>
</div>
<div class="section" id="saving-a-video">
<h5>Saving a Video<a class="headerlink" href="#saving-a-video" title="Permalink to this headline">¶</a></h5>
<p>So we capture a video, process it frame-by-frame and we want to save that video. For images, it is very simple, just use <code class="docutils literal"><span class="pre">cv2.imwrite()</span></code>. Here a little more work is required.</p>
<p>This time we create a <strong>VideoWriter</strong> object. We should specify the output file name (eg: output.avi). Then we should specify the <strong>FourCC</strong> code (details in next paragraph). Then number of frames per second (fps) and frame size should be passed. And last one is <strong>isColor</strong> flag. If it is True, encoder expect color frame, otherwise it works with grayscale frame.</p>
<p><a class="reference external" href="http://en.wikipedia.org/wiki/FourCC">FourCC</a> is a 4-byte code used to specify the video codec. The list of available codes can be found in <a class="reference external" href="http://www.fourcc.org/codecs.php">fourcc.org</a>. It is platform dependent. Following codecs works fine for me.</p>
<ul class="simple">
<li>In Fedora: DIVX, XVID, MJPG, X264, WMV1, WMV2. (XVID is more preferable. MJPG results in high size video. X264 gives very small size video)</li>
<li>In Windows: DIVX (More to be tested and added)</li>
<li>In OSX : <em>(I don&#8217;t have access to OSX. Can some one fill this?)</em></li>
</ul>
<p>FourCC code is passed as <code class="docutils literal"><span class="pre">cv2.VideoWriter_fourcc('M','J','P','G')</span></code> or <code class="docutils literal"><span class="pre">cv2.VideoWriter_fourcc(*'MJPG)</span></code> for MJPG.</p>
<p>Below code capture from a Camera, flip every frame in vertical direction and saves it.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cv2</span>

<span class="n">cap</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">VideoCapture</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Define the codec and create VideoWriter object</span>
<span class="n">fourcc</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">VideoWriter_fourcc</span><span class="p">(</span><span class="o">*</span><span class="s1">&#39;XVID&#39;</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">VideoWriter</span><span class="p">(</span><span class="s1">&#39;output.avi&#39;</span><span class="p">,</span><span class="n">fourcc</span><span class="p">,</span> <span class="mf">20.0</span><span class="p">,</span> <span class="p">(</span><span class="mi">640</span><span class="p">,</span><span class="mi">480</span><span class="p">))</span>

<span class="k">while</span><span class="p">(</span><span class="n">cap</span><span class="o">.</span><span class="n">isOpened</span><span class="p">()):</span>
    <span class="n">ret</span><span class="p">,</span> <span class="n">frame</span> <span class="o">=</span> <span class="n">cap</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">ret</span><span class="o">==</span><span class="kc">True</span><span class="p">:</span>
        <span class="n">frame</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># write the flipped frame</span>
        <span class="n">out</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>

        <span class="n">cv2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s1">&#39;frame&#39;</span><span class="p">,</span><span class="n">frame</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">cv2</span><span class="o">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="mh">0xFF</span> <span class="o">==</span> <span class="nb">ord</span><span class="p">(</span><span class="s1">&#39;q&#39;</span><span class="p">):</span>
            <span class="k">break</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">break</span>

<span class="c1"># Release everything if job is finished</span>
<span class="n">cap</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>
<span class="n">out</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
</div>
</div>
<span id="document-py_tutorials/py_gui/py_drawing_functions/py_drawing_functions"></span><div class="section" id="drawing-functions-in-opencv">
<span id="drawing-functions"></span><h4>Drawing Functions in OpenCV<a class="headerlink" href="#drawing-functions-in-opencv" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goal">
<h5>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h5>
<div class="enumeratevisibleitemswithsquare docutils container">
<ul class="simple">
<li>Learn to draw different geometric shapes with OpenCV</li>
<li>You will learn these functions : <strong>cv2.line()</strong>, <strong>cv2.circle()</strong> , <strong>cv2.rectangle()</strong>, <strong>cv2.ellipse()</strong>, <strong>cv2.putText()</strong> etc.</li>
</ul>
</div>
</div>
<div class="section" id="code">
<h5>Code<a class="headerlink" href="#code" title="Permalink to this headline">¶</a></h5>
<p>In all the above functions, you will see some common arguments as given below:</p>
<blockquote>
<div><ul class="simple">
<li>img : The image where you want to draw the shapes</li>
<li>color : Color of the shape. for BGR, pass it as a tuple, eg: <code class="docutils literal"><span class="pre">(255,0,0)</span></code> for blue. For grayscale, just pass the scalar value.</li>
<li>thickness : Thickness of the line or circle etc. If <strong>-1</strong> is passed for closed figures like circles, it will fill the shape. <em>default thickness = 1</em></li>
<li>lineType : Type of line, whether 8-connected, anti-aliased line etc. <em>By default, it is 8-connected.</em> <code class="docutils literal"><span class="pre">cv2.LINE_AA</span></code> gives anti-aliased line which looks great for curves.</li>
</ul>
</div></blockquote>
<div class="section" id="drawing-line">
<h6>Drawing Line<a class="headerlink" href="#drawing-line" title="Permalink to this headline">¶</a></h6>
<p>To draw a line, you need to pass starting and ending coordinates of line. We will create a black image and draw a blue line on it from top-left to bottom-right corners.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cv2</span>

<span class="c1"># Create a black image</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>

<span class="c1"># Draw a diagonal blue line with thickness of 5 px</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">line</span><span class="p">(</span><span class="n">img</span><span class="p">,(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),(</span><span class="mi">511</span><span class="p">,</span><span class="mi">511</span><span class="p">),(</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="drawing-rectangle">
<h6>Drawing Rectangle<a class="headerlink" href="#drawing-rectangle" title="Permalink to this headline">¶</a></h6>
<p>To draw a rectangle, you need top-left corner and bottom-right corner of rectangle. This time we will draw a green rectangle at the top-right corner of image.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">rectangle</span><span class="p">(</span><span class="n">img</span><span class="p">,(</span><span class="mi">384</span><span class="p">,</span><span class="mi">0</span><span class="p">),(</span><span class="mi">510</span><span class="p">,</span><span class="mi">128</span><span class="p">),(</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="drawing-circle">
<h6>Drawing Circle<a class="headerlink" href="#drawing-circle" title="Permalink to this headline">¶</a></h6>
<p>To draw a circle, you need its center coordinates and radius. We will draw a circle inside the rectangle drawn above.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">circle</span><span class="p">(</span><span class="n">img</span><span class="p">,(</span><span class="mi">447</span><span class="p">,</span><span class="mi">63</span><span class="p">),</span> <span class="mi">63</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="drawing-ellipse">
<h6>Drawing Ellipse<a class="headerlink" href="#drawing-ellipse" title="Permalink to this headline">¶</a></h6>
<p>To draw the ellipse, we need to pass several arguments. One argument is the center location (x,y). Next argument is axes lengths (major axis length, minor axis length). <code class="docutils literal"><span class="pre">angle</span></code> is the angle of rotation of ellipse in anti-clockwise direction. <code class="docutils literal"><span class="pre">startAngle</span></code> and <code class="docutils literal"><span class="pre">endAngle</span></code> denotes the starting and ending of ellipse arc measured in clockwise direction from major axis. i.e. giving values 0 and 360 gives the full ellipse. For more details, check the documentation of <strong>cv2.ellipse()</strong>. Below example draws a half ellipse at the center of the image.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">ellipse</span><span class="p">(</span><span class="n">img</span><span class="p">,(</span><span class="mi">256</span><span class="p">,</span><span class="mi">256</span><span class="p">),(</span><span class="mi">100</span><span class="p">,</span><span class="mi">50</span><span class="p">),</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">180</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="drawing-polygon">
<h6>Drawing Polygon<a class="headerlink" href="#drawing-polygon" title="Permalink to this headline">¶</a></h6>
<p>To draw a polygon, first you need coordinates of vertices. Make those points into an array of shape <code class="docutils literal"><span class="pre">ROWSx1x2</span></code> where ROWS are number of vertices and it should be of type <code class="docutils literal"><span class="pre">int32</span></code>. Here we draw a small polygon of with four vertices in yellow color.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">pts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">10</span><span class="p">,</span><span class="mi">5</span><span class="p">],[</span><span class="mi">20</span><span class="p">,</span><span class="mi">30</span><span class="p">],[</span><span class="mi">70</span><span class="p">,</span><span class="mi">20</span><span class="p">],[</span><span class="mi">50</span><span class="p">,</span><span class="mi">10</span><span class="p">]],</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="n">pts</span> <span class="o">=</span> <span class="n">pts</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">polylines</span><span class="p">(</span><span class="n">img</span><span class="p">,[</span><span class="n">pts</span><span class="p">],</span><span class="kc">True</span><span class="p">,(</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="mi">255</span><span class="p">))</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">If third argument is <code class="docutils literal"><span class="pre">False</span></code>, you will get a polylines joining all the points, not a closed shape.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><code class="docutils literal"><span class="pre">cv2.polylines()</span></code> can be used to draw multiple lines. Just create a list of all the lines you want to draw and pass it to the function. All lines will be drawn individually. It is more better and faster way to draw a group of lines than calling <code class="docutils literal"><span class="pre">cv2.line()</span></code> for each line.</p>
</div>
</div>
<div class="section" id="adding-text-to-images">
<h6>Adding Text to Images:<a class="headerlink" href="#adding-text-to-images" title="Permalink to this headline">¶</a></h6>
<dl class="docutils">
<dt>To put texts in images, you need specify following things.</dt>
<dd><ul class="first last simple">
<li>Text data that you want to write</li>
<li>Position coordinates of where you want put it (i.e. bottom-left corner where data starts).</li>
<li>Font type (Check <strong>cv2.putText()</strong> docs for supported fonts)</li>
<li>Font Scale (specifies the size of font)</li>
<li>regular things like color, thickness, lineType etc. For better look, <code class="docutils literal"><span class="pre">lineType</span> <span class="pre">=</span> <span class="pre">cv2.LINE_AA</span></code> is recommended.</li>
</ul>
</dd>
</dl>
<p>We will write <strong>OpenCV</strong> on our image in white color.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">font</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">FONT_HERSHEY_SIMPLEX</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">putText</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="s1">&#39;OpenCV&#39;</span><span class="p">,(</span><span class="mi">10</span><span class="p">,</span><span class="mi">500</span><span class="p">),</span> <span class="n">font</span><span class="p">,</span> <span class="mi">4</span><span class="p">,(</span><span class="mi">255</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="mi">255</span><span class="p">),</span><span class="mi">2</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">LINE_AA</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="result">
<h6>Result<a class="headerlink" href="#result" title="Permalink to this headline">¶</a></h6>
<p>So it is time to see the final result of our drawing. As you studied in previous articles, display the image to see it.</p>
<blockquote>
<div><img alt="Drawing Functions in OpenCV" class="align-center" src="_images/drawing.jpg" />
</div></blockquote>
</div>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
<ol class="arabic simple">
<li>The angles used in ellipse function is not our circular angles. For more details, visit <a class="reference external" href="http://answers.opencv.org/question/14541/angles-in-ellipse-function/">this discussion</a>.</li>
</ol>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
<ol class="arabic simple">
<li>Try to create the logo of OpenCV using drawing functions available in OpenCV</li>
</ol>
</div>
</div>
<span id="document-py_tutorials/py_gui/py_mouse_handling/py_mouse_handling"></span><div class="section" id="mouse-as-a-paint-brush">
<span id="mouse-handling"></span><h4>Mouse as a Paint-Brush<a class="headerlink" href="#mouse-as-a-paint-brush" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goal">
<h5>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h5>
<div class="enumeratevisibleitemswithsquare docutils container">
<ul class="simple">
<li>Learn to handle mouse events in OpenCV</li>
<li>You will learn these functions : <strong>cv2.setMouseCallback()</strong></li>
</ul>
</div>
</div>
<div class="section" id="simple-demo">
<h5>Simple Demo<a class="headerlink" href="#simple-demo" title="Permalink to this headline">¶</a></h5>
<p>Here, we create a simple application which draws a circle on an image wherever we double-click on it.</p>
<p>First we create a mouse callback function which is executed when a mouse event take place. Mouse event can be anything related to mouse like left-button down, left-button up, left-button double-click etc. It gives us the coordinates (x,y) for every mouse event. With this event and location, we can do whatever we like. To list all available events available, run the following code in Python terminal:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">events</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">(</span><span class="n">cv2</span><span class="p">)</span> <span class="k">if</span> <span class="s1">&#39;EVENT&#39;</span> <span class="ow">in</span> <span class="n">i</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span> <span class="n">events</span>
</pre></div>
</div>
<p>Creating mouse callback function has a specific format which is same everywhere. It differs only in what the function does. So our mouse callback function does one thing, it draws a circle where we double-click. So see the code below. Code is self-explanatory from comments :</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># mouse callback function</span>
<span class="k">def</span> <span class="nf">draw_circle</span><span class="p">(</span><span class="n">event</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">flags</span><span class="p">,</span><span class="n">param</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">event</span> <span class="o">==</span> <span class="n">cv2</span><span class="o">.</span><span class="n">EVENT_LBUTTONDBLCLK</span><span class="p">:</span>
        <span class="n">cv2</span><span class="o">.</span><span class="n">circle</span><span class="p">(</span><span class="n">img</span><span class="p">,(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">),</span><span class="mi">100</span><span class="p">,(</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Create a black image, a window and bind the function to window</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">namedWindow</span><span class="p">(</span><span class="s1">&#39;image&#39;</span><span class="p">)</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">setMouseCallback</span><span class="p">(</span><span class="s1">&#39;image&#39;</span><span class="p">,</span><span class="n">draw_circle</span><span class="p">)</span>

<span class="k">while</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">cv2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s1">&#39;image&#39;</span><span class="p">,</span><span class="n">img</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">cv2</span><span class="o">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span> <span class="o">&amp;</span> <span class="mh">0xFF</span> <span class="o">==</span> <span class="mi">27</span><span class="p">:</span>
        <span class="k">break</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="more-advanced-demo">
<h5>More Advanced Demo<a class="headerlink" href="#more-advanced-demo" title="Permalink to this headline">¶</a></h5>
<p>Now we go for much more better application. In this, we draw either rectangles or circles (depending on the mode we select) by dragging the mouse like we do in Paint application. So our mouse callback function has two parts, one to draw rectangle and other to draw the circles. This specific example will be really helpful in creating and understanding some interactive applications like object tracking, image segmentation etc.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">drawing</span> <span class="o">=</span> <span class="kc">False</span> <span class="c1"># true if mouse is pressed</span>
<span class="n">mode</span> <span class="o">=</span> <span class="kc">True</span> <span class="c1"># if True, draw rectangle. Press &#39;m&#39; to toggle to curve</span>
<span class="n">ix</span><span class="p">,</span><span class="n">iy</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span>

<span class="c1"># mouse callback function</span>
<span class="k">def</span> <span class="nf">draw_circle</span><span class="p">(</span><span class="n">event</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">flags</span><span class="p">,</span><span class="n">param</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">ix</span><span class="p">,</span><span class="n">iy</span><span class="p">,</span><span class="n">drawing</span><span class="p">,</span><span class="n">mode</span>

    <span class="k">if</span> <span class="n">event</span> <span class="o">==</span> <span class="n">cv2</span><span class="o">.</span><span class="n">EVENT_LBUTTONDOWN</span><span class="p">:</span>
        <span class="n">drawing</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">ix</span><span class="p">,</span><span class="n">iy</span> <span class="o">=</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span>

    <span class="k">elif</span> <span class="n">event</span> <span class="o">==</span> <span class="n">cv2</span><span class="o">.</span><span class="n">EVENT_MOUSEMOVE</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">drawing</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
                <span class="n">cv2</span><span class="o">.</span><span class="n">rectangle</span><span class="p">(</span><span class="n">img</span><span class="p">,(</span><span class="n">ix</span><span class="p">,</span><span class="n">iy</span><span class="p">),(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">),(</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">cv2</span><span class="o">.</span><span class="n">circle</span><span class="p">(</span><span class="n">img</span><span class="p">,(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">),</span><span class="mi">5</span><span class="p">,(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">),</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">elif</span> <span class="n">event</span> <span class="o">==</span> <span class="n">cv2</span><span class="o">.</span><span class="n">EVENT_LBUTTONUP</span><span class="p">:</span>
        <span class="n">drawing</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">cv2</span><span class="o">.</span><span class="n">rectangle</span><span class="p">(</span><span class="n">img</span><span class="p">,(</span><span class="n">ix</span><span class="p">,</span><span class="n">iy</span><span class="p">),(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">),(</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">cv2</span><span class="o">.</span><span class="n">circle</span><span class="p">(</span><span class="n">img</span><span class="p">,(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">),</span><span class="mi">5</span><span class="p">,(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">),</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>Next we have to bind this mouse callback function to OpenCV window. In the main loop, we should set a keyboard binding for key &#8216;m&#8217; to toggle between rectangle and circle.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">512</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">namedWindow</span><span class="p">(</span><span class="s1">&#39;image&#39;</span><span class="p">)</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">setMouseCallback</span><span class="p">(</span><span class="s1">&#39;image&#39;</span><span class="p">,</span><span class="n">draw_circle</span><span class="p">)</span>

<span class="k">while</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">cv2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s1">&#39;image&#39;</span><span class="p">,</span><span class="n">img</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="mh">0xFF</span>
    <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="nb">ord</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">):</span>
        <span class="n">mode</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">mode</span>
    <span class="k">elif</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">27</span><span class="p">:</span>
        <span class="k">break</span>

<span class="n">cv2</span><span class="o">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
<ol class="arabic simple">
<li>In our last example, we drew filled rectangle. You modify the code to draw an unfilled rectangle.</li>
</ol>
</div>
</div>
<span id="document-py_tutorials/py_gui/py_trackbar/py_trackbar"></span><div class="section" id="trackbar-as-the-color-palette">
<span id="trackbar"></span><h4>Trackbar as the Color Palette<a class="headerlink" href="#trackbar-as-the-color-palette" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goal">
<h5>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h5>
<div class="enumeratevisibleitemswithsquare docutils container">
<ul class="simple">
<li>Learn to bind trackbar to OpenCV windows</li>
<li>You will learn these functions : <strong>cv2.getTrackbarPos()</strong>, <strong>cv2.createTrackbar()</strong> etc.</li>
</ul>
</div>
</div>
<div class="section" id="code-demo">
<h5>Code Demo<a class="headerlink" href="#code-demo" title="Permalink to this headline">¶</a></h5>
<p>Here we will create a simple application which shows the color you specify. You have a window which shows the color and three trackbars to specify each of B,G,R colors. You slide the trackbar and correspondingly window color changes. By default, initial color will be set to Black.</p>
<p>For cv2.getTrackbarPos() function, first argument is the trackbar name, second one is the window name to which it is attached, third argument is the default value, fourth one is the maximum value and fifth one is the callback function which is executed everytime trackbar value changes. The callback function always has a default argument which is the trackbar position. In our case, function does nothing, so we simply pass.</p>
<p>Another important application of trackbar is to use it as a button or switch. OpenCV, by default, doesn&#8217;t have button functionality. So you can use trackbar to get such functionality. In our application, we have created one switch in which application works only if switch is ON, otherwise screen is always black.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">nothing</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">pass</span>

<span class="c1"># Create a black image, a window</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">300</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">namedWindow</span><span class="p">(</span><span class="s1">&#39;image&#39;</span><span class="p">)</span>

<span class="c1"># create trackbars for color change</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">createTrackbar</span><span class="p">(</span><span class="s1">&#39;R&#39;</span><span class="p">,</span><span class="s1">&#39;image&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="n">nothing</span><span class="p">)</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">createTrackbar</span><span class="p">(</span><span class="s1">&#39;G&#39;</span><span class="p">,</span><span class="s1">&#39;image&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="n">nothing</span><span class="p">)</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">createTrackbar</span><span class="p">(</span><span class="s1">&#39;B&#39;</span><span class="p">,</span><span class="s1">&#39;image&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="n">nothing</span><span class="p">)</span>

<span class="c1"># create switch for ON/OFF functionality</span>
<span class="n">switch</span> <span class="o">=</span> <span class="s1">&#39;0 : OFF </span><span class="se">\n</span><span class="s1">1 : ON&#39;</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">createTrackbar</span><span class="p">(</span><span class="n">switch</span><span class="p">,</span> <span class="s1">&#39;image&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">nothing</span><span class="p">)</span>

<span class="k">while</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">cv2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s1">&#39;image&#39;</span><span class="p">,</span><span class="n">img</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="mh">0xFF</span>
    <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">27</span><span class="p">:</span>
        <span class="k">break</span>

    <span class="c1"># get current positions of four trackbars</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">getTrackbarPos</span><span class="p">(</span><span class="s1">&#39;R&#39;</span><span class="p">,</span><span class="s1">&#39;image&#39;</span><span class="p">)</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">getTrackbarPos</span><span class="p">(</span><span class="s1">&#39;G&#39;</span><span class="p">,</span><span class="s1">&#39;image&#39;</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">getTrackbarPos</span><span class="p">(</span><span class="s1">&#39;B&#39;</span><span class="p">,</span><span class="s1">&#39;image&#39;</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">getTrackbarPos</span><span class="p">(</span><span class="n">switch</span><span class="p">,</span><span class="s1">&#39;image&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">s</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">img</span><span class="p">[:]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">img</span><span class="p">[:]</span> <span class="o">=</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span><span class="n">g</span><span class="p">,</span><span class="n">r</span><span class="p">]</span>

<span class="n">cv2</span><span class="o">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>
</pre></div>
</div>
<p>The screenshot of the application looks like below :</p>
<blockquote>
<div><img alt="Screenshot of Image with Trackbars" class="align-center" src="_images/trackbar_screenshot.jpg" />
</div></blockquote>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
<ol class="arabic simple">
<li>Create a Paint application with adjustable colors and brush radius using trackbars. For drawing, refer previous tutorial on mouse handling.</li>
</ol>
</div>
</div>
</div>
</div>
<span id="document-py_tutorials/py_core/py_table_of_contents_core/py_table_of_contents_core"></span><div class="section" id="core-operations">
<span id="py-table-of-content-core"></span><h3>Core Operations<a class="headerlink" href="#core-operations" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference internal" href="index.html#basic-ops"><span class="std std-ref">Basic Operations on Images</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="11%" />
<col width="89%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/pixel_ops.jpg"><img alt="core_1" src="_images/pixel_ops.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Learn to read and edit pixel values, working with image ROI and other basic operations.</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#image-arithmetics"><span class="std std-ref">Arithmetic Operations on Images</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="17%" />
<col width="83%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/image_arithmetic.jpg"><img alt="core_2" src="_images/image_arithmetic.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Perform arithmetic operations on images</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#optimization-techniques"><span class="std std-ref">Performance Measurement and Improvement Techniques</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="7%" />
<col width="93%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/speed.jpg"><img alt="core_4" src="_images/speed.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Getting a solution is important. But getting it in the fastest way is more important. Learn to check the speed of your code, optimize the code etc.</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#mathematical-tools"><span class="std std-ref">Mathematical Tools in OpenCV</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="13%" />
<col width="87%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/maths_tools.jpg"><img alt="core_5" src="_images/maths_tools.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Learn some of the mathematical tools provided by OpenCV like PCA, SVD etc.</td>
</tr>
</tbody>
</table>
</div></blockquote>
<div class="toctree-wrapper compound">
<span id="document-py_tutorials/py_core/py_basic_ops/py_basic_ops"></span><div class="section" id="basic-operations-on-images">
<span id="basic-ops"></span><h4>Basic Operations on Images<a class="headerlink" href="#basic-operations-on-images" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goal">
<h5>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h5>
<p>Learn to:</p>
<blockquote>
<div><ul class="simple">
<li>Access pixel values and modify them</li>
<li>Access image properties</li>
<li>Setting Region of Image (ROI)</li>
<li>Splitting and Merging images</li>
</ul>
<p>Almost all the operations in this section is mainly related to Numpy rather than OpenCV. A good knowledge of Numpy is required to write better optimized code with OpenCV.</p>
<p><em>( Examples will be shown in Python terminal since most of them are just single line codes )</em></p>
</div></blockquote>
</div>
<div class="section" id="accessing-and-modifying-pixel-values">
<h5>Accessing and Modifying pixel values<a class="headerlink" href="#accessing-and-modifying-pixel-values" title="Permalink to this headline">¶</a></h5>
<p>Let&#8217;s load a color image first:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;messi5.jpg&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>You can access a pixel value by its row and column coordinates. For BGR image, it returns an array of Blue, Green, Red values. For grayscale image, just corresponding intensity is returned.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">px</span> <span class="o">=</span> <span class="n">img</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span><span class="mi">100</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span> <span class="n">px</span>
<span class="go">[157 166 200]</span>

<span class="go"># accessing only blue pixel</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">blue</span> <span class="o">=</span> <span class="n">img</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span> <span class="n">blue</span>
<span class="go">157</span>
</pre></div>
</div>
<p>You can modify the pixel values the same way.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">img</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span><span class="mi">100</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">255</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="mi">255</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span> <span class="n">img</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span><span class="mi">100</span><span class="p">]</span>
<span class="go">[255 255 255]</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Numpy is a optimized library for fast array calculations. So simply accessing each and every pixel values and modifying it will be very slow and it is discouraged.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Above mentioned method is normally used for selecting a region of array, say first 5 rows and last 3 columns like that. For individual pixel access, Numpy array methods, <code class="docutils literal"><span class="pre">array.item()</span></code> and <code class="docutils literal"><span class="pre">array.itemset()</span></code> is considered to be better. But it always returns a scalar. So if you want to access all B,G,R values, you need to call <code class="docutils literal"><span class="pre">array.item()</span></code> separately for all.</p>
</div>
<p>Better pixel accessing and editing method :</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1"># accessing RED value</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">img</span><span class="o">.</span><span class="n">item</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="mi">59</span>

<span class="c1"># modifying RED value</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">img</span><span class="o">.</span><span class="n">itemset</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span><span class="mi">100</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">img</span><span class="o">.</span><span class="n">item</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="mi">100</span>
</pre></div>
</div>
</div>
<div class="section" id="accessing-image-properties">
<h5>Accessing Image Properties<a class="headerlink" href="#accessing-image-properties" title="Permalink to this headline">¶</a></h5>
<p>Image properties include number of rows, columns and channels, type of image data, number of pixels etc.</p>
<p>Shape of image is accessed by <code class="docutils literal"><span class="pre">img.shape</span></code>. It returns a tuple of number of rows, columns and channels (if image is color):</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span> <span class="n">img</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(342, 548, 3)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">If image is grayscale, tuple returned contains only number of rows and columns. So it is a good method to check if loaded image is grayscale or color image.</p>
</div>
<p>Total number of pixels is accessed by <code class="docutils literal"><span class="pre">img.size</span></code>:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span> <span class="n">img</span><span class="o">.</span><span class="n">size</span>
<span class="go">562248</span>
</pre></div>
</div>
<p>Image datatype is obtained by <code class="docutils literal"><span class="pre">img.dtype</span></code>:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span> <span class="n">img</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">uint8</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><code class="docutils literal"><span class="pre">img.dtype</span></code> is very important while debugging because a large number of errors in OpenCV-Python code is caused by invalid datatype.</p>
</div>
</div>
<div class="section" id="image-roi">
<h5>Image ROI<a class="headerlink" href="#image-roi" title="Permalink to this headline">¶</a></h5>
<p>Sometimes, you will have to play with certain region of images. For eye detection in images, first perform face detection over the image until the face is found, then search within the face region for eyes. This approach improves accuracy (because eyes are always on faces :D ) and performance (because we search for a small area).</p>
<p>ROI is again obtained using Numpy indexing. Here I am selecting the ball and copying it to another region in the image:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ball</span> <span class="o">=</span> <span class="n">img</span><span class="p">[</span><span class="mi">280</span><span class="p">:</span><span class="mi">340</span><span class="p">,</span> <span class="mi">330</span><span class="p">:</span><span class="mi">390</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">img</span><span class="p">[</span><span class="mi">273</span><span class="p">:</span><span class="mi">333</span><span class="p">,</span> <span class="mi">100</span><span class="p">:</span><span class="mi">160</span><span class="p">]</span> <span class="o">=</span> <span class="n">ball</span>
</pre></div>
</div>
<p>Check the results below:</p>
<blockquote>
<div><img alt="Image ROI" class="align-center" src="_images/roi.jpg" />
</div></blockquote>
</div>
<div class="section" id="splitting-and-merging-image-channels">
<h5>Splitting and Merging Image Channels<a class="headerlink" href="#splitting-and-merging-image-channels" title="Permalink to this headline">¶</a></h5>
<p>The B,G,R channels of an image can be split into their individual planes when needed. Then, the individual channels can be merged back together to form a BGR image again. This can be performed by:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="p">,</span><span class="n">g</span><span class="p">,</span><span class="n">r</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">merge</span><span class="p">((</span><span class="n">b</span><span class="p">,</span><span class="n">g</span><span class="p">,</span><span class="n">r</span><span class="p">))</span>
</pre></div>
</div>
<p>Or</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">img</span><span class="p">[:,:,</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p>Suppose, you want to make all the red pixels to zero, you need not split like this and put it equal to zero. You can simply use Numpy indexing which is faster.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">img</span><span class="p">[:,:,</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last"><code class="docutils literal"><span class="pre">cv2.split()</span></code> is a costly operation (in terms of time), so only use it if necessary. Numpy indexing is much more efficient and should be used if possible.</p>
</div>
</div>
<div class="section" id="making-borders-for-images-padding">
<h5>Making Borders for Images (Padding)<a class="headerlink" href="#making-borders-for-images-padding" title="Permalink to this headline">¶</a></h5>
<p>If you want to create a border around the image, something like a photo frame, you can use <strong>cv2.copyMakeBorder()</strong> function. But it has more applications for convolution operation, zero padding etc. This function takes following arguments:</p>
<blockquote>
<div><ul class="simple">
<li><strong>src</strong> - input image</li>
<li><strong>top</strong>, <strong>bottom</strong>, <strong>left</strong>, <strong>right</strong> - border width in number of pixels in corresponding directions</li>
<li><dl class="first docutils">
<dt><strong>borderType</strong> - Flag defining what kind of border to be added. It can be following types:</dt>
<dd><ul class="first last">
<li><strong>cv2.BORDER_CONSTANT</strong> - Adds a constant colored border. The value should be given as next argument.</li>
<li><strong>cv2.BORDER_REFLECT</strong> - Border will be mirror reflection of the border elements, like this : <em>fedcba|abcdefgh|hgfedcb</em></li>
<li><strong>cv2.BORDER_REFLECT_101</strong> or <strong>cv2.BORDER_DEFAULT</strong> - Same as above, but with a slight change, like this : <em>gfedcb|abcdefgh|gfedcba</em></li>
<li><strong>cv2.BORDER_REPLICATE</strong> - Last element is replicated throughout, like this: <em>aaaaaa|abcdefgh|hhhhhhh</em></li>
<li><strong>cv2.BORDER_WRAP</strong> - Can&#8217;t explain, it will look like this : <em>cdefgh|abcdefgh|abcdefg</em></li>
</ul>
</dd>
</dl>
</li>
<li><strong>value</strong> - Color of border if border type is <code class="docutils literal"><span class="pre">cv2.BORDER_CONSTANT</span></code></li>
</ul>
</div></blockquote>
<p>Below is a sample code demonstrating all these border types for better understanding:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">BLUE</span> <span class="o">=</span> <span class="p">[</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>

<span class="n">img1</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;opencv_logo.png&#39;</span><span class="p">)</span>

<span class="n">replicate</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">copyMakeBorder</span><span class="p">(</span><span class="n">img1</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">BORDER_REPLICATE</span><span class="p">)</span>
<span class="n">reflect</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">copyMakeBorder</span><span class="p">(</span><span class="n">img1</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">BORDER_REFLECT</span><span class="p">)</span>
<span class="n">reflect101</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">copyMakeBorder</span><span class="p">(</span><span class="n">img1</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">BORDER_REFLECT_101</span><span class="p">)</span>
<span class="n">wrap</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">copyMakeBorder</span><span class="p">(</span><span class="n">img1</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">BORDER_WRAP</span><span class="p">)</span>
<span class="n">constant</span><span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">copyMakeBorder</span><span class="p">(</span><span class="n">img1</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">BORDER_CONSTANT</span><span class="p">,</span><span class="n">value</span><span class="o">=</span><span class="n">BLUE</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">231</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img1</span><span class="p">,</span><span class="s1">&#39;gray&#39;</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;ORIGINAL&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">232</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">replicate</span><span class="p">,</span><span class="s1">&#39;gray&#39;</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;REPLICATE&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">233</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">reflect</span><span class="p">,</span><span class="s1">&#39;gray&#39;</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;REFLECT&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">234</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">reflect101</span><span class="p">,</span><span class="s1">&#39;gray&#39;</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;REFLECT_101&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">235</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">wrap</span><span class="p">,</span><span class="s1">&#39;gray&#39;</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;WRAP&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">236</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">constant</span><span class="p">,</span><span class="s1">&#39;gray&#39;</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;CONSTANT&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>See the result below. (Image is displayed with matplotlib. So RED and BLUE planes will be interchanged):</p>
<blockquote>
<div><img alt="Border Types" class="align-center" src="_images/border.jpg" />
</div></blockquote>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
</div>
</div>
<span id="document-py_tutorials/py_core/py_image_arithmetics/py_image_arithmetics"></span><div class="section" id="arithmetic-operations-on-images">
<span id="image-arithmetics"></span><h4>Arithmetic Operations on Images<a class="headerlink" href="#arithmetic-operations-on-images" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goal">
<h5>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h5>
<blockquote>
<div><ul class="simple">
<li>Learn several arithmetic operations on images like addition, subtraction, bitwise operations etc.</li>
<li>You will learn these functions : <strong>cv2.add()</strong>, <strong>cv2.addWeighted()</strong> etc.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="image-addition">
<h5>Image Addition<a class="headerlink" href="#image-addition" title="Permalink to this headline">¶</a></h5>
<p>You can add two images by OpenCV function, <code class="docutils literal"><span class="pre">cv2.add()</span></code> or simply by numpy operation, <code class="docutils literal"><span class="pre">res</span> <span class="pre">=</span> <span class="pre">img1</span> <span class="pre">+</span> <span class="pre">img2</span></code>. Both images should be of same depth and type, or second image can just be a scalar value.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">There is a difference between OpenCV addition and Numpy addition. OpenCV addition is a saturated operation while Numpy addition is a modulo operation.</p>
</div>
<p>For example, consider below sample:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">([</span><span class="mi">250</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">([</span><span class="mi">10</span><span class="p">])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span> <span class="n">cv2</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span> <span class="c1"># 250+10 = 260 =&gt; 255</span>
<span class="go">[[255]]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span> <span class="n">x</span><span class="o">+</span><span class="n">y</span>          <span class="c1"># 250+10 = 260 % 256 = 4</span>
<span class="go">[4]</span>
</pre></div>
</div>
<p>It will be more visible when you add two images. OpenCV function will provide a better result. So always better stick to OpenCV functions.</p>
</div>
<div class="section" id="image-blending">
<h5>Image Blending<a class="headerlink" href="#image-blending" title="Permalink to this headline">¶</a></h5>
<p>This is also image addition, but different weights are given to images so that it gives a feeling of blending or transparency. Images are added as per the equation below:</p>
<div class="math">
<p><img src="_images/math/8086cd5f33e2aed7d185e1f55fc31ceab4433c2b.png" alt="g(x) = (1 - \alpha)f_{0}(x) + \alpha f_{1}(x)"/></p>
</div><p>By varying <img class="math" src="_images/math/ad59b6e24a4a00ac621801f8d7513d68be654ab5.png" alt="\alpha"/> from <img class="math" src="_images/math/e8b0946e02b57d6440cad75c8e0666f071d5ab3c.png" alt="0 \rightarrow 1"/>, you can perform a cool transition between one image to another.</p>
<p>Here I took two images to blend them together. First image is given a weight of 0.7 and second image is given 0.3. <code class="docutils literal"><span class="pre">cv2.addWeighted()</span></code> applies following equation on the image.</p>
<div class="math">
<p><img src="_images/math/ce1ee966236689be38f566b9fb6bc92812bbd54d.png" alt="dst = \alpha \cdot img1 + \beta \cdot img2 + \gamma"/></p>
</div><p>Here <img class="math" src="_images/math/0ebb67342b546ca42a1c634b1ef03c893c4cdedb.png" alt="\gamma"/> is taken as zero.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">img1</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;ml.png&#39;</span><span class="p">)</span>
<span class="n">img2</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;opencv_logo.jpg&#39;</span><span class="p">)</span>

<span class="n">dst</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">addWeighted</span><span class="p">(</span><span class="n">img1</span><span class="p">,</span><span class="mf">0.7</span><span class="p">,</span><span class="n">img2</span><span class="p">,</span><span class="mf">0.3</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>

<span class="n">cv2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s1">&#39;dst&#39;</span><span class="p">,</span><span class="n">dst</span><span class="p">)</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>
</pre></div>
</div>
<p>Check the result below:</p>
<blockquote>
<div><img alt="Image Blending" class="align-center" src="_images/blending.jpg" />
</div></blockquote>
</div>
<div class="section" id="bitwise-operations">
<h5>Bitwise Operations<a class="headerlink" href="#bitwise-operations" title="Permalink to this headline">¶</a></h5>
<p>This includes bitwise AND, OR, NOT and XOR operations. They will be highly useful while extracting any part of the image (as we will see in coming chapters), defining and working with non-rectangular ROI etc. Below we will see an example on how to change a particular region of an image.</p>
<p>I want to put OpenCV logo above an image. If I add two images, it will change color. If I blend it, I get an transparent effect. But I want it to be opaque. If it was a rectangular region, I could use ROI as we did in last chapter. But OpenCV logo is a not a rectangular shape. So you can do it with bitwise operations as below:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="c1"># Load two images</span>
<span class="n">img1</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;messi5.jpg&#39;</span><span class="p">)</span>
<span class="n">img2</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;opencv_logo.png&#39;</span><span class="p">)</span>

<span class="c1"># I want to put logo on top-left corner, So I create a ROI</span>
<span class="n">rows</span><span class="p">,</span><span class="n">cols</span><span class="p">,</span><span class="n">channels</span> <span class="o">=</span> <span class="n">img2</span><span class="o">.</span><span class="n">shape</span>
<span class="n">roi</span> <span class="o">=</span> <span class="n">img1</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">rows</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="n">cols</span> <span class="p">]</span>

<span class="c1"># Now create a mask of logo and create its inverse mask also</span>
<span class="n">img2gray</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">img2</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2GRAY</span><span class="p">)</span>
<span class="n">ret</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">threshold</span><span class="p">(</span><span class="n">img2gray</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">THRESH_BINARY</span><span class="p">)</span>
<span class="n">mask_inv</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">bitwise_not</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>

<span class="c1"># Now black-out the area of logo in ROI</span>
<span class="n">img1_bg</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">bitwise_and</span><span class="p">(</span><span class="n">roi</span><span class="p">,</span><span class="n">roi</span><span class="p">,</span><span class="n">mask</span> <span class="o">=</span> <span class="n">mask_inv</span><span class="p">)</span>

<span class="c1"># Take only region of logo from logo image.</span>
<span class="n">img2_fg</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">bitwise_and</span><span class="p">(</span><span class="n">img2</span><span class="p">,</span><span class="n">img2</span><span class="p">,</span><span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="p">)</span>

<span class="c1"># Put logo in ROI and modify the main image</span>
<span class="n">dst</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">img1_bg</span><span class="p">,</span><span class="n">img2_fg</span><span class="p">)</span>
<span class="n">img1</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">rows</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="n">cols</span> <span class="p">]</span> <span class="o">=</span> <span class="n">dst</span>

<span class="n">cv2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s1">&#39;res&#39;</span><span class="p">,</span><span class="n">img1</span><span class="p">)</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>
</pre></div>
</div>
<p>See the result below. Left image shows the mask we created. Right image shows the final result. For more understanding, display all the intermediate images in the above code, especially <code class="docutils literal"><span class="pre">img1_bg</span></code> and <code class="docutils literal"><span class="pre">img2_fg</span></code>.</p>
<blockquote>
<div><img alt="Otsu's Thresholding" class="align-center" src="_images/overlay.jpg" />
</div></blockquote>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
<ol class="arabic simple">
<li>Create a slide show of images in a folder with smooth transition between images using <code class="docutils literal"><span class="pre">cv2.addWeighted</span></code> function</li>
</ol>
</div>
</div>
<span id="document-py_tutorials/py_core/py_optimization/py_optimization"></span><div class="section" id="performance-measurement-and-improvement-techniques">
<span id="optimization-techniques"></span><h4>Performance Measurement and Improvement Techniques<a class="headerlink" href="#performance-measurement-and-improvement-techniques" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goal">
<h5>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h5>
<p>In image processing, since you are dealing with large number of operations per second, it is mandatory that your code is not only providing the correct solution, but also in the fastest manner. So in this chapter, you will learn</p>
<blockquote>
<div><ul class="simple">
<li>To measure the performance of your code.</li>
<li>Some tips to improve the performance of your code.</li>
<li>You will see these functions : <strong>cv2.getTickCount</strong>, <strong>cv2.getTickFrequency</strong> etc.</li>
</ul>
</div></blockquote>
<p>Apart from OpenCV, Python also provides a module <strong>time</strong> which is helpful in measuring the time of execution. Another module <strong>profile</strong> helps to get detailed report on the code, like how much time each function in the code took, how many times the function was called etc. But, if you are using IPython, all these features are integrated in an user-friendly manner. We will see some important ones, and for more details, check links in <strong>Additional Resouces</strong> section.</p>
</div>
<div class="section" id="measuring-performance-with-opencv">
<h5>Measuring Performance with OpenCV<a class="headerlink" href="#measuring-performance-with-opencv" title="Permalink to this headline">¶</a></h5>
<p><strong>cv2.getTickCount</strong> function returns the number of clock-cycles after a reference event (like the moment machine was switched ON) to the moment this function is called. So if you call it before and after the function execution, you get number of clock-cycles used to execute a function.</p>
<p><strong>cv2.getTickFrequency</strong> function returns the frequency of clock-cycles, or the number of clock-cycles per second. So to find the time of execution in seconds, you can do following:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">e1</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">getTickCount</span><span class="p">()</span>
<span class="c1"># your code execution</span>
<span class="n">e2</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">getTickCount</span><span class="p">()</span>
<span class="n">time</span> <span class="o">=</span> <span class="p">(</span><span class="n">e2</span> <span class="o">-</span> <span class="n">e1</span><span class="p">)</span><span class="o">/</span> <span class="n">cv2</span><span class="o">.</span><span class="n">getTickFrequency</span><span class="p">()</span>
</pre></div>
</div>
<p>We will demonstrate with following example. Following example apply median filtering with a kernel of odd size ranging from 5 to 49. (Don&#8217;t worry about what will the result look like, that is not our goal):</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">img1</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;messi5.jpg&#39;</span><span class="p">)</span>

<span class="n">e1</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">getTickCount</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">49</span><span class="p">,</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">img1</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">medianBlur</span><span class="p">(</span><span class="n">img1</span><span class="p">,</span><span class="n">i</span><span class="p">)</span>
<span class="n">e2</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">getTickCount</span><span class="p">()</span>
<span class="n">t</span> <span class="o">=</span> <span class="p">(</span><span class="n">e2</span> <span class="o">-</span> <span class="n">e1</span><span class="p">)</span><span class="o">/</span><span class="n">cv2</span><span class="o">.</span><span class="n">getTickFrequency</span><span class="p">()</span>
<span class="nb">print</span> <span class="n">t</span>

<span class="c1"># Result I got is 0.521107655 seconds</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">You can do the same with <code class="docutils literal"><span class="pre">time</span></code> module. Instead of <code class="docutils literal"><span class="pre">cv2.getTickCount</span></code>, use <code class="docutils literal"><span class="pre">time.time()</span></code> function. Then take the difference of two times.</p>
</div>
</div>
<div class="section" id="default-optimization-in-opencv">
<h5>Default Optimization in OpenCV<a class="headerlink" href="#default-optimization-in-opencv" title="Permalink to this headline">¶</a></h5>
<p>Many of the OpenCV functions are optimized using SSE2, AVX etc. It contains unoptimized code also. So if our system support these features, we should exploit them (almost all modern day processors support them). It is enabled by default while compiling. So OpenCV runs the optimized code if it is enabled, else it runs the unoptimized code. You can use <strong>cv2.useOptimized()</strong> to check if it is enabled/disabled and <strong>cv2.setUseOptimized()</strong> to enable/disable it. Let&#8217;s see a simple example.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="c1"># check if optimization is enabled</span>
<span class="n">In</span> <span class="p">[</span><span class="mi">5</span><span class="p">]:</span> <span class="n">cv2</span><span class="o">.</span><span class="n">useOptimized</span><span class="p">()</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">5</span><span class="p">]:</span> <span class="kc">True</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">6</span><span class="p">]:</span> <span class="o">%</span><span class="n">timeit</span> <span class="n">res</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">medianBlur</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="mi">49</span><span class="p">)</span>
<span class="mi">10</span> <span class="n">loops</span><span class="p">,</span> <span class="n">best</span> <span class="n">of</span> <span class="mi">3</span><span class="p">:</span> <span class="mf">34.9</span> <span class="n">ms</span> <span class="n">per</span> <span class="n">loop</span>

<span class="c1"># Disable it</span>
<span class="n">In</span> <span class="p">[</span><span class="mi">7</span><span class="p">]:</span> <span class="n">cv2</span><span class="o">.</span><span class="n">setUseOptimized</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">8</span><span class="p">]:</span> <span class="n">cv2</span><span class="o">.</span><span class="n">useOptimized</span><span class="p">()</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">8</span><span class="p">]:</span> <span class="kc">False</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">9</span><span class="p">]:</span> <span class="o">%</span><span class="n">timeit</span> <span class="n">res</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">medianBlur</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="mi">49</span><span class="p">)</span>
<span class="mi">10</span> <span class="n">loops</span><span class="p">,</span> <span class="n">best</span> <span class="n">of</span> <span class="mi">3</span><span class="p">:</span> <span class="mf">64.1</span> <span class="n">ms</span> <span class="n">per</span> <span class="n">loop</span>
</pre></div>
</div>
<p>See, optimized median filtering is ~2x faster than unoptimized version. If you check its source, you can see median filtering is SIMD optimized. So you can use this to enable optimization at the top of your code (remember it is enabled by default).</p>
</div>
<div class="section" id="measuring-performance-in-ipython">
<h5>Measuring Performance in IPython<a class="headerlink" href="#measuring-performance-in-ipython" title="Permalink to this headline">¶</a></h5>
<p>Sometimes you may need to compare the performance of two similar operations. IPython gives you a magic command <code class="docutils literal"><span class="pre">%timeit</span></code> to perform this. It runs the code several times to get more accurate results. Once again, they are suitable to measure single line codes.</p>
<p>For example, do you know which of the following addition operation is more better, <code class="docutils literal"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">5;</span> <span class="pre">y</span> <span class="pre">=</span> <span class="pre">x**2</span></code>, <code class="docutils literal"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">5;</span> <span class="pre">y</span> <span class="pre">=</span> <span class="pre">x*x</span></code>, <code class="docutils literal"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">np.uint8([5]);</span> <span class="pre">y</span> <span class="pre">=</span> <span class="pre">x*x</span></code> or <code class="docutils literal"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">np.square(x)</span></code> ? We will find it with %timeit in IPython shell.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">In</span> <span class="p">[</span><span class="mi">10</span><span class="p">]:</span> <span class="n">x</span> <span class="o">=</span> <span class="mi">5</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">11</span><span class="p">]:</span> <span class="o">%</span><span class="n">timeit</span> <span class="n">y</span><span class="o">=</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span>
<span class="mi">10000000</span> <span class="n">loops</span><span class="p">,</span> <span class="n">best</span> <span class="n">of</span> <span class="mi">3</span><span class="p">:</span> <span class="mi">73</span> <span class="n">ns</span> <span class="n">per</span> <span class="n">loop</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">12</span><span class="p">]:</span> <span class="o">%</span><span class="n">timeit</span> <span class="n">y</span><span class="o">=</span><span class="n">x</span><span class="o">*</span><span class="n">x</span>
<span class="mi">10000000</span> <span class="n">loops</span><span class="p">,</span> <span class="n">best</span> <span class="n">of</span> <span class="mi">3</span><span class="p">:</span> <span class="mf">58.3</span> <span class="n">ns</span> <span class="n">per</span> <span class="n">loop</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">15</span><span class="p">]:</span> <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">([</span><span class="mi">5</span><span class="p">])</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">17</span><span class="p">]:</span> <span class="o">%</span><span class="n">timeit</span> <span class="n">y</span><span class="o">=</span><span class="n">z</span><span class="o">*</span><span class="n">z</span>
<span class="mi">1000000</span> <span class="n">loops</span><span class="p">,</span> <span class="n">best</span> <span class="n">of</span> <span class="mi">3</span><span class="p">:</span> <span class="mf">1.25</span> <span class="n">us</span> <span class="n">per</span> <span class="n">loop</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">19</span><span class="p">]:</span> <span class="o">%</span><span class="n">timeit</span> <span class="n">y</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<span class="mi">1000000</span> <span class="n">loops</span><span class="p">,</span> <span class="n">best</span> <span class="n">of</span> <span class="mi">3</span><span class="p">:</span> <span class="mf">1.16</span> <span class="n">us</span> <span class="n">per</span> <span class="n">loop</span>
</pre></div>
</div>
<p>You can see that, <code class="docutils literal"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">5</span> <span class="pre">;</span> <span class="pre">y</span> <span class="pre">=</span> <span class="pre">x*x</span></code> is fastest and it is around 20x faster compared to Numpy. If you consider the array creation also, it may reach upto 100x faster. Cool, right? <em>(Numpy devs are working on this issue)</em></p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Python scalar operations are faster than Numpy scalar operations. So for operations including one or two elements, Python scalar is better than Numpy arrays. Numpy takes advantage when size of array is a little bit bigger.</p>
</div>
<p>We will try one more example. This time, we will compare the performance of <strong>cv2.countNonZero()</strong> and <strong>np.count_nonzero()</strong> for same image.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">In</span> <span class="p">[</span><span class="mi">35</span><span class="p">]:</span> <span class="o">%</span><span class="n">timeit</span> <span class="n">z</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">countNonZero</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="mi">100000</span> <span class="n">loops</span><span class="p">,</span> <span class="n">best</span> <span class="n">of</span> <span class="mi">3</span><span class="p">:</span> <span class="mf">15.8</span> <span class="n">us</span> <span class="n">per</span> <span class="n">loop</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">36</span><span class="p">]:</span> <span class="o">%</span><span class="n">timeit</span> <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">count_nonzero</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="mi">1000</span> <span class="n">loops</span><span class="p">,</span> <span class="n">best</span> <span class="n">of</span> <span class="mi">3</span><span class="p">:</span> <span class="mi">370</span> <span class="n">us</span> <span class="n">per</span> <span class="n">loop</span>
</pre></div>
</div>
<p>See, OpenCV function is nearly 25x faster than Numpy function.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Normally, OpenCV functions are faster than Numpy functions. So for same operation, OpenCV functions are preferred. But, there can be exceptions, especially when Numpy works with views instead of copies.</p>
</div>
</div>
<div class="section" id="more-ipython-magic-commands">
<h5>More IPython magic commands<a class="headerlink" href="#more-ipython-magic-commands" title="Permalink to this headline">¶</a></h5>
<p>There are several other magic commands to measure the performance, profiling, line profiling, memory measurement etc. They all are well documented. So only links to those docs are provided here. Interested readers are recommended to try them out.</p>
</div>
<div class="section" id="performance-optimization-techniques">
<h5>Performance Optimization Techniques<a class="headerlink" href="#performance-optimization-techniques" title="Permalink to this headline">¶</a></h5>
<p>There are several techniques and coding methods to exploit maximum performance of Python and Numpy. Only relevant ones are noted here and links are given to important sources. The main thing to be noted here is that, first try to implement the algorithm in a simple manner. Once it is working, profile it, find the bottlenecks and optimize them.</p>
<blockquote>
<div><ol class="arabic simple">
<li>Avoid using loops in Python as far as possible, especially double/triple loops etc. They are inherently slow.</li>
<li>Vectorize the algorithm/code to the maximum possible extent because Numpy and OpenCV are optimized for vector operations.</li>
<li>Exploit the cache coherence.</li>
<li>Never make copies of array unless it is needed. Try to use views instead. Array copying is a costly operation.</li>
</ol>
</div></blockquote>
<p>Even after doing all these operations, if your code is still slow, or use of large loops are inevitable, use additional libraries like Cython to make it faster.</p>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
<ol class="arabic simple">
<li><a class="reference external" href="http://wiki.python.org/moin/PythonSpeed/PerformanceTips">Python Optimization Techniques</a></li>
<li>Scipy Lecture Notes - <a class="reference external" href="http://scipy-lectures.github.io/advanced/advanced_numpy/index.html#advanced-numpy">Advanced Numpy</a></li>
<li><a class="reference external" href="http://pynash.org/2013/03/06/timing-and-profiling.html">Timing and Profiling in IPython</a></li>
</ol>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
</div>
</div>
<span id="document-py_tutorials/py_core/py_maths_tools/py_maths_tools"></span><div class="section" id="mathematical-tools-in-opencv">
<span id="mathematical-tools"></span><h4>Mathematical Tools in OpenCV<a class="headerlink" href="#mathematical-tools-in-opencv" title="Permalink to this headline">¶</a></h4>
</div>
</div>
</div>
<span id="document-py_tutorials/py_imgproc/py_table_of_contents_imgproc/py_table_of_contents_imgproc"></span><div class="section" id="image-processing-in-opencv">
<span id="py-table-of-content-imgproc"></span><h3>Image Processing in OpenCV<a class="headerlink" href="#image-processing-in-opencv" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference internal" href="index.html#converting-colorspaces"><span class="std std-ref">Changing Colorspaces</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="14%" />
<col width="86%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/colorspace.jpg"><img alt="imgproc_1" src="_images/colorspace.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td><p class="first">Learn to change images between different color spaces.</p>
<p class="last">Plus learn to track a colored object in a video.</p>
</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#geometric-transformations"><span class="std std-ref">Geometric Transformations of Images</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="12%" />
<col width="88%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/geometric.jpg"><img alt="imgproc_gt" src="_images/geometric.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Learn to apply different geometric transformations to images like rotation, translation etc.</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#thresholding"><span class="std std-ref">Image Thresholding</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="14%" />
<col width="86%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/thresh.jpg"><img alt="imgproc_2" src="_images/thresh.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Learn to convert images to binary images using global thresholding,
Adaptive thresholding, Otsu&#8217;s binarization etc</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#filtering"><span class="std std-ref">Smoothing Images</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="14%" />
<col width="86%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/blurring.jpg"><img alt="imgproc_4" src="_images/blurring.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Learn to blur the images, filter the images with custom kernels etc.</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#morphological-ops"><span class="std std-ref">Morphological Transformations</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="12%" />
<col width="88%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/morphology.jpg"><img alt="imgproc_12" src="_images/morphology.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Learn about morphological transformations like Erosion, Dilation, Opening, Closing etc</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#gradients"><span class="std std-ref">Image Gradients</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="14%" />
<col width="86%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/gradient.jpg"><img alt="imgproc_5" src="_images/gradient.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Learn to find image gradients, edges etc.</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#canny"><span class="std std-ref">Canny Edge Detection</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="14%" />
<col width="86%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/canny.jpg"><img alt="imgproc_8" src="_images/canny.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Learn to find edges with Canny Edge Detection</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#pyramids"><span class="std std-ref">Image Pyramids</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="15%" />
<col width="85%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/pyramid.png"><img alt="imgproc_14" src="_images/pyramid.png" style="width: 90pt; height: 90pt;" /></a></td>
<td>Learn about image pyramids and how to use them for image blending</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#table-of-content-contours"><span class="std std-ref">Contours in OpenCV</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="14%" />
<col width="86%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/contours.jpg"><img alt="imgproc_3" src="_images/contours.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>All about Contours in OpenCV</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#table-of-content-histograms"><span class="std std-ref">Histograms in OpenCV</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="14%" />
<col width="86%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/histogram.jpg"><img alt="imgproc_6" src="_images/histogram.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>All about histograms in OpenCV</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#table-of-content-transforms"><span class="std std-ref">Image Transforms in OpenCV</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="11%" />
<col width="89%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/transforms.jpg"><img alt="imgproc_7" src="_images/transforms.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Meet different Image Transforms in OpenCV like Fourier Transform, Cosine Transform etc.</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#template-matching"><span class="std std-ref">Template Matching</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="14%" />
<col width="86%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/template.jpg"><img alt="imgproc_9" src="_images/template.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Learn to search for an object in an image using Template Matching</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#hough-lines"><span class="std std-ref">Hough Line Transform</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="15%" />
<col width="85%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/houghlines.jpg"><img alt="imgproc_10" src="_images/houghlines.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Learn to detect lines in an image</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#hough-circles"><span class="std std-ref">Hough Circle Transform</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="15%" />
<col width="85%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/houghcircles.jpg"><img alt="imgproc_11" src="_images/houghcircles.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Learn to detect circles in an image</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#watershed"><span class="std std-ref">Image Segmentation with Watershed Algorithm</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="15%" />
<col width="85%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/watershed.jpg"><img alt="imgproc_13" src="_images/watershed.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Learn to segment images with watershed segmentation</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#grabcut"><span class="std std-ref">Interactive Foreground Extraction using GrabCut Algorithm</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="15%" />
<col width="85%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/grabcut1.jpg"><img alt="imgproc_15" src="_images/grabcut1.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Learn to extract foreground with GrabCut algorithm</td>
</tr>
</tbody>
</table>
</div></blockquote>
<div class="toctree-wrapper compound">
<span id="document-py_tutorials/py_imgproc/py_colorspaces/py_colorspaces"></span><div class="section" id="changing-colorspaces">
<span id="converting-colorspaces"></span><h4>Changing Colorspaces<a class="headerlink" href="#changing-colorspaces" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goal">
<h5>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h5>
<blockquote>
<div><ul class="simple">
<li>In this tutorial, you will learn how to convert images from one color-space to another, like BGR <img class="math" src="_images/math/41a61df92c33a32be9fd6375536739eca63f43ab.png" alt="\leftrightarrow"/> Gray, BGR <img class="math" src="_images/math/41a61df92c33a32be9fd6375536739eca63f43ab.png" alt="\leftrightarrow"/> HSV etc.</li>
<li>In addition to that, we will create an application which extracts a colored object in a video</li>
<li>You will learn following functions : <strong>cv2.cvtColor()</strong>, <strong>cv2.inRange()</strong> etc.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="changing-color-space">
<h5>Changing Color-space<a class="headerlink" href="#changing-color-space" title="Permalink to this headline">¶</a></h5>
<p>There are more than 150 color-space conversion methods available in OpenCV. But we will look into only two which are most widely used ones, BGR <img class="math" src="_images/math/41a61df92c33a32be9fd6375536739eca63f43ab.png" alt="\leftrightarrow"/> Gray and BGR <img class="math" src="_images/math/41a61df92c33a32be9fd6375536739eca63f43ab.png" alt="\leftrightarrow"/> HSV.</p>
<p>For color conversion, we use the function <code class="docutils literal"><span class="pre">cv2.cvtColor(input_image,</span> <span class="pre">flag)</span></code> where <code class="docutils literal"><span class="pre">flag</span></code> determines the type of conversion.</p>
<p>For BGR <img class="math" src="_images/math/a9c4c6156d25f42923975ce449aadad9848ed7dc.png" alt="\rightarrow"/> Gray conversion we use the flags <code class="docutils literal"><span class="pre">cv2.COLOR_BGR2GRAY</span></code>. Similarly for BGR <img class="math" src="_images/math/a9c4c6156d25f42923975ce449aadad9848ed7dc.png" alt="\rightarrow"/> HSV, we use the flag <code class="docutils literal"><span class="pre">cv2.COLOR_BGR2HSV</span></code>. To get other flags, just run following commands in your Python terminal :</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">flags</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">(</span><span class="n">cv2</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;COLOR_&#39;</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span> <span class="n">flags</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">For HSV, Hue range is [0,179], Saturation range is [0,255] and Value range is [0,255]. Different softwares use different scales. So if you are comparing OpenCV values with them, you need to normalize these ranges.</p>
</div>
</div>
<div class="section" id="object-tracking">
<h5>Object Tracking<a class="headerlink" href="#object-tracking" title="Permalink to this headline">¶</a></h5>
<p>Now we know how to convert BGR image to HSV, we can use this to extract a colored object. In HSV, it is more easier to represent a color than RGB color-space. In our application, we will try to extract a blue colored object. So here is the method:</p>
<blockquote>
<div><ul class="simple">
<li>Take each frame of the video</li>
<li>Convert from BGR to HSV color-space</li>
<li>We threshold the HSV image for a range of blue color</li>
<li>Now extract the blue object alone, we can do whatever on that image we want.</li>
</ul>
</div></blockquote>
<p>Below is the code which are commented in detail :</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">cap</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">VideoCapture</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="k">while</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>

    <span class="c1"># Take each frame</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">frame</span> <span class="o">=</span> <span class="n">cap</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>

    <span class="c1"># Convert BGR to HSV</span>
    <span class="n">hsv</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2HSV</span><span class="p">)</span>

    <span class="c1"># define range of blue color in HSV</span>
    <span class="n">lower_blue</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">110</span><span class="p">,</span><span class="mi">50</span><span class="p">,</span><span class="mi">50</span><span class="p">])</span>
    <span class="n">upper_blue</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">130</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="mi">255</span><span class="p">])</span>

    <span class="c1"># Threshold the HSV image to get only blue colors</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">inRange</span><span class="p">(</span><span class="n">hsv</span><span class="p">,</span> <span class="n">lower_blue</span><span class="p">,</span> <span class="n">upper_blue</span><span class="p">)</span>

    <span class="c1"># Bitwise-AND mask and original image</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">bitwise_and</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span><span class="n">frame</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span> <span class="n">mask</span><span class="p">)</span>

    <span class="n">cv2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s1">&#39;frame&#39;</span><span class="p">,</span><span class="n">frame</span><span class="p">)</span>
    <span class="n">cv2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s1">&#39;mask&#39;</span><span class="p">,</span><span class="n">mask</span><span class="p">)</span>
    <span class="n">cv2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s1">&#39;res&#39;</span><span class="p">,</span><span class="n">res</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span> <span class="o">&amp;</span> <span class="mh">0xFF</span>
    <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">27</span><span class="p">:</span>
        <span class="k">break</span>

<span class="n">cv2</span><span class="o">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>
</pre></div>
</div>
<p>Below image shows tracking of the blue object:</p>
<blockquote>
<div><a class="reference internal image-reference" href="_images/frame.jpg"><img alt="Blue Object Tracking" class="align-center" src="_images/frame.jpg" style="width: 780pt;" /></a>
</div></blockquote>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">There are some noises in the image. We will see how to remove them in later chapters.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This is the simplest method in object tracking. Once you learn functions of contours, you can do plenty of things like find centroid of this object and use it to track the object, draw diagrams just by moving your hand in front of camera and many other funny stuffs.</p>
</div>
<div class="section" id="how-to-find-hsv-values-to-track">
<h6>How to find HSV values to track?<a class="headerlink" href="#how-to-find-hsv-values-to-track" title="Permalink to this headline">¶</a></h6>
<p>This is a common question found in <a class="reference external" href="www.stackoverflow.com">stackoverflow.com</a>. It is very simple and you can use the same function, <cite>cv2.cvtColor()</cite>. Instead of passing an image, you just pass the BGR values you want. For example, to find the HSV value of Green, try following commands in Python terminal:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">green</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">([[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span> <span class="p">]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hsv_green</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">green</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2HSV</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span> <span class="n">hsv_green</span>
<span class="go">[[[ 60 255 255]]]</span>
</pre></div>
</div>
<p>Now you take [H-10, 100,100] and [H+10, 255, 255] as lower bound and upper bound respectively. Apart from this method, you can use any image editing tools like GIMP or any online converters to find these values, but don&#8217;t forget to adjust the HSV ranges.</p>
</div>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
<ol class="arabic simple">
<li>Try to find a way to extract more than one colored objects, for eg, extract red, blue, green objects simultaneously.</li>
</ol>
</div>
</div>
<span id="document-py_tutorials/py_imgproc/py_thresholding/py_thresholding"></span><div class="section" id="image-thresholding">
<span id="thresholding"></span><h4>Image Thresholding<a class="headerlink" href="#image-thresholding" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goal">
<h5>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h5>
<div class="enumeratevisibleitemswithsquare docutils container">
<ul class="simple">
<li>In this tutorial, you will learn Simple thresholding, Adaptive thresholding, Otsu&#8217;s thresholding etc.</li>
<li>You will learn these functions : <strong>cv2.threshold</strong>, <strong>cv2.adaptiveThreshold</strong> etc.</li>
</ul>
</div>
</div>
<div class="section" id="simple-thresholding">
<h5>Simple Thresholding<a class="headerlink" href="#simple-thresholding" title="Permalink to this headline">¶</a></h5>
<p>Here, the matter is straight forward. If pixel value is greater than a threshold value, it is assigned one value (may be white), else it is assigned another value (may be black). The function used is <strong>cv2.threshold</strong>. First argument is the source image, which <strong>should be a grayscale image</strong>. Second argument is the threshold value which is used to classify the pixel values. Third argument is the maxVal which represents the value to be given if pixel value is more than (sometimes less than) the threshold value. OpenCV provides different styles of thresholding and it is decided by the fourth parameter of the function. Different types are:</p>
<blockquote>
<div><ul class="simple">
<li>cv2.THRESH_BINARY</li>
<li>cv2.THRESH_BINARY_INV</li>
<li>cv2.THRESH_TRUNC</li>
<li>cv2.THRESH_TOZERO</li>
<li>cv2.THRESH_TOZERO_INV</li>
</ul>
</div></blockquote>
<p>Documentation clearly explain what each type is meant for. Please check out the documentation.</p>
<p>Two outputs are obtained. First one is a <strong>retval</strong> which will be explained later. Second output is our <strong>thresholded image</strong>.</p>
<p>Code :</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;gradient.png&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ret</span><span class="p">,</span><span class="n">thresh1</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">threshold</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="mi">127</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">THRESH_BINARY</span><span class="p">)</span>
<span class="n">ret</span><span class="p">,</span><span class="n">thresh2</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">threshold</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="mi">127</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">THRESH_BINARY_INV</span><span class="p">)</span>
<span class="n">ret</span><span class="p">,</span><span class="n">thresh3</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">threshold</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="mi">127</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">THRESH_TRUNC</span><span class="p">)</span>
<span class="n">ret</span><span class="p">,</span><span class="n">thresh4</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">threshold</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="mi">127</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">THRESH_TOZERO</span><span class="p">)</span>
<span class="n">ret</span><span class="p">,</span><span class="n">thresh5</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">threshold</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="mi">127</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">THRESH_TOZERO_INV</span><span class="p">)</span>

<span class="n">titles</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Original Image&#39;</span><span class="p">,</span><span class="s1">&#39;BINARY&#39;</span><span class="p">,</span><span class="s1">&#39;BINARY_INV&#39;</span><span class="p">,</span><span class="s1">&#39;TRUNC&#39;</span><span class="p">,</span><span class="s1">&#39;TOZERO&#39;</span><span class="p">,</span><span class="s1">&#39;TOZERO_INV&#39;</span><span class="p">]</span>
<span class="n">images</span> <span class="o">=</span> <span class="p">[</span><span class="n">img</span><span class="p">,</span> <span class="n">thresh1</span><span class="p">,</span> <span class="n">thresh2</span><span class="p">,</span> <span class="n">thresh3</span><span class="p">,</span> <span class="n">thresh4</span><span class="p">,</span> <span class="n">thresh5</span><span class="p">]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">titles</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([]),</span><span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">To plot multiple images, we have used <cite>plt.subplot()</cite> function. Please checkout Matplotlib docs for more details.</p>
</div>
<p>Result is given below :</p>
<blockquote>
<div><img alt="Simple Thresholding" class="align-center" src="_images/threshold.jpg" />
</div></blockquote>
</div>
<div class="section" id="adaptive-thresholding">
<h5>Adaptive Thresholding<a class="headerlink" href="#adaptive-thresholding" title="Permalink to this headline">¶</a></h5>
<p>In the previous section, we used a global value as threshold value. But it may not be good in all the conditions where image has different lighting conditions in different areas. In that case, we go for adaptive thresholding. In this, the algorithm calculate the threshold for a small regions of the image. So we get different thresholds for different regions of the same image and it gives us better results for images with varying illumination.</p>
<p>It has three ‘special’ input params and only one output argument.</p>
<dl class="docutils">
<dt><strong>Adaptive Method</strong> - It decides how thresholding value is calculated.</dt>
<dd><ul class="first last simple">
<li>cv2.ADAPTIVE_THRESH_MEAN_C : threshold value is the mean of neighbourhood area.</li>
<li>cv2.ADAPTIVE_THRESH_GAUSSIAN_C : threshold value is the weighted sum of neighbourhood values where weights are a gaussian window.</li>
</ul>
</dd>
</dl>
<p><strong>Block Size</strong> - It decides the size of neighbourhood area.</p>
<p><strong>C</strong> - It is just a constant which is subtracted from the mean or weighted mean calculated.</p>
<p>Below piece of code compares global thresholding and adaptive thresholding for an image with varying illumination:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;dave.jpg&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">medianBlur</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>

<span class="n">ret</span><span class="p">,</span><span class="n">th1</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">threshold</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="mi">127</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">THRESH_BINARY</span><span class="p">)</span>
<span class="n">th2</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">adaptiveThreshold</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">ADAPTIVE_THRESH_MEAN_C</span><span class="p">,</span>\
            <span class="n">cv2</span><span class="o">.</span><span class="n">THRESH_BINARY</span><span class="p">,</span><span class="mi">11</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">th3</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">adaptiveThreshold</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">ADAPTIVE_THRESH_GAUSSIAN_C</span><span class="p">,</span>\
            <span class="n">cv2</span><span class="o">.</span><span class="n">THRESH_BINARY</span><span class="p">,</span><span class="mi">11</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>

<span class="n">titles</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Original Image&#39;</span><span class="p">,</span> <span class="s1">&#39;Global Thresholding (v = 127)&#39;</span><span class="p">,</span>
            <span class="s1">&#39;Adaptive Mean Thresholding&#39;</span><span class="p">,</span> <span class="s1">&#39;Adaptive Gaussian Thresholding&#39;</span><span class="p">]</span>
<span class="n">images</span> <span class="o">=</span> <span class="p">[</span><span class="n">img</span><span class="p">,</span> <span class="n">th1</span><span class="p">,</span> <span class="n">th2</span><span class="p">,</span> <span class="n">th3</span><span class="p">]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">titles</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([]),</span><span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Result :</p>
<blockquote>
<div><img alt="Adaptive Thresholding" class="align-center" src="_images/ada_threshold.jpg" />
</div></blockquote>
</div>
<div class="section" id="otsus-binarization">
<h5>Otsu’s Binarization<a class="headerlink" href="#otsus-binarization" title="Permalink to this headline">¶</a></h5>
<p>In the first section, I told you there is a second parameter <strong>retVal</strong>. Its use comes when we go for Otsu’s Binarization. So what is it?</p>
<p>In global thresholding, we used an arbitrary value for threshold value, right? So, how can we know a value we selected is good or not? Answer is, trial and error method. But consider a <strong>bimodal image</strong> (<em>In simple words, bimodal image is an image whose histogram has two peaks</em>). For that image, we can approximately take a value in the middle of those peaks as threshold value, right ? That is what Otsu binarization does. So in simple words, it automatically calculates a threshold value from image histogram for a bimodal image. (For images which are not bimodal, binarization won’t be accurate.)</p>
<p>For this, our cv2.threshold() function is used, but pass an extra flag, <cite>cv2.THRESH_OTSU</cite>. <strong>For threshold value, simply pass zero</strong>. Then the algorithm finds the optimal threshold value and returns you as the second output, <code class="docutils literal"><span class="pre">retVal</span></code>. If Otsu thresholding is not used, retVal is same as the threshold value you used.</p>
<p>Check out below example. Input image is a noisy image. In first case, I applied global thresholding for a value of 127. In second case, I applied Otsu’s thresholding directly. In third case, I filtered image with a 5x5 gaussian kernel to remove the noise, then applied Otsu thresholding. See how noise filtering improves the result.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;noisy2.png&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># global thresholding</span>
<span class="n">ret1</span><span class="p">,</span><span class="n">th1</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">threshold</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="mi">127</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">THRESH_BINARY</span><span class="p">)</span>

<span class="c1"># Otsu&#39;s thresholding</span>
<span class="n">ret2</span><span class="p">,</span><span class="n">th2</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">threshold</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">THRESH_BINARY</span><span class="o">+</span><span class="n">cv2</span><span class="o">.</span><span class="n">THRESH_OTSU</span><span class="p">)</span>

<span class="c1"># Otsu&#39;s thresholding after Gaussian filtering</span>
<span class="n">blur</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">GaussianBlur</span><span class="p">(</span><span class="n">img</span><span class="p">,(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ret3</span><span class="p">,</span><span class="n">th3</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">threshold</span><span class="p">(</span><span class="n">blur</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">THRESH_BINARY</span><span class="o">+</span><span class="n">cv2</span><span class="o">.</span><span class="n">THRESH_OTSU</span><span class="p">)</span>

<span class="c1"># plot all the images and their histograms</span>
<span class="n">images</span> <span class="o">=</span> <span class="p">[</span><span class="n">img</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">th1</span><span class="p">,</span>
          <span class="n">img</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">th2</span><span class="p">,</span>
          <span class="n">blur</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">th3</span><span class="p">]</span>
<span class="n">titles</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Original Noisy Image&#39;</span><span class="p">,</span><span class="s1">&#39;Histogram&#39;</span><span class="p">,</span><span class="s1">&#39;Global Thresholding (v=127)&#39;</span><span class="p">,</span>
          <span class="s1">&#39;Original Noisy Image&#39;</span><span class="p">,</span><span class="s1">&#39;Histogram&#39;</span><span class="p">,</span><span class="s2">&quot;Otsu&#39;s Thresholding&quot;</span><span class="p">,</span>
          <span class="s1">&#39;Gaussian filtered Image&#39;</span><span class="p">,</span><span class="s1">&#39;Histogram&#39;</span><span class="p">,</span><span class="s2">&quot;Otsu&#39;s Thresholding&quot;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">i</span><span class="o">*</span><span class="mi">3</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="mi">3</span><span class="p">],</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">titles</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="mi">3</span><span class="p">]),</span> <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([]),</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">i</span><span class="o">*</span><span class="mi">3</span><span class="o">+</span><span class="mi">2</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span><span class="mi">256</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">titles</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="mi">3</span><span class="o">+</span><span class="mi">1</span><span class="p">]),</span> <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([]),</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">i</span><span class="o">*</span><span class="mi">3</span><span class="o">+</span><span class="mi">3</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="mi">3</span><span class="o">+</span><span class="mi">2</span><span class="p">],</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">titles</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="mi">3</span><span class="o">+</span><span class="mi">2</span><span class="p">]),</span> <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([]),</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Result :</p>
<blockquote>
<div><img alt="Otsu's Thresholding" class="align-center" src="_images/otsu.jpg" />
</div></blockquote>
<div class="section" id="how-otsu-s-binarization-works">
<h6>How Otsu&#8217;s Binarization Works?<a class="headerlink" href="#how-otsu-s-binarization-works" title="Permalink to this headline">¶</a></h6>
<p>This section demonstrates a Python implementation of Otsu&#8217;s binarization to show how it works actually. If you are not interested, you can skip this.</p>
<p>Since we are working with bimodal images, Otsu&#8217;s algorithm tries to find a threshold value (t) which minimizes the <strong>weighted within-class variance</strong> given by the relation :</p>
<div class="math">
<p><img src="_images/math/f816e1d17a5f5959c19df9f31ef3189a88de49a4.png" alt="\sigma_w^2(t) = q_1(t)\sigma_1^2(t)+q_2(t)\sigma_2^2(t)"/></p>
</div><p>where</p>
<div class="math">
<p><img src="_images/math/588a446d9a466ac81104cf588afec8ec9d713ffe.png" alt="q_1(t) = \sum_{i=1}^{t} P(i) \quad \&amp; \quad q_1(t) = \sum_{i=t+1}^{I} P(i)

\mu_1(t) = \sum_{i=1}^{t} \frac{iP(i)}{q_1(t)} \quad \&amp; \quad \mu_2(t) = \sum_{i=t+1}^{I} \frac{iP(i)}{q_2(t)}

\sigma_1^2(t) = \sum_{i=1}^{t} [i-\mu_1(t)]^2 \frac{P(i)}{q_1(t)} \quad \&amp; \quad \sigma_2^2(t) = \sum_{i=t+1}^{I} [i-\mu_1(t)]^2 \frac{P(i)}{q_2(t)}"/></p>
</div><p>It actually finds a value of t which lies in between two peaks such that variances to both classes are minimum. It can be simply implemented in Python as follows:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;noisy2.png&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">blur</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">GaussianBlur</span><span class="p">(</span><span class="n">img</span><span class="p">,(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># find normalized_histogram, and its cumulative distribution function</span>
<span class="n">hist</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">calcHist</span><span class="p">([</span><span class="n">blur</span><span class="p">],[</span><span class="mi">0</span><span class="p">],</span><span class="kc">None</span><span class="p">,[</span><span class="mi">256</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">256</span><span class="p">])</span>
<span class="n">hist_norm</span> <span class="o">=</span> <span class="n">hist</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span><span class="o">/</span><span class="n">hist</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<span class="n">Q</span> <span class="o">=</span> <span class="n">hist_norm</span><span class="o">.</span><span class="n">cumsum</span><span class="p">()</span>

<span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">256</span><span class="p">)</span>

<span class="n">fn_min</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
<span class="n">thresh</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">256</span><span class="p">):</span>
    <span class="n">p1</span><span class="p">,</span><span class="n">p2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hsplit</span><span class="p">(</span><span class="n">hist_norm</span><span class="p">,[</span><span class="n">i</span><span class="p">])</span> <span class="c1"># probabilities</span>
    <span class="n">q1</span><span class="p">,</span><span class="n">q2</span> <span class="o">=</span> <span class="n">Q</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">Q</span><span class="p">[</span><span class="mi">255</span><span class="p">]</span><span class="o">-</span><span class="n">Q</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="c1"># cum sum of classes</span>
    <span class="n">b1</span><span class="p">,</span><span class="n">b2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hsplit</span><span class="p">(</span><span class="n">bins</span><span class="p">,[</span><span class="n">i</span><span class="p">])</span> <span class="c1"># weights</span>

    <span class="c1"># finding means and variances</span>
    <span class="n">m1</span><span class="p">,</span><span class="n">m2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">p1</span><span class="o">*</span><span class="n">b1</span><span class="p">)</span><span class="o">/</span><span class="n">q1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">p2</span><span class="o">*</span><span class="n">b2</span><span class="p">)</span><span class="o">/</span><span class="n">q2</span>
    <span class="n">v1</span><span class="p">,</span><span class="n">v2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(((</span><span class="n">b1</span><span class="o">-</span><span class="n">m1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="n">p1</span><span class="p">)</span><span class="o">/</span><span class="n">q1</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(((</span><span class="n">b2</span><span class="o">-</span><span class="n">m2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="n">p2</span><span class="p">)</span><span class="o">/</span><span class="n">q2</span>

    <span class="c1"># calculates the minimization function</span>
    <span class="n">fn</span> <span class="o">=</span> <span class="n">v1</span><span class="o">*</span><span class="n">q1</span> <span class="o">+</span> <span class="n">v2</span><span class="o">*</span><span class="n">q2</span>
    <span class="k">if</span> <span class="n">fn</span> <span class="o">&lt;</span> <span class="n">fn_min</span><span class="p">:</span>
        <span class="n">fn_min</span> <span class="o">=</span> <span class="n">fn</span>
        <span class="n">thresh</span> <span class="o">=</span> <span class="n">i</span>

<span class="c1"># find otsu&#39;s threshold value with OpenCV function</span>
<span class="n">ret</span><span class="p">,</span> <span class="n">otsu</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">threshold</span><span class="p">(</span><span class="n">blur</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">THRESH_BINARY</span><span class="o">+</span><span class="n">cv2</span><span class="o">.</span><span class="n">THRESH_OTSU</span><span class="p">)</span>
<span class="nb">print</span> <span class="n">thresh</span><span class="p">,</span><span class="n">ret</span>
</pre></div>
</div>
<p><em>(Some of the functions may be new here, but we will cover them in coming chapters)</em></p>
</div>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
<ol class="arabic simple">
<li>Digital Image Processing, Rafael C. Gonzalez</li>
</ol>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
<ol class="arabic simple">
<li>There are some optimizations available for Otsu&#8217;s binarization. You can search and implement it.</li>
</ol>
</div>
</div>
<span id="document-py_tutorials/py_imgproc/py_geometric_transformations/py_geometric_transformations"></span><div class="section" id="geometric-transformations-of-images">
<span id="geometric-transformations"></span><h4>Geometric Transformations of Images<a class="headerlink" href="#geometric-transformations-of-images" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goals">
<h5>Goals<a class="headerlink" href="#goals" title="Permalink to this headline">¶</a></h5>
<blockquote>
<div><ul class="simple">
<li>Learn to apply different geometric transformation to images like translation, rotation, affine transformation etc.</li>
<li>You will see these functions: <strong>cv2.getPerspectiveTransform</strong></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="transformations">
<h5>Transformations<a class="headerlink" href="#transformations" title="Permalink to this headline">¶</a></h5>
<p>OpenCV provides two transformation functions, <strong>cv2.warpAffine</strong> and <strong>cv2.warpPerspective</strong>, with which you can have all kinds of transformations. <strong>cv2.warpAffine</strong> takes a 2x3 transformation matrix while <strong>cv2.warpPerspective</strong> takes a 3x3 transformation matrix as input.</p>
<div class="section" id="scaling">
<h6>Scaling<a class="headerlink" href="#scaling" title="Permalink to this headline">¶</a></h6>
<p>Scaling is just resizing of the image. OpenCV comes with a function <strong>cv2.resize()</strong> for this purpose. The size of the image can be specified manually, or you can specify the scaling factor. Different interpolation methods are used. Preferable interpolation methods are <strong>cv2.INTER_AREA</strong> for shrinking and <strong>cv2.INTER_CUBIC</strong> (slow) &amp; <strong>cv2.INTER_LINEAR</strong> for zooming. By default, interpolation method used is <strong>cv2.INTER_LINEAR</strong> for all resizing purposes. You can resize an input image either of following methods:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;messi5.jpg&#39;</span><span class="p">)</span>

<span class="n">res</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="kc">None</span><span class="p">,</span><span class="n">fx</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">fy</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">interpolation</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">INTER_CUBIC</span><span class="p">)</span>

<span class="c1">#OR</span>

<span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">img</span><span class="p">,(</span><span class="mi">2</span><span class="o">*</span><span class="n">width</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">height</span><span class="p">),</span> <span class="n">interpolation</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">INTER_CUBIC</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="translation">
<h6>Translation<a class="headerlink" href="#translation" title="Permalink to this headline">¶</a></h6>
<p>Translation is the shifting of object&#8217;s location. If you know the shift in (x,y) direction, let it be <img class="math" src="_images/math/0e83abfc665fcb85f416011f46e40cfc43a29fc8.png" alt="(t_x,t_y)"/>, you can create the transformation matrix <img class="math" src="_images/math/45fa95c4085ad196cc95e3ab4f3488ac3076e758.png" alt="\textbf{M}"/> as follows:</p>
<div class="math">
<p><img src="_images/math/22fe551f03b8e94f1a7a75731a660f0163030540.png" alt="M = \begin{bmatrix} 1 &amp; 0 &amp; t_x \\ 0 &amp; 1 &amp; t_y  \end{bmatrix}"/></p>
</div><p>You can take make it into a Numpy array of type <code class="docutils literal"><span class="pre">np.float32</span></code> and pass it into <strong>cv2.warpAffine()</strong> function. See below example for a shift of (100,50):</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;messi5.jpg&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">rows</span><span class="p">,</span><span class="n">cols</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">shape</span>

<span class="n">M</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">100</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">50</span><span class="p">]])</span>
<span class="n">dst</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">warpAffine</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">M</span><span class="p">,(</span><span class="n">cols</span><span class="p">,</span><span class="n">rows</span><span class="p">))</span>

<span class="n">cv2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s1">&#39;img&#39;</span><span class="p">,</span><span class="n">dst</span><span class="p">)</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Third argument of the <strong>cv2.warpAffine()</strong> function is the size of the output image, which should be in the form of <strong>(width, height)</strong>. Remember width = number of columns, and height = number of rows.</p>
</div>
<p>See the result below:</p>
<blockquote>
<div><img alt="Translation" class="align-center" src="_images/translation.jpg" />
</div></blockquote>
</div>
<div class="section" id="rotation">
<h6>Rotation<a class="headerlink" href="#rotation" title="Permalink to this headline">¶</a></h6>
<p>Rotation of an image for an angle <img class="math" src="_images/math/a9cfbeb8ebee1f365919e147a79e242dcb67ee5d.png" alt="\theta"/> is achieved by the transformation matrix of the form</p>
<div class="math">
<p><img src="_images/math/f3a6bed945808a1f3a9df71b260f68f8e653af95.png" alt="M = \begin{bmatrix} cos\theta &amp; -sin\theta \\ sin\theta &amp; cos\theta   \end{bmatrix}"/></p>
</div><p>But OpenCV provides scaled rotation with adjustable center of rotation so that you can rotate at any location you prefer. Modified transformation matrix is given by</p>
<div class="math">
<p><img src="_images/math/91ff2b9b1db0760f4764631010749e594cdf5f5f.png" alt="\begin{bmatrix} \alpha &amp;  \beta &amp; (1- \alpha )  \cdot center.x -  \beta \cdot center.y \\ - \beta &amp;  \alpha &amp;  \beta \cdot center.x + (1- \alpha )  \cdot center.y \end{bmatrix}"/></p>
</div><p>where:</p>
<div class="math">
<p><img src="_images/math/383c254fc602c57a059a8296357f90fdf421aee7.png" alt="\begin{array}{l} \alpha =  scale \cdot \cos \theta , \\ \beta =  scale \cdot \sin \theta \end{array}"/></p>
</div><p>To find this transformation matrix, OpenCV provides a function, <strong>cv2.getRotationMatrix2D</strong>. Check below example which rotates the image by 90 degree with respect to center without any scaling.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;messi5.jpg&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">rows</span><span class="p">,</span><span class="n">cols</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">shape</span>

<span class="n">M</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">getRotationMatrix2D</span><span class="p">((</span><span class="n">cols</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span><span class="n">rows</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span><span class="mi">90</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">dst</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">warpAffine</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">M</span><span class="p">,(</span><span class="n">cols</span><span class="p">,</span><span class="n">rows</span><span class="p">))</span>
</pre></div>
</div>
<p>See the result:</p>
<blockquote>
<div><img alt="Rotation of Image" class="align-center" src="_images/rotation.jpg" />
</div></blockquote>
</div>
<div class="section" id="affine-transformation">
<h6>Affine Transformation<a class="headerlink" href="#affine-transformation" title="Permalink to this headline">¶</a></h6>
<p>In affine transformation, all parallel lines in the original image will still be parallel in the output image. To find the transformation matrix, we need three points from input image and their corresponding locations in output image. Then <strong>cv2.getAffineTransform</strong> will create a 2x3 matrix which is to be passed to <strong>cv2.warpAffine</strong>.</p>
<p>Check below example, and also look at the points I selected (which are marked in Green color):</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;drawing.png&#39;</span><span class="p">)</span>
<span class="n">rows</span><span class="p">,</span><span class="n">cols</span><span class="p">,</span><span class="n">ch</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">shape</span>

<span class="n">pts1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">([[</span><span class="mi">50</span><span class="p">,</span><span class="mi">50</span><span class="p">],[</span><span class="mi">200</span><span class="p">,</span><span class="mi">50</span><span class="p">],[</span><span class="mi">50</span><span class="p">,</span><span class="mi">200</span><span class="p">]])</span>
<span class="n">pts2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">([[</span><span class="mi">10</span><span class="p">,</span><span class="mi">100</span><span class="p">],[</span><span class="mi">200</span><span class="p">,</span><span class="mi">50</span><span class="p">],[</span><span class="mi">100</span><span class="p">,</span><span class="mi">250</span><span class="p">]])</span>

<span class="n">M</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">getAffineTransform</span><span class="p">(</span><span class="n">pts1</span><span class="p">,</span><span class="n">pts2</span><span class="p">)</span>

<span class="n">dst</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">warpAffine</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">M</span><span class="p">,(</span><span class="n">cols</span><span class="p">,</span><span class="n">rows</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Input&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">dst</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Output&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>See the result:</p>
<blockquote>
<div><img alt="Affine Transformation" class="align-center" src="_images/affine.jpg" />
</div></blockquote>
</div>
<div class="section" id="perspective-transformation">
<h6>Perspective Transformation<a class="headerlink" href="#perspective-transformation" title="Permalink to this headline">¶</a></h6>
<p>For perspective transformation, you need a 3x3 transformation matrix. Straight lines will remain straight even after the transformation. To find this transformation matrix, you need 4 points on the input image and corresponding points on the output image. Among these 4 points, 3 of them should not be collinear. Then transformation matrix can be found by the function <strong>cv2.getPerspectiveTransform</strong>. Then apply <strong>cv2.warpPerspective</strong> with this 3x3 transformation matrix.</p>
<p>See the code below:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;sudokusmall.png&#39;</span><span class="p">)</span>
<span class="n">rows</span><span class="p">,</span><span class="n">cols</span><span class="p">,</span><span class="n">ch</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">shape</span>

<span class="n">pts1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">([[</span><span class="mi">56</span><span class="p">,</span><span class="mi">65</span><span class="p">],[</span><span class="mi">368</span><span class="p">,</span><span class="mi">52</span><span class="p">],[</span><span class="mi">28</span><span class="p">,</span><span class="mi">387</span><span class="p">],[</span><span class="mi">389</span><span class="p">,</span><span class="mi">390</span><span class="p">]])</span>
<span class="n">pts2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">300</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">300</span><span class="p">],[</span><span class="mi">300</span><span class="p">,</span><span class="mi">300</span><span class="p">]])</span>

<span class="n">M</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">getPerspectiveTransform</span><span class="p">(</span><span class="n">pts1</span><span class="p">,</span><span class="n">pts2</span><span class="p">)</span>

<span class="n">dst</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">warpPerspective</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">M</span><span class="p">,(</span><span class="mi">300</span><span class="p">,</span><span class="mi">300</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Input&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">dst</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Output&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Result:</p>
<blockquote>
<div><img alt="Perspective Transformation" class="align-center" src="_images/perspective.jpg" />
</div></blockquote>
</div>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
<ol class="arabic simple">
<li>&#8220;Computer Vision: Algorithms and Applications&#8221;, Richard Szeliski</li>
</ol>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
</div>
</div>
<span id="document-py_tutorials/py_imgproc/py_filtering/py_filtering"></span><div class="section" id="smoothing-images">
<span id="filtering"></span><h4>Smoothing Images<a class="headerlink" href="#smoothing-images" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goals">
<h5>Goals<a class="headerlink" href="#goals" title="Permalink to this headline">¶</a></h5>
<dl class="docutils">
<dt>Learn to:</dt>
<dd><ul class="first last simple">
<li>Blur  imagess with various low pass filters</li>
<li>Apply custom-made filters to images (2D convolution)</li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="d-convolution-image-filtering">
<h5>2D Convolution ( Image Filtering )<a class="headerlink" href="#d-convolution-image-filtering" title="Permalink to this headline">¶</a></h5>
<p>As for one-dimensional signals, images also can be filtered with various low-pass filters (LPF), high-pass filters (HPF), etc. A LPF helps in removing noise, or blurring the image. A HPF filters helps in finding edges in an image.</p>
<p>OpenCV provides a function, <strong>cv2.filter2D()</strong>, to convolve a kernel with an image. As an example, we will try an averaging filter on an image. A 5x5 averaging filter kernel can be defined as follows:</p>
<div class="math">
<p><img src="_images/math/220e403e44b16ea8e05d350c4ce69e9aedff5bd1.png" alt="K =  \frac{1}{25} \begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1  \\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \end{bmatrix}"/></p>
</div><p>Filtering with the above kernel results in the following being performed: for each pixel, a 5x5 window is centered on this pixel,  all pixels falling within this window are summed up, and the result is then divided by 25. This equates to computing the average of the pixel values inside that window. This operation is performed for all the pixels in the image to produce the output filtered image. Try this code and check the result:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;opencv_logo.png&#39;</span><span class="p">)</span>

<span class="n">kernel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">/</span><span class="mi">25</span>
<span class="n">dst</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">filter2D</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">kernel</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Original&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([]),</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">dst</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Averaging&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([]),</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Result:</p>
<blockquote>
<div><img alt="Averaging Filter" class="align-center" src="_images/filter.jpg" />
</div></blockquote>
</div>
<div class="section" id="image-blurring-image-smoothing">
<h5>Image Blurring (Image Smoothing)<a class="headerlink" href="#image-blurring-image-smoothing" title="Permalink to this headline">¶</a></h5>
<p>Image blurring is achieved by convolving the image with a low-pass filter kernel. It is useful for removing noise. It actually removes high frequency content (e.g: noise, edges) from the image resulting in edges being blurred when this is filter is applied. (Well, there are blurring techniques which do not blur edges). OpenCV provides mainly four types of blurring techniques.</p>
<div class="section" id="averaging">
<h6>1. Averaging<a class="headerlink" href="#averaging" title="Permalink to this headline">¶</a></h6>
<p>This is done by convolving the image with a normalized box filter. It simply takes the average of all the pixels under kernel area and replaces the central element with this average. This is done by the function <strong>cv2.blur()</strong> or <strong>cv2.boxFilter()</strong>. Check the docs for more details about the kernel. We should specify the width and height of kernel. A 3x3 normalized box filter would look like this:</p>
<div class="math">
<p><img src="_images/math/42f61cdcb41615a23af32b0fd95e674090afdc8d.png" alt="K =  \frac{1}{9} \begin{bmatrix} 1 &amp; 1 &amp; 1  \\ 1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 1 \end{bmatrix}"/></p>
</div><div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">If you don&#8217;t want to use a normalized box filter, use <strong>cv2.boxFilter()</strong> and pass the argument <code class="docutils literal"><span class="pre">normalize=False</span></code> to the function.</p>
</div>
<p>Check the sample demo below with a kernel of 5x5 size:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;opencv_logo.png&#39;</span><span class="p">)</span>

<span class="n">blur</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">blur</span><span class="p">(</span><span class="n">img</span><span class="p">,(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Original&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([]),</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">blur</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Blurred&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([]),</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Result:</p>
<blockquote>
<div><img alt="Averaging Filter" class="align-center" src="_images/blur.jpg" />
</div></blockquote>
</div>
<div class="section" id="gaussian-filtering">
<h6>2. Gaussian Filtering<a class="headerlink" href="#gaussian-filtering" title="Permalink to this headline">¶</a></h6>
<p>In this approach, instead of a box filter consisting of equal filter coefficients, a Gaussian kernel is used. It is done with the function, <strong>cv2.GaussianBlur()</strong>. We should specify the width and height of the kernel which should be positive and odd. We also should specify the standard deviation in the X and Y directions, sigmaX and sigmaY respectively. If only sigmaX is specified, sigmaY is taken as equal to sigmaX. If both are given as zeros, they are calculated from the kernel size. Gaussian filtering is highly effective in removing Gaussian noise from the image.</p>
<p>If you want, you can create a Gaussian kernel with the function, <strong>cv2.getGaussianKernel()</strong>.</p>
<p>The above code can be modified for Gaussian blurring:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">blur</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">GaussianBlur</span><span class="p">(</span><span class="n">img</span><span class="p">,(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>Result:</p>
<blockquote>
<div><img alt="Gaussian Blurring" class="align-center" src="_images/gaussian.jpg" />
</div></blockquote>
</div>
<div class="section" id="median-filtering">
<h6>3. Median Filtering<a class="headerlink" href="#median-filtering" title="Permalink to this headline">¶</a></h6>
<p>Here, the function <strong>cv2.medianBlur()</strong> computes the median of all the pixels under the kernel window and the central pixel is replaced with this median value. This is highly effective in removing salt-and-pepper noise. One interesting thing to note is that, in the Gaussian and box filters, the filtered value for the central element can be a value which may not exist in the original image. However this is not the case in median filtering, since the central element is always replaced by some pixel value in the image. This reduces the noise effectively. The kernel size must be a positive odd integer.</p>
<p>In this demo, we add a 50% noise to our original image and use a median filter. Check the result:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">median</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">medianBlur</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
<p>Result:</p>
<blockquote>
<div><img alt="Median Blurring" class="align-center" src="_images/median.jpg" />
</div></blockquote>
</div>
<div class="section" id="bilateral-filtering">
<h6>4. Bilateral Filtering<a class="headerlink" href="#bilateral-filtering" title="Permalink to this headline">¶</a></h6>
<p>As we noted, the filters we presented earlier tend to blur edges. This is not the case for the bilateral filter, <strong>cv2.bilateralFilter()</strong>, which was defined for, and is highly effective at noise removal while preserving edges. But the operation is slower compared to other filters. We already saw that a Gaussian filter takes the a neighborhood around the pixel and finds its Gaussian weighted average. This Gaussian filter is a function of space alone, that is, nearby pixels are considered while filtering. It does not consider whether pixels have almost the same intensity value and does not consider whether the pixel lies on an edge or not. The resulting effect is that Gaussian filters tend to blur edges, which is undesirable.</p>
<p>The bilateral filter also uses a Gaussian filter in the space domain, but it also uses one more (multiplicative) Gaussian filter component which is a function of pixel intensity differences. The Gaussian function of space makes sure that only pixels are &#8216;spatial neighbors&#8217; are considered for filtering, while the Gaussian component applied in the intensity domain (a Gaussian function of intensity differences) ensures that only those pixels with intensities similar to that of the central pixel (&#8216;intensity neighbors&#8217;) are included to compute the blurred intensity value. As a result, this method preserves edges, since for pixels lying near edges, neighboring pixels placed on the other side of the edge, and therefore exhibiting large intensity variations when compared to the central pixel, will not be included for blurring.</p>
<p>The sample below demonstrates the use of bilateral filtering (For details on arguments, see the OpenCV docs).</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">blur</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">bilateralFilter</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">75</span><span class="p">,</span><span class="mi">75</span><span class="p">)</span>
</pre></div>
</div>
<p>Result:</p>
<blockquote>
<div><img alt="Bilateral Filtering" class="align-center" src="_images/bilateral.jpg" />
</div></blockquote>
<p>Note that the texture on the surface is gone, but edges are still preserved.</p>
</div>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
<ol class="arabic simple">
<li>Details about the <a class="reference external" href="http://people.csail.mit.edu/sparis/bf_course/">bilateral filtering can be found at</a></li>
</ol>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
<p>Take an image, add Gaussian noise and salt and pepper noise, compare the effect of blurring via box, Gaussian, median and bilateral filters for both noisy images, as you change the level of noise.</p>
</div>
</div>
<span id="document-py_tutorials/py_imgproc/py_morphological_ops/py_morphological_ops"></span><div class="section" id="morphological-transformations">
<span id="morphological-ops"></span><h4>Morphological Transformations<a class="headerlink" href="#morphological-transformations" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goal">
<h5>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h5>
<dl class="docutils">
<dt>In this chapter,</dt>
<dd><ul class="first last simple">
<li>We will learn different morphological operations like Erosion, Dilation, Opening, Closing etc.</li>
<li>We will see different functions like : <strong>cv2.erode()</strong>, <strong>cv2.dilate()</strong>, <strong>cv2.morphologyEx()</strong> etc.</li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="theory">
<h5>Theory<a class="headerlink" href="#theory" title="Permalink to this headline">¶</a></h5>
<p>Morphological transformations are some simple operations based on the image shape. It is normally performed on binary images. It needs two inputs, one is our original image, second one is called <strong>structuring element</strong> or <strong>kernel</strong> which decides the nature of operation. Two basic morphological operators are Erosion and Dilation. Then its variant forms like Opening, Closing, Gradient etc also comes into play. We will see them one-by-one with help of following image:</p>
<blockquote>
<div><img alt="Input Image" class="align-center" src="_images/j.png" />
</div></blockquote>
<div class="section" id="erosion">
<h6>1. Erosion<a class="headerlink" href="#erosion" title="Permalink to this headline">¶</a></h6>
<p>The basic idea of erosion is just like soil erosion only, it erodes away the boundaries of foreground object (Always try to keep foreground in white). So what does it do? The kernel slides through the image (as in 2D convolution). A pixel in the original image (either 1 or 0) will be considered 1 only if all the pixels under the kernel is 1, otherwise it is eroded (made to zero).</p>
<p>So what happends is that, all the pixels near boundary will be discarded depending upon the size of kernel. So the thickness or size of the foreground object decreases or simply white region decreases in the image. It is useful for removing small white noises (as we have seen in colorspace chapter), detach two connected objects etc.</p>
<p>Here, as an example, I would use a 5x5 kernel with full of ones. Let&#8217;s see it how it works:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;j.png&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">kernel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="n">erosion</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">erode</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">kernel</span><span class="p">,</span><span class="n">iterations</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>Result:</p>
<blockquote>
<div><img alt="Erosion" class="align-center" src="_images/erosion.png" />
</div></blockquote>
</div>
<div class="section" id="dilation">
<h6>2. Dilation<a class="headerlink" href="#dilation" title="Permalink to this headline">¶</a></h6>
<p>It is just opposite of erosion. Here, a pixel element is &#8216;1&#8217; if atleast one pixel under the kernel is &#8216;1&#8217;. So it increases the white region in the image or size of foreground object increases. Normally, in cases like noise removal, erosion is followed by dilation. Because, erosion removes white noises, but it also shrinks our object. So we dilate it. Since noise is gone, they won&#8217;t come back, but our object area increases. It is also useful in joining broken parts of an object.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">dilation</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">dilate</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">kernel</span><span class="p">,</span><span class="n">iterations</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>Result:</p>
<blockquote>
<div><img alt="Dilation" class="align-center" src="_images/dilation.png" />
</div></blockquote>
</div>
<div class="section" id="opening">
<h6>3. Opening<a class="headerlink" href="#opening" title="Permalink to this headline">¶</a></h6>
<p>Opening is just another name of <strong>erosion followed by dilation</strong>. It is useful in removing noise, as we explained above. Here we use the function, <strong>cv2.morphologyEx()</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">opening</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">morphologyEx</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">MORPH_OPEN</span><span class="p">,</span> <span class="n">kernel</span><span class="p">)</span>
</pre></div>
</div>
<p>Result:</p>
<blockquote>
<div><img alt="Opening" class="align-center" src="_images/opening.png" />
</div></blockquote>
</div>
<div class="section" id="closing">
<h6>4. Closing<a class="headerlink" href="#closing" title="Permalink to this headline">¶</a></h6>
<p>Closing is reverse of Opening, <strong>Dilation followed by Erosion</strong>. It is useful in closing small holes inside the foreground objects, or small black points on the object.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">closing</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">morphologyEx</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">MORPH_CLOSE</span><span class="p">,</span> <span class="n">kernel</span><span class="p">)</span>
</pre></div>
</div>
<p>Result:</p>
<blockquote>
<div><img alt="Closing" class="align-center" src="_images/closing.png" />
</div></blockquote>
</div>
<div class="section" id="morphological-gradient">
<h6>5. Morphological Gradient<a class="headerlink" href="#morphological-gradient" title="Permalink to this headline">¶</a></h6>
<p>It is the difference between dilation and erosion of an image.</p>
<p>The result will look like the outline of the object.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">gradient</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">morphologyEx</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">MORPH_GRADIENT</span><span class="p">,</span> <span class="n">kernel</span><span class="p">)</span>
</pre></div>
</div>
<p>Result:</p>
<blockquote>
<div><img alt="Gradient" class="align-center" src="_images/gradient.png" />
</div></blockquote>
</div>
<div class="section" id="top-hat">
<h6>6. Top Hat<a class="headerlink" href="#top-hat" title="Permalink to this headline">¶</a></h6>
<p>It is the difference between input image and Opening of the image. Below example is done for a 9x9 kernel.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">tophat</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">morphologyEx</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">MORPH_TOPHAT</span><span class="p">,</span> <span class="n">kernel</span><span class="p">)</span>
</pre></div>
</div>
<p>Result:</p>
<blockquote>
<div><img alt="Top Hat" class="align-center" src="_images/tophat.png" />
</div></blockquote>
</div>
<div class="section" id="black-hat">
<h6>7. Black Hat<a class="headerlink" href="#black-hat" title="Permalink to this headline">¶</a></h6>
<p>It is the difference between the closing of the input image and input image.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">blackhat</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">morphologyEx</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">MORPH_BLACKHAT</span><span class="p">,</span> <span class="n">kernel</span><span class="p">)</span>
</pre></div>
</div>
<p>Result:</p>
<blockquote>
<div><img alt="Black Hat" class="align-center" src="_images/blackhat.png" />
</div></blockquote>
</div>
</div>
<div class="section" id="structuring-element">
<h5>Structuring Element<a class="headerlink" href="#structuring-element" title="Permalink to this headline">¶</a></h5>
<p>We manually created a structuring elements in the previous examples with help of Numpy. It is rectangular shape. But in some cases, you may need elliptical/circular shaped kernels. So for this purpose, OpenCV has a function, <strong>cv2.getStructuringElement()</strong>. You just pass the shape and size of the kernel, you get the desired kernel.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1"># Rectangular Kernel</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">cv2</span><span class="o">.</span><span class="n">getStructuringElement</span><span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">MORPH_RECT</span><span class="p">,(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">uint8</span><span class="p">)</span>

<span class="c1"># Elliptical Kernel</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">cv2</span><span class="o">.</span><span class="n">getStructuringElement</span><span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">MORPH_ELLIPSE</span><span class="p">,(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">uint8</span><span class="p">)</span>

<span class="c1"># Cross-shaped Kernel</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">cv2</span><span class="o">.</span><span class="n">getStructuringElement</span><span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">MORPH_CROSS</span><span class="p">,(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">uint8</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
<ol class="arabic simple">
<li><a class="reference external" href="http://homepages.inf.ed.ac.uk/rbf/HIPR2/morops.htm">Morphological Operations</a> at HIPR2</li>
</ol>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
</div>
</div>
<span id="document-py_tutorials/py_imgproc/py_gradients/py_gradients"></span><div class="section" id="image-gradients">
<span id="gradients"></span><h4>Image Gradients<a class="headerlink" href="#image-gradients" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goal">
<h5>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h5>
<p>In this chapter, we will learn to:</p>
<blockquote>
<div><ul class="simple">
<li>Find Image gradients, edges etc</li>
<li>We will see following functions : <strong>cv2.Sobel()</strong>, <strong>cv2.Scharr()</strong>, <strong>cv2.Laplacian()</strong> etc</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="theory">
<h5>Theory<a class="headerlink" href="#theory" title="Permalink to this headline">¶</a></h5>
<p>OpenCV provides three types of gradient filters or High-pass filters, Sobel, Scharr and Laplacian. We will see each one of them.</p>
<div class="section" id="sobel-and-scharr-derivatives">
<h6>1. Sobel and Scharr Derivatives<a class="headerlink" href="#sobel-and-scharr-derivatives" title="Permalink to this headline">¶</a></h6>
<p>Sobel operators is a joint Gausssian smoothing plus differentiation operation, so it is more resistant to noise. You can specify the direction of derivatives to be taken, vertical or horizontal (by the arguments, <code class="docutils literal"><span class="pre">yorder</span></code> and <code class="docutils literal"><span class="pre">xorder</span></code> respectively). You can also specify the size of kernel by the argument <code class="docutils literal"><span class="pre">ksize</span></code>. If ksize = -1, a 3x3 Scharr filter is used which gives better results than 3x3 Sobel filter. Please see the docs for kernels used.</p>
</div>
<div class="section" id="laplacian-derivatives">
<h6>2. Laplacian Derivatives<a class="headerlink" href="#laplacian-derivatives" title="Permalink to this headline">¶</a></h6>
<p>It calculates the Laplacian of the image given by the relation, <img class="math" src="_images/math/ff76e0f08a2d7bfa076121be5239e35df97573c8.png" alt="\Delta src = \frac{\partial ^2{src}}{\partial x^2} + \frac{\partial ^2{src}}{\partial y^2}"/> where each derivative is found using Sobel derivatives. If <code class="docutils literal"><span class="pre">ksize</span> <span class="pre">=</span> <span class="pre">1</span></code>, then following kernel is used for filtering:</p>
<div class="math">
<p><img src="_images/math/2e4e208edcbed72b60c09a9e8eb8c00c4b21dbd6.png" alt="kernel = \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; -4 &amp; 1 \\ 0 &amp; 1 &amp; 0  \end{bmatrix}"/></p>
</div></div>
</div>
<div class="section" id="code">
<h5>Code<a class="headerlink" href="#code" title="Permalink to this headline">¶</a></h5>
<p>Below code shows all operators in a single diagram. All kernels are of 5x5 size. Depth of output image is passed -1 to get the result in np.uint8 type.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;dave.jpg&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>

<span class="n">laplacian</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">Laplacian</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">CV_64F</span><span class="p">)</span>
<span class="n">sobelx</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">Sobel</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">CV_64F</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">ksize</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">sobely</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">Sobel</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">CV_64F</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">ksize</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">cmap</span> <span class="o">=</span> <span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Original&#39;</span><span class="p">),</span> <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([]),</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">laplacian</span><span class="p">,</span><span class="n">cmap</span> <span class="o">=</span> <span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Laplacian&#39;</span><span class="p">),</span> <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([]),</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">sobelx</span><span class="p">,</span><span class="n">cmap</span> <span class="o">=</span> <span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Sobel X&#39;</span><span class="p">),</span> <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([]),</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">sobely</span><span class="p">,</span><span class="n">cmap</span> <span class="o">=</span> <span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Sobel Y&#39;</span><span class="p">),</span> <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([]),</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Result:</p>
<blockquote>
<div><img alt="Image Gradients" class="align-center" src="_images/gradients.jpg" />
</div></blockquote>
</div>
<div class="section" id="one-important-matter">
<h5>One Important Matter!<a class="headerlink" href="#one-important-matter" title="Permalink to this headline">¶</a></h5>
<p>In our last example, output datatype is cv2.CV_8U or np.uint8. But there is a slight problem with that. Black-to-White transition is taken as Positive slope (it has a positive value) while White-to-Black transition is taken as a Negative slope (It has negative value). So when you convert data to np.uint8, all negative slopes are made zero. In simple words, you miss that edge.</p>
<p>If you want to detect both edges, better option is to keep the output datatype to some higher forms, like cv2.CV_16S, cv2.CV_64F etc, take its absolute value and then convert back to cv2.CV_8U. Below code demonstrates this procedure for a horizontal Sobel filter and difference in results.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;box.png&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Output dtype = cv2.CV_8U</span>
<span class="n">sobelx8u</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">Sobel</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">CV_8U</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">ksize</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># Output dtype = cv2.CV_64F. Then take its absolute and convert to cv2.CV_8U</span>
<span class="n">sobelx64f</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">Sobel</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">CV_64F</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">ksize</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">abs_sobel64f</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">absolute</span><span class="p">(</span><span class="n">sobelx64f</span><span class="p">)</span>
<span class="n">sobel_8u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">(</span><span class="n">abs_sobel64f</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">cmap</span> <span class="o">=</span> <span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Original&#39;</span><span class="p">),</span> <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([]),</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">sobelx8u</span><span class="p">,</span><span class="n">cmap</span> <span class="o">=</span> <span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Sobel CV_8U&#39;</span><span class="p">),</span> <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([]),</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">sobel_8u</span><span class="p">,</span><span class="n">cmap</span> <span class="o">=</span> <span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Sobel abs(CV_64F)&#39;</span><span class="p">),</span> <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([]),</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Check the result below:</p>
<blockquote>
<div><img alt="Double Edges" class="align-center" src="_images/double_edge.jpg" />
</div></blockquote>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
</div>
</div>
<span id="document-py_tutorials/py_imgproc/py_canny/py_canny"></span><div class="section" id="canny-edge-detection">
<span id="canny"></span><h4>Canny Edge Detection<a class="headerlink" href="#canny-edge-detection" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goal">
<h5>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h5>
<p>In this chapter, we will learn about</p>
<blockquote>
<div><ul class="simple">
<li>Concept of Canny edge detection</li>
<li>OpenCV functions for that : <strong>cv2.Canny()</strong></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="theory">
<h5>Theory<a class="headerlink" href="#theory" title="Permalink to this headline">¶</a></h5>
<p>Canny Edge Detection is a popular edge detection algorithm. It was developed by John F. Canny in 1986. It is a multi-stage algorithm and we will go through each stages.</p>
<ol class="arabic simple">
<li><strong>Noise Reduction</strong></li>
</ol>
<p>Since edge detection is susceptible to noise in the image, first step is to remove the noise in the image with a 5x5 Gaussian filter. We have already seen this in previous chapters.</p>
<ol class="arabic simple" start="2">
<li><strong>Finding Intensity Gradient of the Image</strong></li>
</ol>
<p>Smoothened image is then filtered with a Sobel kernel in both horizontal and vertical direction to get first derivative in horizontal direction (<img class="math" src="_images/math/435f8877ff8b95bab1e4f3bd435524ffee5c14e6.png" alt="G_x"/>) and vertical direction (<img class="math" src="_images/math/8bc80afcfda0a10c756110ee5b0dfdfa5c137392.png" alt="G_y"/>). From these two images, we can find edge gradient and direction for each pixel as follows:</p>
<div class="math">
<p><img src="_images/math/fc9752466c9c38d07985d62e86946489e23c61e2.png" alt="Edge\_Gradient \; (G) = \sqrt{G_x^2 + G_y^2}

Angle \; (\theta) = \tan^{-1} \bigg(\frac{G_y}{G_x}\bigg)"/></p>
</div><p>Gradient direction is always perpendicular to edges. It is rounded to one of four angles representing vertical, horizontal and two diagonal directions.</p>
<ol class="arabic simple" start="3">
<li><strong>Non-maximum Suppression</strong></li>
</ol>
<p>After getting gradient magnitude and direction, a full scan of image is done to remove any unwanted pixels which may not constitute the edge. For this, at every pixel, pixel is checked if it is a local maximum in its neighborhood in the direction of gradient. Check the image below:</p>
<blockquote>
<div><img alt="Non-Maximum Suppression" class="align-center" src="_images/nms.jpg" />
</div></blockquote>
<p>Point A is on the edge ( in vertical direction). Gradient direction is normal to the edge. Point B and C are in gradient directions. So point A is checked with point B and C to see if it forms a local maximum. If so, it is considered for next stage, otherwise, it is suppressed ( put to zero).</p>
<p>In short, the result you get is a binary image with &#8220;thin edges&#8221;.</p>
<ol class="arabic simple" start="4">
<li><strong>Hysteresis Thresholding</strong></li>
</ol>
<p>This stage decides which are all edges are really edges and which are not. For this, we need two threshold values, <cite>minVal</cite> and <cite>maxVal</cite>. Any edges with intensity gradient more than <cite>maxVal</cite> are sure to be edges and those below <cite>minVal</cite> are sure to be non-edges, so discarded. Those who lie between these two thresholds are classified edges or non-edges based on their connectivity. If they are connected to &#8220;sure-edge&#8221; pixels, they are considered to be part of edges. Otherwise, they are also discarded. See the image below:</p>
<blockquote>
<div><img alt="Hysteresis Thresholding" class="align-center" src="_images/hysteresis.jpg" />
</div></blockquote>
<p>The edge A is above the <cite>maxVal</cite>, so considered as &#8220;sure-edge&#8221;. Although edge C is below <cite>maxVal</cite>, it is connected to edge A, so that also considered as valid edge and we get that full curve. But edge B, although it is above <cite>minVal</cite> and is in same region as that of edge C, it is not connected to any &#8220;sure-edge&#8221;, so that is discarded. So it is very important that we have to select <cite>minVal</cite> and <cite>maxVal</cite> accordingly to get the correct result.</p>
<p>This stage also removes small pixels noises on the assumption that edges are long lines.</p>
<p>So what we finally get is strong edges in the image.</p>
</div>
<div class="section" id="canny-edge-detection-in-opencv">
<h5>Canny Edge Detection in OpenCV<a class="headerlink" href="#canny-edge-detection-in-opencv" title="Permalink to this headline">¶</a></h5>
<p>OpenCV puts all the above in single function, <strong>cv2.Canny()</strong>. We will see how to use it. First argument is our input image. Second and third arguments are our <cite>minVal</cite> and <cite>maxVal</cite> respectively. Third argument is <cite>aperture_size</cite>. It is the size of Sobel kernel used for find image gradients. By default it is 3. Last argument is <cite>L2gradient</cite> which specifies the equation for finding gradient magnitude. If it is <code class="docutils literal"><span class="pre">True</span></code>, it uses the equation mentioned above which is more accurate, otherwise it uses this function: <img class="math" src="_images/math/559f1d19fb3ffb98feccf9e5931edc0f73e1f26e.png" alt="Edge\_Gradient \; (G) = |G_x| + |G_y|"/>. By default, it is <code class="docutils literal"><span class="pre">False</span></code>.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;messi5.jpg&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">edges</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">Canny</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">200</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">cmap</span> <span class="o">=</span> <span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Original Image&#39;</span><span class="p">),</span> <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([]),</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">edges</span><span class="p">,</span><span class="n">cmap</span> <span class="o">=</span> <span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Edge Image&#39;</span><span class="p">),</span> <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([]),</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>See the result below:</p>
<blockquote>
<div><img alt="Canny Edge Detection" class="align-center" src="_images/canny1.jpg" />
</div></blockquote>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
<ol class="arabic simple">
<li>Canny edge detector at <a class="reference external" href="http://en.wikipedia.org/wiki/Canny_edge_detector">Wikipedia</a></li>
<li><a class="reference external" href="http://dasl.mem.drexel.edu/alumni/bGreen/www.pages.drexel.edu/_weg22/can_tut.html">Canny Edge Detection Tutorial</a> by Bill Green, 2002.</li>
</ol>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
<ol class="arabic simple">
<li>Write a small application to find the Canny edge detection whose threshold values can be varied using two trackbars. This way, you can understand the effect of threshold values.</li>
</ol>
</div>
</div>
<span id="document-py_tutorials/py_imgproc/py_pyramids/py_pyramids"></span><div class="section" id="image-pyramids">
<span id="pyramids"></span><h4>Image Pyramids<a class="headerlink" href="#image-pyramids" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goal">
<h5>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h5>
<dl class="docutils">
<dt>In this chapter,</dt>
<dd><ul class="first last simple">
<li>We will learn about Image Pyramids</li>
<li>We will use Image pyramids to create a new fruit, &#8220;Orapple&#8221;</li>
<li>We will see these functions: <strong>cv2.pyrUp()</strong>, <strong>cv2.pyrDown()</strong></li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="theory">
<h5>Theory<a class="headerlink" href="#theory" title="Permalink to this headline">¶</a></h5>
<p>Normally, we used to work with an image of constant size. But in some occassions, we need to work with images of different resolution of the same image. For example, while searching for something in an image, like face, we are not sure at what size the object will be present in the image. In that case, we will need to create a set of images with different resolution and search for object in all the images. These set of images with different resolution are called Image Pyramids (because when they are kept in a stack with biggest image at bottom and smallest image at top look like a pyramid).</p>
<p>There are two kinds of Image Pyramids. 1) Gaussian Pyramid and 2) Laplacian Pyramids</p>
<p>Higher level (Low resolution) in a Gaussian Pyramid is formed by removing consecutive rows and columns in Lower level (higher resolution) image. Then each pixel in higher level is formed by the contribution from 5 pixels in underlying level with gaussian weights. By doing so, a <img class="math" src="_images/math/079287797417e8d6eaa8727e42778ca8ccb4ed98.png" alt="M \times N"/> image becomes <img class="math" src="_images/math/6bb265c6aa9317e94ad819f1f208bf92b70b4fe8.png" alt="M/2 \times N/2"/> image. So area reduces to one-fourth of original area. It is called an Octave. The same pattern continues as we go upper in pyramid (ie, resolution decreases). Similarly while expanding, area becomes 4 times in each level. We can find Gaussian pyramids using <strong>cv2.pyrDown()</strong> and <strong>cv2.pyrUp()</strong> functions.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;messi5.jpg&#39;</span><span class="p">)</span>
<span class="n">lower_reso</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">pyrDown</span><span class="p">(</span><span class="n">higher_reso</span><span class="p">)</span>
</pre></div>
</div>
<p>Below is the 4 levels in an image pyramid.</p>
<blockquote>
<div><img alt="Gaussian Pyramid" class="align-center" src="_images/messipyr.jpg" />
</div></blockquote>
<p>Now you can go down the image pyramid with <strong>cv2.pyrUp()</strong> function.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">higher_reso2</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">pyrUp</span><span class="p">(</span><span class="n">lower_reso</span><span class="p">)</span>
</pre></div>
</div>
<p>Remember, <cite>higher_reso2</cite> is not equal to <cite>higher_reso</cite>, because once you decrease the resolution, you loose the information. Below image is 3 level down the pyramid created from smallest image in previous case. Compare it with original image:</p>
<blockquote>
<div><img alt="Gaussian Pyramid" class="align-center" src="_images/messiup.jpg" />
</div></blockquote>
<p>Laplacian Pyramids are formed from the Gaussian Pyramids. There is no exclusive function for that. Laplacian pyramid images are like edge images only. Most of its elements are zeros. They are used in image compression. A level in Laplacian Pyramid is formed by the difference between that level in Gaussian Pyramid and expanded version of its upper level in Gaussian Pyramid. The three levels of a Laplacian level will look like below (contrast is adjusted to enhance the contents):</p>
<blockquote>
<div><img alt="Laplacian Pyramid" class="align-center" src="_images/lap.jpg" />
</div></blockquote>
</div>
<div class="section" id="image-blending-using-pyramids">
<h5>Image Blending using Pyramids<a class="headerlink" href="#image-blending-using-pyramids" title="Permalink to this headline">¶</a></h5>
<p>One application of Pyramids is Image Blending. For example, in image stitching, you will need to stack two images together, but it may not look good due to discontinuities between images. In that case, image blending with Pyramids gives you seamless blending without leaving much data in the images. One classical example of this is the blending of two fruits, Orange and Apple. See the result now itself to understand what I am saying:</p>
<blockquote>
<div><img alt="Pyramid Blending" class="align-center" src="_images/orapple.jpg" />
</div></blockquote>
<p>Please check first reference in additional resources, it has full diagramatic details on image blending, Laplacian Pyramids etc. Simply it is done as follows:</p>
<blockquote>
<div><ol class="arabic simple">
<li>Load the two images of apple and orange</li>
<li>Find the Gaussian Pyramids for apple and orange (in this particular example, number of levels is 6)</li>
<li>From Gaussian Pyramids, find their Laplacian Pyramids</li>
<li>Now join the left half of apple and right half of orange in each levels of Laplacian Pyramids</li>
<li>Finally from this joint image pyramids, reconstruct the original image.</li>
</ol>
</div></blockquote>
<p>Below is the full code. (For sake of simplicity, each step is done separately which may take more memory. You can optimize it if you want so).</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span><span class="o">,</span><span class="nn">sys</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;apple.jpg&#39;</span><span class="p">)</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;orange.jpg&#39;</span><span class="p">)</span>

<span class="c1"># generate Gaussian pyramid for A</span>
<span class="n">G</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">gpA</span> <span class="o">=</span> <span class="p">[</span><span class="n">G</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
    <span class="n">G</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">pyrDown</span><span class="p">(</span><span class="n">G</span><span class="p">)</span>
    <span class="n">gpA</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">G</span><span class="p">)</span>

<span class="c1"># generate Gaussian pyramid for B</span>
<span class="n">G</span> <span class="o">=</span> <span class="n">B</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">gpB</span> <span class="o">=</span> <span class="p">[</span><span class="n">G</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
    <span class="n">G</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">pyrDown</span><span class="p">(</span><span class="n">G</span><span class="p">)</span>
    <span class="n">gpB</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">G</span><span class="p">)</span>

<span class="c1"># generate Laplacian Pyramid for A</span>
<span class="n">lpA</span> <span class="o">=</span> <span class="p">[</span><span class="n">gpA</span><span class="p">[</span><span class="mi">5</span><span class="p">]]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">GE</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">pyrUp</span><span class="p">(</span><span class="n">gpA</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">L</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">subtract</span><span class="p">(</span><span class="n">gpA</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">GE</span><span class="p">)</span>
    <span class="n">lpA</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>

<span class="c1"># generate Laplacian Pyramid for B</span>
<span class="n">lpB</span> <span class="o">=</span> <span class="p">[</span><span class="n">gpB</span><span class="p">[</span><span class="mi">5</span><span class="p">]]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">GE</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">pyrUp</span><span class="p">(</span><span class="n">gpB</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">L</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">subtract</span><span class="p">(</span><span class="n">gpB</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">GE</span><span class="p">)</span>
    <span class="n">lpB</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>

<span class="c1"># Now add left and right halves of images in each level</span>
<span class="n">LS</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">la</span><span class="p">,</span><span class="n">lb</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">lpA</span><span class="p">,</span><span class="n">lpB</span><span class="p">):</span>
    <span class="n">rows</span><span class="p">,</span><span class="n">cols</span><span class="p">,</span><span class="n">dpt</span> <span class="o">=</span> <span class="n">la</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">ls</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">la</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="n">cols</span><span class="o">/</span><span class="mi">2</span><span class="p">],</span> <span class="n">lb</span><span class="p">[:,</span><span class="n">cols</span><span class="o">/</span><span class="mi">2</span><span class="p">:]))</span>
    <span class="n">LS</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ls</span><span class="p">)</span>

<span class="c1"># now reconstruct</span>
<span class="n">ls_</span> <span class="o">=</span> <span class="n">LS</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">6</span><span class="p">):</span>
    <span class="n">ls_</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">pyrUp</span><span class="p">(</span><span class="n">ls_</span><span class="p">)</span>
    <span class="n">ls_</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">ls_</span><span class="p">,</span> <span class="n">LS</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

<span class="c1"># image with direct connecting each half</span>
<span class="n">real</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">A</span><span class="p">[:,:</span><span class="n">cols</span><span class="o">/</span><span class="mi">2</span><span class="p">],</span><span class="n">B</span><span class="p">[:,</span><span class="n">cols</span><span class="o">/</span><span class="mi">2</span><span class="p">:]))</span>

<span class="n">cv2</span><span class="o">.</span><span class="n">imwrite</span><span class="p">(</span><span class="s1">&#39;Pyramid_blending2.jpg&#39;</span><span class="p">,</span><span class="n">ls_</span><span class="p">)</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">imwrite</span><span class="p">(</span><span class="s1">&#39;Direct_blending.jpg&#39;</span><span class="p">,</span><span class="n">real</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
<ol class="arabic simple">
<li><a class="reference external" href="http://pages.cs.wisc.edu/~csverma/CS766_09/ImageMosaic/imagemosaic.html">Image Blending</a></li>
</ol>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
</div>
</div>
<span id="document-py_tutorials/py_imgproc/py_contours/py_table_of_contents_contours/py_table_of_contents_contours"></span><div class="section" id="contours-in-opencv">
<span id="table-of-content-contours"></span><h4>Contours in OpenCV<a class="headerlink" href="#contours-in-opencv" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><a class="reference internal" href="index.html#contours-getting-started"><span class="std std-ref">Contours : Getting Started</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="14%" />
<col width="86%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/contour_starting.jpg"><img alt="contour_1" src="_images/contour_starting.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Learn to find and draw Contours</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#contour-features"><span class="std std-ref">Contour Features</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="11%" />
<col width="89%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/contour_features.jpg"><img alt="contour_2" src="_images/contour_features.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Learn to find different features of contours like area, perimeter, bounding rectangle etc.</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#contour-properties"><span class="std std-ref">Contour Properties</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="12%" />
<col width="88%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/contour_properties.jpg"><img alt="contour_3" src="_images/contour_properties.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Learn to find different properties of contours like Solidity, Mean Intensity etc.</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#contours-more-functions"><span class="std std-ref">Contours : More Functions</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="12%" />
<col width="88%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/contour_defects.jpg"><img alt="contour_4" src="_images/contour_defects.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Learn to find convexity defects, pointPolygonTest, match different shapes etc.</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#contours-hierarchy"><span class="std std-ref">Contours Hierarchy</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="14%" />
<col width="86%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/contour_hierarchy.jpg"><img alt="contour_5" src="_images/contour_hierarchy.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Learn about Contour Hierarchy</td>
</tr>
</tbody>
</table>
</div></blockquote>
<div class="toctree-wrapper compound">
<span id="document-py_tutorials/py_imgproc/py_contours/py_contours_begin/py_contours_begin"></span><div class="section" id="contours-getting-started">
<span id="id1"></span><h5>Contours : Getting Started<a class="headerlink" href="#contours-getting-started" title="Permalink to this headline">¶</a></h5>
<div class="section" id="goal">
<h6>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h6>
<blockquote>
<div><ul class="simple">
<li>Understand what contours are.</li>
<li>Learn to find contours, draw contours etc</li>
<li>You will see these functions : <strong>cv2.findContours()</strong>, <strong>cv2.drawContours()</strong></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="what-are-contours">
<h6>What are contours?<a class="headerlink" href="#what-are-contours" title="Permalink to this headline">¶</a></h6>
<p>Contours can be explained simply as a curve joining all the continuous points (along the boundary), having same color or intensity. The contours are a useful tool for shape analysis and object detection and recognition.</p>
<blockquote>
<div><ul class="simple">
<li>For better accuracy, use binary images. So before finding contours, apply threshold or canny edge detection.</li>
<li>findContours function modifies the source image. So if you want source image even after finding contours, already store it to some other variables.</li>
<li>In OpenCV, finding contours is like finding white object from black background. So remember, object to be found should be white and background should be black.</li>
</ul>
</div></blockquote>
<p>Let&#8217;s see how to find contours of a binary image:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cv2</span>

<span class="n">im</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;test.jpg&#39;</span><span class="p">)</span>
<span class="n">imgray</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">im</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2GRAY</span><span class="p">)</span>
<span class="n">ret</span><span class="p">,</span><span class="n">thresh</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">threshold</span><span class="p">(</span><span class="n">imgray</span><span class="p">,</span><span class="mi">127</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">image</span><span class="p">,</span> <span class="n">contours</span><span class="p">,</span> <span class="n">hierarchy</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">findContours</span><span class="p">(</span><span class="n">thresh</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">RETR_TREE</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">CHAIN_APPROX_SIMPLE</span><span class="p">)</span>
</pre></div>
</div>
<p>See, there are three arguments in <strong>cv2.findContours()</strong> function, first one is source image, second is contour retrieval mode, third is contour approximation method. And it outputs the image, contours and hierarchy. <code class="docutils literal"><span class="pre">contours</span></code> is a Python list of all the contours in the image. Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">We will discuss second and third arguments and about hierarchy in details later. Until then, the values given to them in code sample will work fine for all images.</p>
</div>
</div>
<div class="section" id="how-to-draw-the-contours">
<h6>How to draw the contours?<a class="headerlink" href="#how-to-draw-the-contours" title="Permalink to this headline">¶</a></h6>
<p>To draw the contours, <code class="docutils literal"><span class="pre">cv2.drawContours</span></code> function is used. It can also be used to draw any shape provided you have its boundary points. Its first argument is source image, second argument is the contours which should be passed as a Python list, third argument is index of contours (useful when drawing individual contour. To draw all contours, pass -1) and remaining arguments are color, thickness etc.</p>
<p>To draw all the contours in an image:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">drawContours</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">contours</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
<p>To draw an individual contour, say 4th contour:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">drawContours</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">contours</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
<p>But most of the time, below method will be useful:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">cnt</span> <span class="o">=</span> <span class="n">contours</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">drawContours</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="p">[</span><span class="n">cnt</span><span class="p">],</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Last two methods are same, but when you go forward, you will see last one is more useful.</p>
</div>
</div>
<div class="section" id="contour-approximation-method">
<h6>Contour Approximation Method<a class="headerlink" href="#contour-approximation-method" title="Permalink to this headline">¶</a></h6>
<p>This is the third argument in <code class="docutils literal"><span class="pre">cv2.findContours</span></code> function. What does it denote actually?</p>
<p>Above, we told that contours are the boundaries of a shape with same intensity. It stores the (x,y) coordinates of the boundary of a shape. But does it store all the coordinates ? That is specified by this contour approximation method.</p>
<p>If you pass <code class="docutils literal"><span class="pre">cv2.CHAIN_APPROX_NONE</span></code>, all the boundary points are stored. But actually do we need all the points? For eg, you found the contour of a straight line. Do you need all the points on the line to represent that line? No, we need just two end points of that line. This is what <code class="docutils literal"><span class="pre">cv2.CHAIN_APPROX_SIMPLE</span></code> does. It removes all redundant points and compresses the contour, thereby saving memory.</p>
<p>Below image of a rectangle demonstrate this technique. Just draw a circle on all the coordinates in the contour array (drawn in blue color). First image shows points I got with <code class="docutils literal"><span class="pre">cv2.CHAIN_APPROX_NONE</span></code> (734 points) and second image shows the one with <code class="docutils literal"><span class="pre">cv2.CHAIN_APPROX_SIMPLE</span></code> (only 4 points). See, how much memory it saves!!!</p>
<blockquote>
<div><img alt="Contour Retrieval Method" class="align-center" src="_images/none.jpg" />
</div></blockquote>
</div>
<div class="section" id="additional-resources">
<h6>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h6>
</div>
<div class="section" id="exercises">
<h6>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h6>
</div>
</div>
<span id="document-py_tutorials/py_imgproc/py_contours/py_contour_features/py_contour_features"></span><div class="section" id="contour-features">
<span id="id1"></span><h5>Contour Features<a class="headerlink" href="#contour-features" title="Permalink to this headline">¶</a></h5>
<div class="section" id="goal">
<h6>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h6>
<p>In this article, we will learn</p>
<blockquote>
<div><ul class="simple">
<li>To find the different features of contours, like area, perimeter, centroid, bounding box etc</li>
<li>You will see plenty of functions related to contours.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="moments">
<h6>1. Moments<a class="headerlink" href="#moments" title="Permalink to this headline">¶</a></h6>
<p>Image moments help you to calculate some features like center of mass of the object, area of the object etc. Check out the wikipedia page on <a class="reference external" href="http://en.wikipedia.org/wiki/Image_moment">Image Moments</a></p>
<p>The function <strong>cv2.moments()</strong> gives a dictionary of all moment values calculated. See below:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;star.jpg&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ret</span><span class="p">,</span><span class="n">thresh</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">threshold</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="mi">127</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">contours</span><span class="p">,</span><span class="n">hierarchy</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">findContours</span><span class="p">(</span><span class="n">thresh</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">cnt</span> <span class="o">=</span> <span class="n">contours</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">M</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">moments</span><span class="p">(</span><span class="n">cnt</span><span class="p">)</span>
<span class="nb">print</span> <span class="n">M</span>
</pre></div>
</div>
<p>From this moments, you can extract useful data like area, centroid etc. Centroid is given by the relations, <img class="math" src="_images/math/41d9674b5828dd601e9517557fbdef3f2ee81c7b.png" alt="C_x = \frac{M_{10}}{M_{00}}"/> and <img class="math" src="_images/math/33e173a81f373aea8d354af006ddd31a82ac4e10.png" alt="C_y = \frac{M_{01}}{M_{00}}"/>. This can be done as follows:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">cx</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">M</span><span class="p">[</span><span class="s1">&#39;m10&#39;</span><span class="p">]</span><span class="o">/</span><span class="n">M</span><span class="p">[</span><span class="s1">&#39;m00&#39;</span><span class="p">])</span>
<span class="n">cy</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">M</span><span class="p">[</span><span class="s1">&#39;m01&#39;</span><span class="p">]</span><span class="o">/</span><span class="n">M</span><span class="p">[</span><span class="s1">&#39;m00&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="section" id="contour-area">
<h6>2. Contour Area<a class="headerlink" href="#contour-area" title="Permalink to this headline">¶</a></h6>
<p>Contour area is given by the function <strong>cv2.contourArea()</strong> or from moments, <strong>M[&#8216;m00&#8217;]</strong>.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">area</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">contourArea</span><span class="p">(</span><span class="n">cnt</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="contour-perimeter">
<h6>3. Contour Perimeter<a class="headerlink" href="#contour-perimeter" title="Permalink to this headline">¶</a></h6>
<p>It is also called arc length. It can be found out using <strong>cv2.arcLength()</strong> function. Second argument specify whether shape is a closed contour (if passed <code class="docutils literal"><span class="pre">True</span></code>), or just a curve.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">perimeter</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">arcLength</span><span class="p">(</span><span class="n">cnt</span><span class="p">,</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="contour-approximation">
<h6>4. Contour Approximation<a class="headerlink" href="#contour-approximation" title="Permalink to this headline">¶</a></h6>
<p>It approximates a contour shape to another shape with less number of vertices depending upon the precision we specify. It is an implementation of <a class="reference external" href="http://en.wikipedia.org/wiki/Ramer-Douglas-Peucker_algorithm">Douglas-Peucker algorithm</a>. Check the wikipedia page for algorithm and demonstration.</p>
<p>To understand this, suppose you are trying to find a square in an image, but due to some problems in the image, you didn&#8217;t get a perfect square, but a &#8220;bad shape&#8221; (As shown in first image below). Now you can use this function to approximate the shape. In this, second argument is called <code class="docutils literal"><span class="pre">epsilon</span></code>, which is maximum distance from contour to approximated contour. It is an accuracy parameter. A wise selection of <code class="docutils literal"><span class="pre">epsilon</span></code> is needed to get the correct output.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.1</span><span class="o">*</span><span class="n">cv2</span><span class="o">.</span><span class="n">arcLength</span><span class="p">(</span><span class="n">cnt</span><span class="p">,</span><span class="kc">True</span><span class="p">)</span>
<span class="n">approx</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">approxPolyDP</span><span class="p">(</span><span class="n">cnt</span><span class="p">,</span><span class="n">epsilon</span><span class="p">,</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>Below, in second image, green line shows the approximated curve for <code class="docutils literal"><span class="pre">epsilon</span> <span class="pre">=</span> <span class="pre">10%</span> <span class="pre">of</span> <span class="pre">arc</span> <span class="pre">length</span></code>. Third image shows the same for <code class="docutils literal"><span class="pre">epsilon</span> <span class="pre">=</span> <span class="pre">1%</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">arc</span> <span class="pre">length</span></code>. Third argument specifies whether curve is closed or not.</p>
<blockquote>
<div><img alt="Contour Approximation" class="align-center" src="_images/approx.jpg" />
</div></blockquote>
</div>
<div class="section" id="convex-hull">
<h6>5. Convex Hull<a class="headerlink" href="#convex-hull" title="Permalink to this headline">¶</a></h6>
<p>Convex Hull will look similar to contour approximation, but it is not (Both may provide same results in some cases). Here, <strong>cv2.convexHull()</strong> function checks a curve for convexity defects and corrects it. Generally speaking, convex curves are the curves which are always bulged out, or at-least flat. And if it is bulged inside, it is called convexity defects. For example, check the below image of hand. Red line shows the convex hull of hand. The double-sided arrow marks shows the convexity defects, which are the local maximum deviations of hull from contours.</p>
<blockquote>
<div><img alt="Convex Hull" class="align-center" src="_images/convexitydefects.jpg" />
</div></blockquote>
<p>There is a little bit things to discuss about it its syntax:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">hull</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">convexHull</span><span class="p">(</span><span class="n">points</span><span class="p">[,</span> <span class="n">hull</span><span class="p">[,</span> <span class="n">clockwise</span><span class="p">[,</span> <span class="n">returnPoints</span><span class="p">]]</span>
</pre></div>
</div>
<p>Arguments details:</p>
<blockquote>
<div><ul class="simple">
<li><strong>points</strong> are the contours we pass into.</li>
<li><strong>hull</strong> is the output, normally we avoid it.</li>
<li><strong>clockwise</strong> : Orientation flag. If it is <code class="docutils literal"><span class="pre">True</span></code>, the output convex hull is oriented clockwise. Otherwise, it is oriented counter-clockwise.</li>
<li><strong>returnPoints</strong> : By default, <code class="docutils literal"><span class="pre">True</span></code>. Then it returns the coordinates of the hull points. If <code class="docutils literal"><span class="pre">False</span></code>, it returns the indices of contour points corresponding to the hull points.</li>
</ul>
</div></blockquote>
<p>So to get a convex hull as in above image, following is sufficient:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">hull</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">convexHull</span><span class="p">(</span><span class="n">cnt</span><span class="p">)</span>
</pre></div>
</div>
<p>But if you want to find convexity defects, you need to pass <code class="docutils literal"><span class="pre">returnPoints</span> <span class="pre">=</span> <span class="pre">False</span></code>. To understand it, we will take the rectangle image above. First I found its contour as <code class="docutils literal"><span class="pre">cnt</span></code>. Now I found its convex hull with <code class="docutils literal"><span class="pre">returnPoints</span> <span class="pre">=</span> <span class="pre">True</span></code>, I got following values: <code class="docutils literal"><span class="pre">[[[234</span> <span class="pre">202]],</span> <span class="pre">[[</span> <span class="pre">51</span> <span class="pre">202]],</span> <span class="pre">[[</span> <span class="pre">51</span> <span class="pre">79]],</span> <span class="pre">[[234</span> <span class="pre">79]]]</span></code> which are the four corner points of rectangle. Now if do the same with <code class="docutils literal"><span class="pre">returnPoints</span> <span class="pre">=</span> <span class="pre">False</span></code>, I get following result: <code class="docutils literal"><span class="pre">[[129],[</span> <span class="pre">67],[</span> <span class="pre">0],[142]]</span></code>. These are the indices of corresponding points in contours. For eg, check the first value: <code class="docutils literal"><span class="pre">cnt[129]</span> <span class="pre">=</span> <span class="pre">[[234,</span> <span class="pre">202]]</span></code> which is same as first result (and so on for others).</p>
<p>You will see it again when we discuss about convexity defects.</p>
</div>
<div class="section" id="checking-convexity">
<h6>6. Checking Convexity<a class="headerlink" href="#checking-convexity" title="Permalink to this headline">¶</a></h6>
<p>There is a function to check if a curve is convex or not, <strong>cv2.isContourConvex()</strong>. It just return whether True or False. Not a big deal.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">k</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">isContourConvex</span><span class="p">(</span><span class="n">cnt</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="bounding-rectangle">
<h6>7. Bounding Rectangle<a class="headerlink" href="#bounding-rectangle" title="Permalink to this headline">¶</a></h6>
<p>There are two types of bounding rectangles.</p>
<div class="section" id="a-straight-bounding-rectangle">
<h7>7.a. Straight Bounding Rectangle<a class="headerlink" href="#a-straight-bounding-rectangle" title="Permalink to this headline">¶</a></h7>
<p>It is a straight rectangle, it doesn&#8217;t consider the rotation of the object. So area of the bounding rectangle won&#8217;t be minimum. It is found by the function <strong>cv2.boundingRect()</strong>.</p>
<p>Let (x,y) be the top-left coordinate of the rectangle and (w,h) be its width and height.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="n">h</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">boundingRect</span><span class="p">(</span><span class="n">cnt</span><span class="p">)</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">rectangle</span><span class="p">(</span><span class="n">img</span><span class="p">,(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">),(</span><span class="n">x</span><span class="o">+</span><span class="n">w</span><span class="p">,</span><span class="n">y</span><span class="o">+</span><span class="n">h</span><span class="p">),(</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="b-rotated-rectangle">
<h7>7.b. Rotated Rectangle<a class="headerlink" href="#b-rotated-rectangle" title="Permalink to this headline">¶</a></h7>
<p>Here, bounding rectangle is drawn with minimum area, so it considers the rotation also. The function used is <strong>cv2.minAreaRect()</strong>. It returns a Box2D structure which contains following detals - ( top-left corner(x,y), (width, height), angle of rotation ). But to draw this rectangle, we need 4 corners of the rectangle. It is obtained by the function <strong>cv2.boxPoints()</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">rect</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">minAreaRect</span><span class="p">(</span><span class="n">cnt</span><span class="p">)</span>
<span class="n">box</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">boxPoints</span><span class="p">(</span><span class="n">rect</span><span class="p">)</span>
<span class="n">box</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">int0</span><span class="p">(</span><span class="n">box</span><span class="p">)</span>
<span class="n">im</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">drawContours</span><span class="p">(</span><span class="n">im</span><span class="p">,[</span><span class="n">box</span><span class="p">],</span><span class="mi">0</span><span class="p">,(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">),</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>Both the rectangles are shown in a single image. Green rectangle shows the normal bounding rect. Red rectangle is the rotated rect.</p>
<blockquote>
<div><img alt="Bounding Rectangle" class="align-center" src="_images/boundingrect.png" />
</div></blockquote>
</div>
</div>
<div class="section" id="minimum-enclosing-circle">
<h6>8. Minimum Enclosing Circle<a class="headerlink" href="#minimum-enclosing-circle" title="Permalink to this headline">¶</a></h6>
<p>Next we find the circumcircle of an object using the function <strong>cv2.minEnclosingCircle()</strong>. It is a circle which completely covers the object with minimum area.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">),</span><span class="n">radius</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">minEnclosingCircle</span><span class="p">(</span><span class="n">cnt</span><span class="p">)</span>
<span class="n">center</span> <span class="o">=</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="nb">int</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
<span class="n">radius</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">radius</span><span class="p">)</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">circle</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">center</span><span class="p">,</span><span class="n">radius</span><span class="p">,(</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<img alt="Minimum Enclosing Circle" class="align-center" src="_images/circumcircle.png" />
</div>
<div class="section" id="fitting-an-ellipse">
<h6>9. Fitting an Ellipse<a class="headerlink" href="#fitting-an-ellipse" title="Permalink to this headline">¶</a></h6>
<p>Next one is to fit an ellipse to an object. It returns the rotated rectangle in which the ellipse is inscribed.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">ellipse</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">fitEllipse</span><span class="p">(</span><span class="n">cnt</span><span class="p">)</span>
<span class="n">im</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">ellipse</span><span class="p">(</span><span class="n">im</span><span class="p">,</span><span class="n">ellipse</span><span class="p">,(</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<img alt="Fitting an Ellipse" class="align-center" src="_images/fitellipse.png" />
</div>
<div class="section" id="fitting-a-line">
<h6>10. Fitting a Line<a class="headerlink" href="#fitting-a-line" title="Permalink to this headline">¶</a></h6>
<p>Similarly we can fit a line to a set of points. Below image contains a set of white points. We can approximate a straight line to it.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">rows</span><span class="p">,</span><span class="n">cols</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
<span class="p">[</span><span class="n">vx</span><span class="p">,</span><span class="n">vy</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">fitLine</span><span class="p">(</span><span class="n">cnt</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">DIST_L2</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.01</span><span class="p">,</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">lefty</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="o">-</span><span class="n">x</span><span class="o">*</span><span class="n">vy</span><span class="o">/</span><span class="n">vx</span><span class="p">)</span> <span class="o">+</span> <span class="n">y</span><span class="p">)</span>
<span class="n">righty</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(((</span><span class="n">cols</span><span class="o">-</span><span class="n">x</span><span class="p">)</span><span class="o">*</span><span class="n">vy</span><span class="o">/</span><span class="n">vx</span><span class="p">)</span><span class="o">+</span><span class="n">y</span><span class="p">)</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">line</span><span class="p">(</span><span class="n">img</span><span class="p">,(</span><span class="n">cols</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">righty</span><span class="p">),(</span><span class="mi">0</span><span class="p">,</span><span class="n">lefty</span><span class="p">),(</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<img alt="Fitting a Line" class="align-center" src="_images/fitline.jpg" />
</div>
<div class="section" id="additional-resources">
<h6>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h6>
</div>
<div class="section" id="exercises">
<h6>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h6>
</div>
</div>
<span id="document-py_tutorials/py_imgproc/py_contours/py_contour_properties/py_contour_properties"></span><div class="section" id="contour-properties">
<span id="id1"></span><h5>Contour Properties<a class="headerlink" href="#contour-properties" title="Permalink to this headline">¶</a></h5>
<p>Here we will learn to extract some frequently used properties of objects like Solidity, Equivalent Diameter, Mask image, Mean Intensity etc. More features can be found at <a class="reference external" href="http://www.mathworks.in/help/images/ref/regionprops.html">Matlab regionprops documentation</a>.</p>
<p><em>(NB : Centroid, Area, Perimeter etc also belong to this category, but we have seen it in last chapter)</em></p>
<div class="section" id="aspect-ratio">
<h6>1. Aspect Ratio<a class="headerlink" href="#aspect-ratio" title="Permalink to this headline">¶</a></h6>
<p>It is the ratio of width to height of bounding rect of the object.</p>
<div class="math">
<p><img src="_images/math/f9b59fc75e892c6f0b6b5cefb322c048f41d1e81.png" alt="Aspect \; Ratio = \frac{Width}{Height}"/></p>
</div><div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="n">h</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">boundingRect</span><span class="p">(</span><span class="n">cnt</span><span class="p">)</span>
<span class="n">aspect_ratio</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">w</span><span class="p">)</span><span class="o">/</span><span class="n">h</span>
</pre></div>
</div>
</div>
<div class="section" id="extent">
<h6>2. Extent<a class="headerlink" href="#extent" title="Permalink to this headline">¶</a></h6>
<p>Extent is the ratio of contour area to bounding rectangle area.</p>
<div class="math">
<p><img src="_images/math/22a73ff9d27701129a1bfed7259db29e62892729.png" alt="Extent = \frac{Object \; Area}{Bounding \; Rectangle \; Area}"/></p>
</div><div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">area</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">contourArea</span><span class="p">(</span><span class="n">cnt</span><span class="p">)</span>
<span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="n">h</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">boundingRect</span><span class="p">(</span><span class="n">cnt</span><span class="p">)</span>
<span class="n">rect_area</span> <span class="o">=</span> <span class="n">w</span><span class="o">*</span><span class="n">h</span>
<span class="n">extent</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">area</span><span class="p">)</span><span class="o">/</span><span class="n">rect_area</span>
</pre></div>
</div>
</div>
<div class="section" id="solidity">
<h6>3. Solidity<a class="headerlink" href="#solidity" title="Permalink to this headline">¶</a></h6>
<p>Solidity is the ratio of contour area to its convex hull area.</p>
<div class="math">
<p><img src="_images/math/686e09ed3af5ffc8d58648dba226e47992d749e6.png" alt="Solidity = \frac{Contour \; Area}{Convex \; Hull \; Area}"/></p>
</div><div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">area</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">contourArea</span><span class="p">(</span><span class="n">cnt</span><span class="p">)</span>
<span class="n">hull</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">convexHull</span><span class="p">(</span><span class="n">cnt</span><span class="p">)</span>
<span class="n">hull_area</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">contourArea</span><span class="p">(</span><span class="n">hull</span><span class="p">)</span>
<span class="n">solidity</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">area</span><span class="p">)</span><span class="o">/</span><span class="n">hull_area</span>
</pre></div>
</div>
</div>
<div class="section" id="equivalent-diameter">
<h6>4. Equivalent Diameter<a class="headerlink" href="#equivalent-diameter" title="Permalink to this headline">¶</a></h6>
<p>Equivalent Diameter is the diameter of the circle whose area is same as the contour area.</p>
<div class="math">
<p><img src="_images/math/dd455145891023674d52ad7f48d9ff5a2613e49b.png" alt="Equivalent \; Diameter = \sqrt{\frac{4 \times Contour \; Area}{\pi}}"/></p>
</div><div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">area</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">contourArea</span><span class="p">(</span><span class="n">cnt</span><span class="p">)</span>
<span class="n">equi_diameter</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">4</span><span class="o">*</span><span class="n">area</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="orientation">
<h6>5. Orientation<a class="headerlink" href="#orientation" title="Permalink to this headline">¶</a></h6>
<p>Orientation is the angle at which object is directed. Following method also gives the Major Axis and Minor Axis lengths.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">),(</span><span class="n">MA</span><span class="p">,</span><span class="n">ma</span><span class="p">),</span><span class="n">angle</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">fitEllipse</span><span class="p">(</span><span class="n">cnt</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="mask-and-pixel-points">
<h6>6. Mask and Pixel Points<a class="headerlink" href="#mask-and-pixel-points" title="Permalink to this headline">¶</a></h6>
<p>In some cases, we may need all the points which comprises that object. It can be done as follows:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">imgray</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">drawContours</span><span class="p">(</span><span class="n">mask</span><span class="p">,[</span><span class="n">cnt</span><span class="p">],</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">pixelpoints</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">mask</span><span class="p">))</span>
<span class="c1">#pixelpoints = cv2.findNonZero(mask)</span>
</pre></div>
</div>
<p>Here, two methods, one using Numpy functions, next one using OpenCV function (last commented line) are given to do the same. Results are also same, but with a slight difference. Numpy gives coordinates in <strong>(row, column)</strong> format, while OpenCV gives coordinates in <strong>(x,y)</strong> format. So basically the answers will be interchanged. Note that, <strong>row = x</strong> and <strong>column = y</strong>.</p>
</div>
<div class="section" id="maximum-value-minimum-value-and-their-locations">
<h6>7. Maximum Value, Minimum Value and their locations<a class="headerlink" href="#maximum-value-minimum-value-and-their-locations" title="Permalink to this headline">¶</a></h6>
<p>We can find these parameters using a mask image.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">min_val</span><span class="p">,</span> <span class="n">max_val</span><span class="p">,</span> <span class="n">min_loc</span><span class="p">,</span> <span class="n">max_loc</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">minMaxLoc</span><span class="p">(</span><span class="n">imgray</span><span class="p">,</span><span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="mean-color-or-mean-intensity">
<h6>8. Mean Color or Mean Intensity<a class="headerlink" href="#mean-color-or-mean-intensity" title="Permalink to this headline">¶</a></h6>
<p>Here, we can find the average color of an object. Or it can be average intensity of the object in grayscale mode. We again use the same mask to do it.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">mean_val</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">im</span><span class="p">,</span><span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="extreme-points">
<h6>9. Extreme Points<a class="headerlink" href="#extreme-points" title="Permalink to this headline">¶</a></h6>
<p>Extreme Points means topmost, bottommost, rightmost and leftmost points of the object.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">leftmost</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">cnt</span><span class="p">[</span><span class="n">cnt</span><span class="p">[:,:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">argmin</span><span class="p">()][</span><span class="mi">0</span><span class="p">])</span>
<span class="n">rightmost</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">cnt</span><span class="p">[</span><span class="n">cnt</span><span class="p">[:,:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">argmax</span><span class="p">()][</span><span class="mi">0</span><span class="p">])</span>
<span class="n">topmost</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">cnt</span><span class="p">[</span><span class="n">cnt</span><span class="p">[:,:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">argmin</span><span class="p">()][</span><span class="mi">0</span><span class="p">])</span>
<span class="n">bottommost</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">cnt</span><span class="p">[</span><span class="n">cnt</span><span class="p">[:,:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">argmax</span><span class="p">()][</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
<p>For eg, if I apply it to an Indian map, I get the following result :</p>
<blockquote>
<div><img alt="Extreme Points" class="align-center" src="_images/extremepoints.jpg" />
</div></blockquote>
</div>
<div class="section" id="additional-resources">
<h6>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h6>
</div>
<div class="section" id="exercises">
<h6>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h6>
<ol class="arabic simple">
<li>There are still some features left in matlab regionprops doc. Try to implement them.</li>
</ol>
</div>
</div>
<span id="document-py_tutorials/py_imgproc/py_contours/py_contours_more_functions/py_contours_more_functions"></span><div class="section" id="contours-more-functions">
<span id="id1"></span><h5>Contours : More Functions<a class="headerlink" href="#contours-more-functions" title="Permalink to this headline">¶</a></h5>
<div class="section" id="goal">
<h6>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h6>
<dl class="docutils">
<dt>In this chapter, we will learn about</dt>
<dd><ul class="first last simple">
<li>Convexity defects and how to find them.</li>
<li>Finding shortest distance from a point to a polygon</li>
<li>Matching different shapes</li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="theory-and-code">
<h6>Theory and Code<a class="headerlink" href="#theory-and-code" title="Permalink to this headline">¶</a></h6>
<div class="section" id="convexity-defects">
<h7>1. Convexity Defects<a class="headerlink" href="#convexity-defects" title="Permalink to this headline">¶</a></h7>
<p>We saw what is convex hull in second chapter about contours. Any deviation of the object from this hull can be considered as convexity defect.</p>
<p>OpenCV comes with a ready-made function to find this, <strong>cv2.convexityDefects()</strong>. A basic function call would look like below:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">hull</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">convexHull</span><span class="p">(</span><span class="n">cnt</span><span class="p">,</span><span class="n">returnPoints</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
<span class="n">defects</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">convexityDefects</span><span class="p">(</span><span class="n">cnt</span><span class="p">,</span><span class="n">hull</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Remember we have to pass <code class="docutils literal"><span class="pre">returnPoints</span> <span class="pre">=</span> <span class="pre">False</span></code> while finding convex hull, in order to find convexity defects.</p>
</div>
<p>It returns an array where each row contains these values - <strong>[ start point, end point, farthest point, approximate distance to farthest point ]</strong>. We can visualize it using an image. We draw a line joining start point and end point, then draw a circle at the farthest point. Remember first three values returned are indices of <code class="docutils literal"><span class="pre">cnt</span></code>. So we have to bring those values from <code class="docutils literal"><span class="pre">cnt</span></code>.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;star.jpg&#39;</span><span class="p">)</span>
<span class="n">img_gray</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2GRAY</span><span class="p">)</span>
<span class="n">ret</span><span class="p">,</span> <span class="n">thresh</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">threshold</span><span class="p">(</span><span class="n">img_gray</span><span class="p">,</span> <span class="mi">127</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">contours</span><span class="p">,</span><span class="n">hierarchy</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">findContours</span><span class="p">(</span><span class="n">thresh</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">cnt</span> <span class="o">=</span> <span class="n">contours</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">hull</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">convexHull</span><span class="p">(</span><span class="n">cnt</span><span class="p">,</span><span class="n">returnPoints</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
<span class="n">defects</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">convexityDefects</span><span class="p">(</span><span class="n">cnt</span><span class="p">,</span><span class="n">hull</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">defects</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="n">s</span><span class="p">,</span><span class="n">e</span><span class="p">,</span><span class="n">f</span><span class="p">,</span><span class="n">d</span> <span class="o">=</span> <span class="n">defects</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">start</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">cnt</span><span class="p">[</span><span class="n">s</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">end</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">cnt</span><span class="p">[</span><span class="n">e</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">far</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">cnt</span><span class="p">[</span><span class="n">f</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">cv2</span><span class="o">.</span><span class="n">line</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">start</span><span class="p">,</span><span class="n">end</span><span class="p">,[</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">cv2</span><span class="o">.</span><span class="n">circle</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">far</span><span class="p">,</span><span class="mi">5</span><span class="p">,[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">],</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">cv2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s1">&#39;img&#39;</span><span class="p">,</span><span class="n">img</span><span class="p">)</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>
</pre></div>
</div>
<p>And see the result:</p>
<blockquote>
<div><img alt="Convexity Defects" class="align-center" src="_images/defects.jpg" />
</div></blockquote>
</div>
<div class="section" id="point-polygon-test">
<h7>2. Point Polygon Test<a class="headerlink" href="#point-polygon-test" title="Permalink to this headline">¶</a></h7>
<p>This function finds the shortest distance between a point in the image and a contour. It returns the distance which is negative when point is outside the contour, positive when point is inside and zero if point is on the contour.</p>
<p>For example, we can check the point (50,50) as follows:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">dist</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">pointPolygonTest</span><span class="p">(</span><span class="n">cnt</span><span class="p">,(</span><span class="mi">50</span><span class="p">,</span><span class="mi">50</span><span class="p">),</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>In the function, third argument is <code class="docutils literal"><span class="pre">measureDist</span></code>. If it is <code class="docutils literal"><span class="pre">True</span></code>, it finds the signed distance. If <code class="docutils literal"><span class="pre">False</span></code>, it finds whether the point is inside or outside or on the contour (it returns +1, -1, 0 respectively).</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">If you don&#8217;t want to find the distance, make sure third argument is <code class="docutils literal"><span class="pre">False</span></code>, because, it is a time consuming process. So, making it <code class="docutils literal"><span class="pre">False</span></code> gives about 2-3X speedup.</p>
</div>
</div>
<div class="section" id="match-shapes">
<h7>3. Match Shapes<a class="headerlink" href="#match-shapes" title="Permalink to this headline">¶</a></h7>
<p>OpenCV comes with a function <strong>cv2.matchShapes()</strong> which enables us to compare two shapes, or two contours and returns a metric showing the similarity. The lower the result, the better match it is. It is calculated based on the hu-moment values. Different measurement methods are explained in the docs.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">img1</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;star.jpg&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">img2</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;star2.jpg&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>

<span class="n">ret</span><span class="p">,</span> <span class="n">thresh</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">threshold</span><span class="p">(</span><span class="n">img1</span><span class="p">,</span> <span class="mi">127</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ret</span><span class="p">,</span> <span class="n">thresh2</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">threshold</span><span class="p">(</span><span class="n">img2</span><span class="p">,</span> <span class="mi">127</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">contours</span><span class="p">,</span><span class="n">hierarchy</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">findContours</span><span class="p">(</span><span class="n">thresh</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">cnt1</span> <span class="o">=</span> <span class="n">contours</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">contours</span><span class="p">,</span><span class="n">hierarchy</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">findContours</span><span class="p">(</span><span class="n">thresh2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">cnt2</span> <span class="o">=</span> <span class="n">contours</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">ret</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">matchShapes</span><span class="p">(</span><span class="n">cnt1</span><span class="p">,</span><span class="n">cnt2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mf">0.0</span><span class="p">)</span>
<span class="nb">print</span> <span class="n">ret</span>
</pre></div>
</div>
<p>I tried matching shapes with different shapes given below:</p>
<blockquote>
<div><img alt="Match Shapes" class="align-center" src="_images/matchshapes.jpg" />
</div></blockquote>
<p>I got following results:</p>
<blockquote>
<div><ul class="simple">
<li>Matching Image A with itself = 0.0</li>
<li>Matching Image A with Image B = 0.001946</li>
<li>Matching Image A with Image C = 0.326911</li>
</ul>
</div></blockquote>
<p>See, even image rotation doesn&#8217;t affect much on this comparison.</p>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><a class="reference external" href="http://en.wikipedia.org/wiki/Image_moment#Rotation_invariant_moments">Hu-Moments</a> are seven moments invariant to translation, rotation and scale. Seventh one is skew-invariant. Those values can be found using <strong>cv2.HuMoments()</strong> function.</p>
</div>
</div>
</div>
<div class="section" id="additional-resources">
<h6>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h6>
</div>
<div class="section" id="exercises">
<h6>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h6>
<ol class="arabic simple">
<li>Check the documentation for <strong>cv2.pointPolygonTest()</strong>, you can find a nice image in Red and Blue color. It represents the distance from all pixels to the white curve on it. All pixels inside curve is blue depending on the distance. Similarly outside points are red. Contour edges are marked with White. So problem is simple. Write a code to create such a representation of distance.</li>
<li>Compare images of digits or letters using <strong>cv2.matchShapes()</strong>. ( That would be a simple step towards OCR )</li>
</ol>
</div>
</div>
<span id="document-py_tutorials/py_imgproc/py_contours/py_contours_hierarchy/py_contours_hierarchy"></span><div class="section" id="contours-hierarchy">
<span id="id1"></span><h5>Contours Hierarchy<a class="headerlink" href="#contours-hierarchy" title="Permalink to this headline">¶</a></h5>
<div class="section" id="goal">
<h6>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h6>
<p>This time, we learn about the hierarchy of contours, i.e. the parent-child relationship in Contours.</p>
</div>
<div class="section" id="theory">
<h6>Theory<a class="headerlink" href="#theory" title="Permalink to this headline">¶</a></h6>
<p>In the last few articles on contours, we have worked with several functions related to contours provided by OpenCV. But when we found the contours in image using <strong>cv2.findContours()</strong> function, we have passed an argument, <strong>Contour Retrieval Mode</strong>. We usually passed <strong>cv2.RETR_LIST</strong> or <strong>cv2.RETR_TREE</strong> and it worked nice. But what does it actually mean ?</p>
<p>Also, in the output, we got three arrays, first is the image, second is our contours, and one more output which we named as <strong>hierarchy</strong> (Please checkout the codes in previous articles). But we never used this hierarchy anywhere. Then what is this hierarchy and what is it for ? What is its relationship with the previous mentioned function argument ?</p>
<p>That is what we are going to deal in this article.</p>
<div class="section" id="what-is-hierarchy">
<h7>What is Hierarchy?<a class="headerlink" href="#what-is-hierarchy" title="Permalink to this headline">¶</a></h7>
<p>Normally we use the <strong>cv2.findContours()</strong> function to detect objects in an image, right ? Sometimes objects are in different locations. But in some cases, some shapes are inside other shapes. Just like nested figures. In this case, we call outer one as <strong>parent</strong> and inner one as <strong>child</strong>. This way, contours in an image has some relationship to each other. And we can specify how one contour is connected to each other, like, is it child of some other contour, or is it a parent etc. Representation of this relationship is called the <strong>Hierarchy</strong>.</p>
<p>Consider an example image below :</p>
<blockquote>
<div><img alt="Hierarchy Representation" class="align-center" src="_images/hierarchy.png" />
</div></blockquote>
<p>In this image, there are a few shapes which I have numbered from <strong>0-5</strong>. <em>2 and 2a</em> denotes the external and internal contours of the outermost box.</p>
<p>Here, contours 0,1,2 are <strong>external or outermost</strong>. We can say, they are in <strong>hierarchy-0</strong> or simply they are in <strong>same hierarchy level</strong>.</p>
<p>Next comes <strong>contour-2a</strong>. It can be considered as a <strong>child of contour-2</strong> (or in opposite way, contour-2 is parent of contour-2a). So let it be in <strong>hierarchy-1</strong>. Similarly contour-3 is child of contour-2 and it comes in next hierarchy. Finally contours 4,5 are the children of contour-3a, and they come in the last hierarchy level. From the way I numbered the boxes, I would say contour-4 is the first child of contour-3a (It can be contour-5 also).</p>
<p>I mentioned these things to understand terms like <strong>same hierarchy level</strong>, <strong>external contour</strong>, <strong>child contour</strong>, <strong>parent contour</strong>, <strong>first child</strong> etc. Now let&#8217;s get into OpenCV.</p>
</div>
<div class="section" id="hierarchy-representation-in-opencv">
<h7>Hierarchy Representation in OpenCV<a class="headerlink" href="#hierarchy-representation-in-opencv" title="Permalink to this headline">¶</a></h7>
<p>So each contour has its own information regarding what hierarchy it is, who is its child, who is its parent etc. OpenCV represents it as an array of four values : <strong>[Next, Previous, First_Child, Parent]</strong></p>
<p class="centered">
<strong><em>&#8220;Next denotes next contour at the same hierarchical level.&#8221;</em></strong></p><p>For eg, take contour-0 in our picture. Who is next contour in its same level ? It is contour-1. So simply put <code class="docutils literal"><span class="pre">Next</span> <span class="pre">=</span> <span class="pre">1</span></code>. Similarly for Contour-1, next is contour-2. So <code class="docutils literal"><span class="pre">Next</span> <span class="pre">=</span> <span class="pre">2</span></code>.</p>
<p>What about contour-2? There is no next contour in the same level. So simply, put <code class="docutils literal"><span class="pre">Next</span> <span class="pre">=</span> <span class="pre">-1</span></code>. What about contour-4? It is in same level with contour-5. So its next contour is contour-5, so <code class="docutils literal"><span class="pre">Next</span> <span class="pre">=</span> <span class="pre">5</span></code>.</p>
<p class="centered">
<strong><em>&#8220;Previous denotes previous contour at the same hierarchical level.&#8221;</em></strong></p><p>It is same as above. Previous contour of contour-1 is contour-0 in the same level. Similarly for contour-2, it is contour-1. And for contour-0, there is no previous, so put it as -1.</p>
<p class="centered">
<strong><em>&#8220;First_Child denotes its first child contour.&#8221;</em></strong></p><p>There is no need of any explanation. For contour-2, child is contour-2a. So it gets the corresponding index value of contour-2a. What about contour-3a? It has two children. But we take only first child. And it is contour-4. So <code class="docutils literal"><span class="pre">First_Child</span> <span class="pre">=</span> <span class="pre">4</span></code> for contour-3a.</p>
<p class="centered">
<strong><em>&#8220;Parent denotes index of its parent contour.&#8221;</em></strong></p><p>It is just opposite of <strong>First_Child</strong>. Both for contour-4 and contour-5, parent contour is contour-3a. For contour-3a, it is contour-3 and so on.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">If there is no child or parent, that field is taken as -1</p>
</div>
<p>So now we know about the hierarchy style used in OpenCV, we can check into Contour Retrieval Modes in OpenCV with the help of same image given above. ie what do flags like cv2.RETR_LIST, cv2.RETR_TREE, cv2.RETR_CCOMP, cv2.RETR_EXTERNAL etc mean?</p>
</div>
</div>
<div class="section" id="contour-retrieval-mode">
<h6>Contour Retrieval Mode<a class="headerlink" href="#contour-retrieval-mode" title="Permalink to this headline">¶</a></h6>
<div class="section" id="retr-list">
<h7>1. RETR_LIST<a class="headerlink" href="#retr-list" title="Permalink to this headline">¶</a></h7>
<p>This is the simplest of the four flags (from explanation point of view). It simply retrieves all the contours, but doesn&#8217;t create any parent-child relationship. <strong>Parents and kids are equal under this rule, and they are just contours</strong>. ie they all belongs to same hierarchy level.</p>
<p>So here, 3rd and 4th term in hierarchy array is always -1. But obviously, Next and Previous terms will have their corresponding values. Just check it yourself and verify it.</p>
<p>Below is the result I got, and each row is hierarchy details of corresponding contour. For eg, first row corresponds to contour 0. Next contour is contour 1. So Next = 1. There is no previous contour, so Previous = 0. And the remaining two, as told before, it is -1.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">hierarchy</span>
<span class="go">array([[[ 1, -1, -1, -1],</span>
<span class="go">        [ 2,  0, -1, -1],</span>
<span class="go">        [ 3,  1, -1, -1],</span>
<span class="go">        [ 4,  2, -1, -1],</span>
<span class="go">        [ 5,  3, -1, -1],</span>
<span class="go">        [ 6,  4, -1, -1],</span>
<span class="go">        [ 7,  5, -1, -1],</span>
<span class="go">        [-1,  6, -1, -1]]])</span>
</pre></div>
</div>
<p>This is the good choice to use in your code, if you are not using any hierarchy features.</p>
</div>
<div class="section" id="retr-external">
<h7>2. RETR_EXTERNAL<a class="headerlink" href="#retr-external" title="Permalink to this headline">¶</a></h7>
<p>If you use this flag, it returns only extreme outer flags. All child contours are left behind. <strong>We can say, under this law, Only the eldest in every family is taken care of. It doesn&#8217;t care about other members of the family :)</strong>.</p>
<p>So, in our image, how many extreme outer contours are there? ie at hierarchy-0 level?. Only 3, ie contours 0,1,2, right? Now try to find the contours using this flag. Here also, values given to each element is same as above. Compare it with above result. Below is what I got :</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">hierarchy</span>
<span class="go">array([[[ 1, -1, -1, -1],</span>
<span class="go">        [ 2,  0, -1, -1],</span>
<span class="go">        [-1,  1, -1, -1]]])</span>
</pre></div>
</div>
<p>You can use this flag if you want to extract only the outer contours. It might be useful in some cases.</p>
</div>
<div class="section" id="retr-ccomp">
<h7>3. RETR_CCOMP<a class="headerlink" href="#retr-ccomp" title="Permalink to this headline">¶</a></h7>
<p>This flag retrieves all the contours and arranges them to a 2-level hierarchy. ie external contours of the object (ie its boundary) are placed in hierarchy-1. And the contours of holes inside object (if any) is placed in hierarchy-2. If any object inside it, its contour is placed again in hierarchy-1 only. And its hole in hierarchy-2 and so on.</p>
<p>Just consider the image of a &#8220;big white zero&#8221; on a black background. Outer circle of zero belongs to first hierarchy, and inner circle of zero belongs to second hierarchy.</p>
<p>We can explain it with a simple image. Here I have labelled the order of contours in red color and the hierarchy they belongs to, in green color (either 1 or 2). The order is same as the order OpenCV detects contours.</p>
<blockquote>
<div><img alt="CCOMP Hierarchy" class="align-center" src="_images/ccomp_hierarchy.png" />
</div></blockquote>
<p>So consider first contour, ie contour-0. It is hierarchy-1. It has two holes, contours 1&amp;2, and they belong to hierarchy-2. So for contour-0, Next contour in same hierarchy level is contour-3. And there is no previous one. And its first is child is contour-1 in hierarchy-2. It has no parent, because it is in hierarchy-1. So its hierarchy array is [3,-1,1,-1]</p>
<p>Now take contour-1. It is in hierarchy-2. Next one in same hierarchy (under the parenthood of contour-1) is contour-2. No previous one. No child, but parent is contour-0. So array is [2,-1,-1,0].</p>
<p>Similarly contour-2 : It is in hierarchy-2. There is not next contour in same hierarchy under contour-0. So no Next. Previous is contour-1. No child, parent is contour-0. So array is [-1,1,-1,0].</p>
<p>Contour - 3 : Next in hierarchy-1 is contour-5. Previous is contour-0. Child is contour-4 and no parent. So array is [5,0,4,-1].</p>
<p>Contour - 4 : It is in hierarchy 2 under contour-3 and it has no sibling. So no next, no previous, no child, parent is contour-3. So array is [-1,-1,-1,3].</p>
<p>Remaining you can fill up. This is the final answer I got:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">hierarchy</span>
<span class="go">array([[[ 3, -1,  1, -1],</span>
<span class="go">        [ 2, -1, -1,  0],</span>
<span class="go">        [-1,  1, -1,  0],</span>
<span class="go">        [ 5,  0,  4, -1],</span>
<span class="go">        [-1, -1, -1,  3],</span>
<span class="go">        [ 7,  3,  6, -1],</span>
<span class="go">        [-1, -1, -1,  5],</span>
<span class="go">        [ 8,  5, -1, -1],</span>
<span class="go">        [-1,  7, -1, -1]]])</span>
</pre></div>
</div>
</div>
<div class="section" id="retr-tree">
<h7>4. RETR_TREE<a class="headerlink" href="#retr-tree" title="Permalink to this headline">¶</a></h7>
<p>And this is the final guy, Mr.Perfect. It retrieves all the contours and creates a full family hierarchy list. <strong>It even tells, who is the grandpa, father, son, grandson and even beyond... :)</strong>.</p>
<p>For examle, I took above image, rewrite the code for cv2.RETR_TREE, reorder the contours as per the result given by OpenCV and analyze it. Again, red letters give the contour number and green letters give the hierarchy order.</p>
<blockquote>
<div><img alt="CCOMP Hierarchy" class="align-center" src="_images/tree_hierarchy.png" />
</div></blockquote>
<p>Take contour-0 : It is in hierarchy-0. Next contour in same hierarchy is contour-7. No previous contours. Child is contour-1. And no parent. So array is [7,-1,1,-1].</p>
<p>Take contour-2 : It is in hierarchy-1. No contour in same level. No previous one. Child is contour-2. Parent is contour-0. So array is [-1,-1,2,0].</p>
<p>And remaining, try yourself. Below is the full answer:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">hierarchy</span>
<span class="go">array([[[ 7, -1,  1, -1],</span>
<span class="go">        [-1, -1,  2,  0],</span>
<span class="go">        [-1, -1,  3,  1],</span>
<span class="go">        [-1, -1,  4,  2],</span>
<span class="go">        [-1, -1,  5,  3],</span>
<span class="go">        [ 6, -1, -1,  4],</span>
<span class="go">        [-1,  5, -1,  4],</span>
<span class="go">        [ 8,  0, -1, -1],</span>
<span class="go">        [-1,  7, -1, -1]]])</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="additional-resources">
<h6>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h6>
</div>
<div class="section" id="exercises">
<h6>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h6>
</div>
</div>
</div>
</div>
<span id="document-py_tutorials/py_imgproc/py_histograms/py_table_of_contents_histograms/py_table_of_contents_histograms"></span><div class="section" id="histograms-in-opencv">
<span id="table-of-content-histograms"></span><h4>Histograms in OpenCV<a class="headerlink" href="#histograms-in-opencv" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><a class="reference internal" href="index.html#histograms-getting-started"><span class="std std-ref">Histograms - 1 : Find, Plot, Analyze !!!</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="14%" />
<col width="86%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/histograms_1d.jpg"><img alt="hist_1" src="_images/histograms_1d.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Learn to find and draw Contours</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#histogram-equalization"><span class="std std-ref">Histograms - 2: Histogram Equalization</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="14%" />
<col width="86%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/histograms_equ.jpg"><img alt="hist_2" src="_images/histograms_equ.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Learn to Equalize Histograms to get better contrast for images</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#twod-histogram"><span class="std std-ref">Histograms - 3 : 2D Histograms</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="14%" />
<col width="86%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/histograms_2d.jpg"><img alt="hist_3" src="_images/histograms_2d.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Learn to find and plot 2D Histograms</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#histogram-backprojection"><span class="std std-ref">Histogram - 4 : Histogram Backprojection</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="14%" />
<col width="86%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/histograms_bp.jpg"><img alt="hist_4" src="_images/histograms_bp.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Learn histogram backprojection to segment colored objects</td>
</tr>
</tbody>
</table>
</div></blockquote>
<div class="toctree-wrapper compound">
<span id="document-py_tutorials/py_imgproc/py_histograms/py_histogram_begins/py_histogram_begins"></span><div class="section" id="histograms-1-find-plot-analyze">
<span id="histograms-getting-started"></span><h5>Histograms - 1 : Find, Plot, Analyze !!!<a class="headerlink" href="#histograms-1-find-plot-analyze" title="Permalink to this headline">¶</a></h5>
<div class="section" id="goal">
<h6>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h6>
<dl class="docutils">
<dt>Learn to</dt>
<dd><ul class="first last simple">
<li>Find histograms, using both OpenCV and Numpy functions</li>
<li>Plot histograms, using OpenCV and Matplotlib functions</li>
<li>You will see these functions : <strong>cv2.calcHist()</strong>, <strong>np.histogram()</strong> etc.</li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="theory">
<h6>Theory<a class="headerlink" href="#theory" title="Permalink to this headline">¶</a></h6>
<p>So what is histogram ? You can consider histogram as a graph or plot, which gives you an overall idea about the intensity distribution of an image. It is a plot with pixel values (ranging from 0 to 255, not always) in X-axis and corresponding number of pixels in the image on Y-axis.</p>
<p>It is just another way of understanding the image. By looking at the histogram of an image, you get intuition about contrast, brightness, intensity distribution etc of that image. Almost all image processing tools today, provides features on histogram. Below is an image from <a class="reference external" href="http://www.cambridgeincolour.com/tutorials/histograms1.htm">Cambridge in Color website</a>, and I recommend you to visit the site for more details.</p>
<blockquote>
<div><img alt="Histogram Example" class="align-center" src="_images/histogram_sample.jpg" />
</div></blockquote>
<p>You can see the image and its histogram. (Remember, this histogram is drawn for grayscale image, not color image). Left region of histogram shows the amount of darker pixels in image and right region shows the amount of brighter pixels. From the histogram, you can see dark region is more than brighter region, and amount of midtones (pixel values in mid-range, say around 127) are very less.</p>
</div>
<div class="section" id="find-histogram">
<h6>Find Histogram<a class="headerlink" href="#find-histogram" title="Permalink to this headline">¶</a></h6>
<p>Now we have an idea on what is histogram, we can look into how to find this. Both OpenCV and Numpy come with in-built function for this. Before using those functions, we need to understand some terminologies related with histograms.</p>
<p><strong>BINS</strong> :The above histogram shows the number of pixels for every pixel value, ie from 0 to 255. ie you need 256 values to show the above histogram. But consider, what if you need not find the number of pixels for all pixel values separately, but number of pixels in a interval of pixel values? say for example, you need to find the number of pixels lying between 0 to 15, then 16 to 31, ..., 240 to 255. You will need only 16 values to represent the histogram. And that is what is shown in example given in <a class="reference external" href="http://docs.opencv.org/doc/tutorials/imgproc/histograms/histogram_calculation/histogram_calculation.html#histogram-calculation">OpenCV Tutorials on histograms</a>.</p>
<p>So what you do is simply split the whole histogram to 16 sub-parts and value of each sub-part is the sum of all pixel count in it. This each sub-part is called &#8220;BIN&#8221;. In first case, number of bins where 256 (one for each pixel) while in second case, it is only 16. BINS is represented by the term <strong>histSize</strong> in OpenCV docs.</p>
<p><strong>DIMS</strong> : It is the number of parameters for which we collect the data. In this case, we collect data regarding only one thing, intensity value. So here it is 1.</p>
<p><strong>RANGE</strong> : It is the range of intensity values you want to measure. Normally, it is [0,256], ie all intensity values.</p>
<div class="section" id="histogram-calculation-in-opencv">
<h7>1. Histogram Calculation in OpenCV<a class="headerlink" href="#histogram-calculation-in-opencv" title="Permalink to this headline">¶</a></h7>
<p>So now we use <strong>cv2.calcHist()</strong> function to find the histogram. Let&#8217;s familiarize with the function and its parameters :</p>
<p class="centered">
<strong><em>cv2.calcHist(images, channels, mask, histSize, ranges[, hist[, accumulate]])</em></strong></p><ol class="arabic simple">
<li>images : it is the source image of type uint8 or float32. it should be given in square brackets, ie, &#8220;[img]&#8221;.</li>
<li>channels : it is also given in square brackets. It the index of channel for which we calculate histogram. For example, if input is grayscale image, its value is [0]. For color image, you can pass [0],[1] or [2] to calculate histogram of blue,green or red channel respectively.</li>
<li>mask : mask image. To find histogram of full image, it is given as &#8220;None&#8221;. But if you want to find histogram of particular region of image, you have to create a mask image for that and give it as mask. (I will show an example later.)</li>
<li>histSize : this represents our BIN count. Need to be given in square brackets. For full scale, we pass [256].</li>
<li>ranges : this is our RANGE. Normally, it is [0,256].</li>
</ol>
<p>So let&#8217;s start with a sample image. Simply load an image in grayscale mode and find its full histogram.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;home.jpg&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">hist</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">calcHist</span><span class="p">([</span><span class="n">img</span><span class="p">],[</span><span class="mi">0</span><span class="p">],</span><span class="kc">None</span><span class="p">,[</span><span class="mi">256</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">256</span><span class="p">])</span>
</pre></div>
</div>
<p>hist is a 256x1 array, each value corresponds to number of pixels in that image with its corresponding pixel value.</p>
</div>
<div class="section" id="histogram-calculation-in-numpy">
<h7>2. Histogram Calculation in Numpy<a class="headerlink" href="#histogram-calculation-in-numpy" title="Permalink to this headline">¶</a></h7>
<p>Numpy also provides you a function, <strong>np.histogram()</strong>. So instead of calcHist() function, you can try below line :</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">hist</span><span class="p">,</span><span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">histogram</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span><span class="mi">256</span><span class="p">,[</span><span class="mi">0</span><span class="p">,</span><span class="mi">256</span><span class="p">])</span>
</pre></div>
</div>
<p>hist is same as we calculated before. But bins will have 257 elements, because Numpy calculates bins as 0-0.99, 1-1.99, 2-2.99 etc. So final range would be 255-255.99. To represent that, they also add 256 at end of bins. But we don&#8217;t need that 256. Upto 255 is sufficient.</p>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last">Numpy has another function, <strong>np.bincount()</strong> which is much faster than (around 10X) np.histogram(). So for one-dimensional histograms, you can better try that. Don&#8217;t forget to set <code class="docutils literal"><span class="pre">minlength</span> <span class="pre">=</span> <span class="pre">256</span></code> in np.bincount. For example, <code class="docutils literal"><span class="pre">hist</span> <span class="pre">=</span> <span class="pre">np.bincount(img.ravel(),minlength=256)</span></code></p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">OpenCV function is more faster than (around 40X) than np.histogram(). So stick with OpenCV function.</p>
</div>
<p>Now we should plot histograms, but how ?</p>
</div>
</div>
<div class="section" id="plotting-histograms">
<h6>Plotting Histograms<a class="headerlink" href="#plotting-histograms" title="Permalink to this headline">¶</a></h6>
<dl class="docutils">
<dt>There are two ways for this,</dt>
<dd><ol class="first last arabic simple">
<li>Short Way : use Matplotlib plotting functions</li>
<li>Long Way : use OpenCV drawing functions</li>
</ol>
</dd>
</dl>
<div class="section" id="using-matplotlib">
<h7>1. Using Matplotlib<a class="headerlink" href="#using-matplotlib" title="Permalink to this headline">¶</a></h7>
<p>Matplotlib comes with a histogram plotting function : matplotlib.pyplot.hist()</p>
<p>It directly finds the histogram and plot it. You need not use calcHist() or np.histogram() function to find the histogram. See the code below:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;home.jpg&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span><span class="mi">256</span><span class="p">,[</span><span class="mi">0</span><span class="p">,</span><span class="mi">256</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>You will get a plot as below :</p>
<blockquote>
<div><img alt="Histogram Plotting in Matplotlib" class="align-center" src="_images/histogram_matplotlib.jpg" />
</div></blockquote>
<p>Or you can use normal plot of matplotlib, which would be good for BGR plot. For that, you need to find the histogram data first. Try below code:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;home.jpg&#39;</span><span class="p">)</span>
<span class="n">color</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span><span class="s1">&#39;g&#39;</span><span class="p">,</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">col</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">color</span><span class="p">):</span>
    <span class="n">histr</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">calcHist</span><span class="p">([</span><span class="n">img</span><span class="p">],[</span><span class="n">i</span><span class="p">],</span><span class="kc">None</span><span class="p">,[</span><span class="mi">256</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">256</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">histr</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="n">col</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">256</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Result:</p>
<blockquote>
<div><img alt="Histogram Plotting in Matplotlib" class="align-center" src="_images/histogram_rgb_plot.jpg" />
</div></blockquote>
<p>You can deduct from the above graph that, blue has some high value areas in the image (obviously it should be due to the sky)</p>
</div>
<div class="section" id="using-opencv">
<h7>2. Using OpenCV<a class="headerlink" href="#using-opencv" title="Permalink to this headline">¶</a></h7>
<p>Well, here you adjust the values of histograms along with its bin values to look like x,y coordinates so that you can draw it using cv2.line() or cv2.polyline() function to generate same image as above. This is already available with OpenCV-Python2 official samples. <a class="reference external" href="https://github.com/Itseez/opencv/raw/master/samples/python2/hist.py">Check the Code</a></p>
</div>
</div>
<div class="section" id="application-of-mask">
<h6>Application of Mask<a class="headerlink" href="#application-of-mask" title="Permalink to this headline">¶</a></h6>
<p>We used cv2.calcHist() to find the histogram of the full image. What if you want to find histograms of some regions of an image? Just create a mask image with white color on the region you want to find histogram and black otherwise. Then pass this as the mask.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;home.jpg&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># create a mask</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="n">mask</span><span class="p">[</span><span class="mi">100</span><span class="p">:</span><span class="mi">300</span><span class="p">,</span> <span class="mi">100</span><span class="p">:</span><span class="mi">400</span><span class="p">]</span> <span class="o">=</span> <span class="mi">255</span>
<span class="n">masked_img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">bitwise_and</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">img</span><span class="p">,</span><span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="p">)</span>

<span class="c1"># Calculate histogram with mask and without mask</span>
<span class="c1"># Check third argument for mask</span>
<span class="n">hist_full</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">calcHist</span><span class="p">([</span><span class="n">img</span><span class="p">],[</span><span class="mi">0</span><span class="p">],</span><span class="kc">None</span><span class="p">,[</span><span class="mi">256</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">256</span><span class="p">])</span>
<span class="n">hist_mask</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">calcHist</span><span class="p">([</span><span class="n">img</span><span class="p">],[</span><span class="mi">0</span><span class="p">],</span><span class="n">mask</span><span class="p">,[</span><span class="mi">256</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">256</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">221</span><span class="p">),</span> <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">222</span><span class="p">),</span> <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">223</span><span class="p">),</span> <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">masked_img</span><span class="p">,</span> <span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hist_full</span><span class="p">),</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hist_mask</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">256</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>See the result. In the histogram plot, blue line shows histogram of full image while green line shows histogram of masked region.</p>
<blockquote>
<div><img alt="Histogram Example" class="align-center" src="_images/histogram_masking.jpg" />
</div></blockquote>
</div>
<div class="section" id="additional-resources">
<h6>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h6>
<ol class="arabic simple">
<li><a class="reference external" href="http://www.cambridgeincolour.com/tutorials/histograms1.htm">Cambridge in Color website</a></li>
</ol>
</div>
<div class="section" id="exercises">
<h6>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h6>
</div>
</div>
<span id="document-py_tutorials/py_imgproc/py_histograms/py_histogram_equalization/py_histogram_equalization"></span><div class="section" id="histograms-2-histogram-equalization">
<span id="histogram-equalization"></span><h5>Histograms - 2: Histogram Equalization<a class="headerlink" href="#histograms-2-histogram-equalization" title="Permalink to this headline">¶</a></h5>
<div class="section" id="goal">
<h6>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h6>
<p>In this section,</p>
<blockquote>
<div><ul class="simple">
<li>We will learn the concepts of histogram equalization and use it to improve the contrast of our images.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="theory">
<h6>Theory<a class="headerlink" href="#theory" title="Permalink to this headline">¶</a></h6>
<p>Consider an image whose pixel values are confined to some specific range of values only. For eg, brighter image will have all pixels confined to high values. But a good image will have pixels from all regions of the image. So you need to stretch this histogram to either ends (as given in below image, from wikipedia) and that is what Histogram Equalization does (in simple words). This normally improves the contrast of the image.</p>
<blockquote>
<div><img alt="Histograms Equalization" class="align-center" src="_images/histogram_equalization.png" />
</div></blockquote>
<p>I would recommend you to read the wikipedia page on <a class="reference external" href="http://en.wikipedia.org/wiki/Histogram_equalization">Histogram Equalization</a> for more details about it. It has a very good explanation with worked out examples, so that you would understand almost everything after reading that. Instead, here we will see its Numpy implementation. After that, we will see OpenCV function.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;wiki.jpg&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>

<span class="n">hist</span><span class="p">,</span><span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">histogram</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span><span class="mi">256</span><span class="p">,[</span><span class="mi">0</span><span class="p">,</span><span class="mi">256</span><span class="p">])</span>

<span class="n">cdf</span> <span class="o">=</span> <span class="n">hist</span><span class="o">.</span><span class="n">cumsum</span><span class="p">()</span>
<span class="n">cdf_normalized</span> <span class="o">=</span> <span class="n">cdf</span> <span class="o">*</span> <span class="n">hist</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">/</span> <span class="n">cdf</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cdf_normalized</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;b&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span><span class="mi">256</span><span class="p">,[</span><span class="mi">0</span><span class="p">,</span><span class="mi">256</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">256</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">((</span><span class="s1">&#39;cdf&#39;</span><span class="p">,</span><span class="s1">&#39;histogram&#39;</span><span class="p">),</span> <span class="n">loc</span> <span class="o">=</span> <span class="s1">&#39;upper left&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img alt="Histograms Equalization" class="align-center" src="_images/histeq_numpy1.jpg" />
<p>You can see histogram lies in brighter region. We need the full spectrum. For that, we need a transformation function which maps the input pixels in brighter region to output pixels in full region. That is what histogram equalization does.</p>
<p>Now we find the minimum histogram value (excluding 0) and apply the histogram equalization equation as given in wiki page. But I have used here, the masked array concept array from Numpy. For masked array, all operations are performed on non-masked elements. You can read more about it from Numpy docs on masked arrays.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">cdf_m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ma</span><span class="o">.</span><span class="n">masked_equal</span><span class="p">(</span><span class="n">cdf</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">cdf_m</span> <span class="o">=</span> <span class="p">(</span><span class="n">cdf_m</span> <span class="o">-</span> <span class="n">cdf_m</span><span class="o">.</span><span class="n">min</span><span class="p">())</span><span class="o">*</span><span class="mi">255</span><span class="o">/</span><span class="p">(</span><span class="n">cdf_m</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">-</span><span class="n">cdf_m</span><span class="o">.</span><span class="n">min</span><span class="p">())</span>
<span class="n">cdf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ma</span><span class="o">.</span><span class="n">filled</span><span class="p">(</span><span class="n">cdf_m</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;uint8&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Now we have the look-up table that gives us the information on what is the output pixel value for every input pixel value. So we just apply the transform.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">img2</span> <span class="o">=</span> <span class="n">cdf</span><span class="p">[</span><span class="n">img</span><span class="p">]</span>
</pre></div>
</div>
<p>Now we calculate its histogram and cdf as before ( you do it) and result looks like below :</p>
<img alt="Histograms Equalization" class="align-center" src="_images/histeq_numpy2.jpg" />
<p>Another important feature is that, even if the image was a darker image (instead of a brighter one we used), after equalization we will get almost the same image as we got. As a result, this is used as a &#8220;reference tool&#8221; to make all images with same lighting conditions. This is useful in many cases. For example, in face recognition, before training the face data, the images of faces are histogram equalized to make them all with same lighting conditions.</p>
</div>
<div class="section" id="histograms-equalization-in-opencv">
<h6>Histograms Equalization in OpenCV<a class="headerlink" href="#histograms-equalization-in-opencv" title="Permalink to this headline">¶</a></h6>
<p>OpenCV has a function to do this, <strong>cv2.equalizeHist()</strong>. Its input is just grayscale image and output is our histogram equalized image.</p>
<p>Below is a simple code snippet showing its usage for same image we used :</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;wiki.jpg&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">equ</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">equalizeHist</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">img</span><span class="p">,</span><span class="n">equ</span><span class="p">))</span> <span class="c1">#stacking images side-by-side</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">imwrite</span><span class="p">(</span><span class="s1">&#39;res.png&#39;</span><span class="p">,</span><span class="n">res</span><span class="p">)</span>
</pre></div>
</div>
<img alt="Histograms Equalization" class="align-center" src="_images/equalization_opencv.jpg" />
<p>So now you can take different images with different light conditions, equalize it and check the results.</p>
<p>Histogram equalization is good when histogram of the image is confined to a particular region. It won&#8217;t work good in places where there is large intensity variations where histogram covers a large region, ie both bright and dark pixels are present. Please check the SOF links in Additional Resources.</p>
</div>
<div class="section" id="clahe-contrast-limited-adaptive-histogram-equalization">
<h6>CLAHE (Contrast Limited Adaptive Histogram Equalization)<a class="headerlink" href="#clahe-contrast-limited-adaptive-histogram-equalization" title="Permalink to this headline">¶</a></h6>
<p>The first histogram equalization we just saw, considers the global contrast of the image. In many cases, it is not a good idea. For example, below image shows an input image and its result after global histogram equalization.</p>
<blockquote>
<div><img alt="Problem of Global HE" class="align-center" src="_images/clahe_1.jpg" />
</div></blockquote>
<p>It is true that the background contrast has improved after histogram equalization. But compare the face of statue in both images. We lost most of the information there due to over-brightness. It is because its histogram is not confined to a particular region as we saw in previous cases (Try to plot histogram of input image, you will get more intuition).</p>
<p>So to solve this problem, <strong>adaptive histogram equalization</strong> is used. In this, image is divided into small blocks called &#8220;tiles&#8221; (tileSize is 8x8 by default in OpenCV). Then each of these blocks are histogram equalized as usual. So in a small area, histogram would confine to a small region (unless there is noise). If noise is there, it will be amplified. To avoid this, <strong>contrast limiting</strong> is applied. If any histogram bin is above the specified contrast limit (by default 40 in OpenCV), those pixels are clipped and distributed uniformly to other bins before applying histogram equalization. After equalization, to remove artifacts in tile borders, bilinear interpolation is applied.</p>
<p>Below code snippet shows how to apply CLAHE in OpenCV:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cv2</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;tsukuba_l.png&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># create a CLAHE object (Arguments are optional).</span>
<span class="n">clahe</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">createCLAHE</span><span class="p">(</span><span class="n">clipLimit</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">tileGridSize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">cl1</span> <span class="o">=</span> <span class="n">clahe</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>

<span class="n">cv2</span><span class="o">.</span><span class="n">imwrite</span><span class="p">(</span><span class="s1">&#39;clahe_2.jpg&#39;</span><span class="p">,</span><span class="n">cl1</span><span class="p">)</span>
</pre></div>
</div>
<p>See the result below and compare it with results above, especially the statue region:</p>
<blockquote>
<div><img alt="Result of CLAHE" class="align-center" src="_images/clahe_2.jpg" />
</div></blockquote>
</div>
<div class="section" id="additional-resources">
<h6>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h6>
<ol class="arabic simple">
<li>Wikipedia page on <a class="reference external" href="http://en.wikipedia.org/wiki/Histogram_equalization">Histogram Equalization</a></li>
<li><a class="reference external" href="http://docs.scipy.org/doc/numpy/reference/maskedarray.html">Masked Arrays in Numpy</a></li>
</ol>
<p>Also check these SOF questions regarding contrast adjustment:</p>
<ol class="arabic simple" start="3">
<li><a class="reference external" href="http://stackoverflow.com/questions/10549245/how-can-i-adjust-contrast-in-opencv-in-c">How can I adjust contrast in OpenCV in C?</a></li>
<li><a class="reference external" href="http://stackoverflow.com/questions/10561222/how-do-i-equalize-contrast-brightness-of-images-using-opencv">How do I equalize contrast &amp; brightness of images using opencv?</a></li>
</ol>
</div>
<div class="section" id="exercises">
<h6>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h6>
</div>
</div>
<span id="document-py_tutorials/py_imgproc/py_histograms/py_2d_histogram/py_2d_histogram"></span><div class="section" id="histograms-3-2d-histograms">
<span id="twod-histogram"></span><h5>Histograms - 3 : 2D Histograms<a class="headerlink" href="#histograms-3-2d-histograms" title="Permalink to this headline">¶</a></h5>
<div class="section" id="goal">
<h6>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h6>
<p>In this chapter, we will learn to find and plot 2D histograms. It will be helpful in coming chapters.</p>
</div>
<div class="section" id="introduction">
<h6>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h6>
<p>In the first article, we calculated and plotted one-dimensional histogram. It is called one-dimensional because we are taking only one feature into our consideration, ie grayscale intensity value of the pixel. But in two-dimensional histograms, you consider two features. Normally it is used for finding color histograms where two features are Hue &amp; Saturation values of every pixel.</p>
<p>There is a <a class="reference external" href="https://github.com/Itseez/opencv/blob/master/samples/python2/color_histogram.py">python sample in the official samples</a> already for finding color histograms. We will try to understand how to create such a color histogram, and it will be useful in understanding further topics like Histogram Back-Projection.</p>
</div>
<div class="section" id="d-histogram-in-opencv">
<h6>2D Histogram in OpenCV<a class="headerlink" href="#d-histogram-in-opencv" title="Permalink to this headline">¶</a></h6>
<p>It is quite simple and calculated using the same function, <strong>cv2.calcHist()</strong>. For color histograms, we need to convert the image from BGR to HSV. (Remember, for 1D histogram, we converted from BGR to Grayscale). For 2D histograms, its parameters will be modified as follows:</p>
<ul class="simple">
<li><strong>channels = [0,1]</strong> <em>because we need to process both H and S plane.</em></li>
<li><strong>bins = [180,256]</strong> <em>180 for H plane and 256 for S plane.</em></li>
<li><strong>range = [0,180,0,256]</strong> <em>Hue value lies between 0 and 180 &amp; Saturation lies between 0 and 256.</em></li>
</ul>
<p>Now check the code below:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;home.jpg&#39;</span><span class="p">)</span>
<span class="n">hsv</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2HSV</span><span class="p">)</span>

<span class="n">hist</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">calcHist</span><span class="p">([</span><span class="n">hsv</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="kc">None</span><span class="p">,</span> <span class="p">[</span><span class="mi">180</span><span class="p">,</span> <span class="mi">256</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">180</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">256</span><span class="p">])</span>
</pre></div>
</div>
<p>That&#8217;s it.</p>
</div>
<div class="section" id="d-histogram-in-numpy">
<h6>2D Histogram in Numpy<a class="headerlink" href="#d-histogram-in-numpy" title="Permalink to this headline">¶</a></h6>
<p>Numpy also provides a specific function for this : <strong>np.histogram2d()</strong>. (Remember, for 1D histogram we used <strong>np.histogram()</strong> ).</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;home.jpg&#39;</span><span class="p">)</span>
<span class="n">hsv</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2HSV</span><span class="p">)</span>

<span class="n">hist</span><span class="p">,</span> <span class="n">xbins</span><span class="p">,</span> <span class="n">ybins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">histogram2d</span><span class="p">(</span><span class="n">h</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span><span class="n">s</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),[</span><span class="mi">180</span><span class="p">,</span><span class="mi">256</span><span class="p">],[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">180</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">256</span><span class="p">]])</span>
</pre></div>
</div>
<p>First argument is H plane, second one is the S plane, third is number of bins for each and fourth is their range.</p>
<p>Now we can check how to plot this color histogram.</p>
</div>
<div class="section" id="plotting-2d-histograms">
<h6>Plotting 2D Histograms<a class="headerlink" href="#plotting-2d-histograms" title="Permalink to this headline">¶</a></h6>
<div class="section" id="method-1-using-cv2-imshow">
<h7>Method - 1 : Using cv2.imshow()<a class="headerlink" href="#method-1-using-cv2-imshow" title="Permalink to this headline">¶</a></h7>
<p>The result we get is a two dimensional array of size 180x256. So we can show them as we do normally, using cv2.imshow() function. It will be a grayscale image and it won&#8217;t give much idea what colors are there, unless you know the Hue values of different colors.</p>
</div>
<div class="section" id="method-2-using-matplotlib">
<h7>Method - 2 : Using Matplotlib<a class="headerlink" href="#method-2-using-matplotlib" title="Permalink to this headline">¶</a></h7>
<p>We can use <strong>matplotlib.pyplot.imshow()</strong> function to plot 2D histogram with different color maps. It gives us much more better idea about the different pixel density. But this also, doesn&#8217;t gives us idea what color is there on a first look, unless you know the Hue values of different colors. Still I prefer this method. It is simple and better.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">While using this function, remember, interpolation flag should be <code class="docutils literal"><span class="pre">nearest</span></code> for better results.</p>
</div>
<p>Consider code:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;home.jpg&#39;</span><span class="p">)</span>
<span class="n">hsv</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2HSV</span><span class="p">)</span>
<span class="n">hist</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">calcHist</span><span class="p">(</span> <span class="p">[</span><span class="n">hsv</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="kc">None</span><span class="p">,</span> <span class="p">[</span><span class="mi">180</span><span class="p">,</span> <span class="mi">256</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">180</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">256</span><span class="p">]</span> <span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">hist</span><span class="p">,</span><span class="n">interpolation</span> <span class="o">=</span> <span class="s1">&#39;nearest&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Below is the input image and its color histogram plot. X axis shows S values and Y axis shows Hue.</p>
<blockquote>
<div><img alt="2D Histograms" class="align-center" src="_images/2dhist_matplotlib.jpg" />
</div></blockquote>
<p>In histogram, you can see some high values near H = 100 and S = 200. It corresponds to blue of sky. Similarly another peak can be seen near H = 25 and S = 100. It corresponds to yellow of the palace. You can verify it with any image editing tools like GIMP.</p>
</div>
<div class="section" id="method-3-opencv-sample-style">
<h7>Method 3 : OpenCV sample style !!<a class="headerlink" href="#method-3-opencv-sample-style" title="Permalink to this headline">¶</a></h7>
<p>There is a <a class="reference external" href="https://github.com/Itseez/opencv/blob/master/samples/python2/color_histogram.py">sample code for color-histogram in OpenCV-Python2 samples</a>. If you run the code, you can see the histogram shows the corresponding color also. Or simply it outputs a color coded histogram. Its result is very good (although you need to add extra bunch of lines).</p>
<p>In that code, the author created a color map in HSV. Then converted it into BGR. The resulting histogram image is multiplied with this color map. He also uses some preprocessing steps to remove small isolated pixels, resulting in a good histogram.</p>
<p>I leave it to the readers to run the code, analyze it and have your own hack arounds. Below is the output of that code for the same image as above:</p>
<blockquote>
<div><img alt="2D Histograms using OpenCV-Python Samples" class="align-center" src="_images/2dhist_opencv.jpg" />
</div></blockquote>
<p>You can clearly see in the histogram what colors are present, blue is there, yellow is there, and some white due to chessboard is there. Nice !!!</p>
</div>
</div>
<div class="section" id="additional-resources">
<h6>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h6>
</div>
<div class="section" id="exercises">
<h6>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h6>
</div>
</div>
<span id="document-py_tutorials/py_imgproc/py_histograms/py_histogram_backprojection/py_histogram_backprojection"></span><div class="section" id="histogram-4-histogram-backprojection">
<span id="histogram-backprojection"></span><h5>Histogram - 4 : Histogram Backprojection<a class="headerlink" href="#histogram-4-histogram-backprojection" title="Permalink to this headline">¶</a></h5>
<div class="section" id="goal">
<h6>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h6>
<p>In this chapter, we will learn about histogram backprojection.</p>
</div>
<div class="section" id="theory">
<h6>Theory<a class="headerlink" href="#theory" title="Permalink to this headline">¶</a></h6>
<p>It was proposed by <strong>Michael J. Swain , Dana H. Ballard</strong> in their paper <strong>Indexing via color histograms</strong>.</p>
<p><strong>What is it actually in simple words?</strong> It is used for image segmentation or finding objects of interest in an image. In simple words, it creates an image of the same size (but single channel) as that of our input image, where each pixel corresponds to the probability of that pixel belonging to our object. In more simpler worlds, the output image will have our object of interest in more white compared to remaining part. Well, that is an intuitive explanation. (I can&#8217;t make it more simpler). Histogram Backprojection is used with camshift algorithm etc.</p>
<p><strong>How do we do it ?</strong> We create a histogram of an image containing our object of interest (in our case, the ground, leaving player and other things). The object should fill the image as far as possible for better results. And a color histogram is preferred over grayscale histogram, because color of the object is more better way to define the object than its grayscale intensity. We then &#8220;back-project&#8221; this histogram over our test image where we need to find the object, ie in other words, we calculate the probability of every pixel belonging to the ground and show it. The resulting output on proper thresholding gives us the ground alone.</p>
</div>
<div class="section" id="algorithm-in-numpy">
<h6>Algorithm in Numpy<a class="headerlink" href="#algorithm-in-numpy" title="Permalink to this headline">¶</a></h6>
<p>1. First we need to calculate the color histogram of both the object we need to find (let it be &#8216;M&#8217;) and the image where we are going to search (let it be &#8216;I&#8217;).</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1">#roi is the object or region of object we need to find</span>
<span class="n">roi</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;rose_red.png&#39;</span><span class="p">)</span>
<span class="n">hsv</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">roi</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2HSV</span><span class="p">)</span>

<span class="c1">#target is the image we search in</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;rose.png&#39;</span><span class="p">)</span>
<span class="n">hsvt</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">target</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2HSV</span><span class="p">)</span>

<span class="c1"># Find the histograms using calcHist. Can be done with np.histogram2d also</span>
<span class="n">M</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">calcHist</span><span class="p">([</span><span class="n">hsv</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="kc">None</span><span class="p">,</span> <span class="p">[</span><span class="mi">180</span><span class="p">,</span> <span class="mi">256</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">180</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">256</span><span class="p">]</span> <span class="p">)</span>
<span class="n">I</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">calcHist</span><span class="p">([</span><span class="n">hsvt</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="kc">None</span><span class="p">,</span> <span class="p">[</span><span class="mi">180</span><span class="p">,</span> <span class="mi">256</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">180</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">256</span><span class="p">]</span> <span class="p">)</span>
</pre></div>
</div>
<p>2. Find the ratio <img class="math" src="_images/math/694236c9c9fcce463a4cd4a1ec8c22a036f921cc.png" alt="R = \frac{M}{I}"/>. Then backproject R, ie use R as palette and create a new image with every pixel as its corresponding probability of being target. ie <code class="docutils literal"><span class="pre">B(x,y)</span> <span class="pre">=</span> <span class="pre">R[h(x,y),s(x,y)]</span></code> where h is hue and s is saturation of the pixel at (x,y). After that apply the condition <img class="math" src="_images/math/903c944c75e80f10d9262a92130bda19bc1b1458.png" alt="B(x,y) = min[B(x,y), 1]"/>.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">h</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">v</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">hsvt</span><span class="p">)</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">R</span><span class="p">[</span><span class="n">h</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span><span class="n">s</span><span class="o">.</span><span class="n">ravel</span><span class="p">()]</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">B</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">B</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">hsvt</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">])</span>
</pre></div>
</div>
<p>3. Now apply a convolution with a circular disc, <img class="math" src="_images/math/659953c73150240d6dd2cfb35df6579d5cf4c174.png" alt="B = D \ast B"/>, where D is the disc kernel.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">disc</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">getStructuringElement</span><span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">MORPH_ELLIPSE</span><span class="p">,(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">filter2D</span><span class="p">(</span><span class="n">B</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">disc</span><span class="p">,</span><span class="n">B</span><span class="p">)</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">B</span><span class="p">,</span><span class="n">B</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">NORM_MINMAX</span><span class="p">)</span>
</pre></div>
</div>
<p>4. Now the location of maximum intensity gives us the location of object. If we are expecting a region in the image, thresholding for a suitable value gives a nice result.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">ret</span><span class="p">,</span><span class="n">thresh</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">threshold</span><span class="p">(</span><span class="n">B</span><span class="p">,</span><span class="mi">50</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>That&#8217;s it !!</p>
</div>
<div class="section" id="backprojection-in-opencv">
<h6>Backprojection in OpenCV<a class="headerlink" href="#backprojection-in-opencv" title="Permalink to this headline">¶</a></h6>
<p>OpenCV provides an inbuilt function <strong>cv2.calcBackProject()</strong>. Its parameters are almost same as the <strong>cv2.calcHist()</strong> function. One of its parameter is histogram which is histogram of the object and we have to find it. Also, the object histogram should be normalized before passing on to the backproject function. It returns the probability image. Then we convolve the image with a disc kernel and apply threshold. Below is my code and output :</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">roi</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;rose_red.png&#39;</span><span class="p">)</span>
<span class="n">hsv</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">roi</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2HSV</span><span class="p">)</span>

<span class="n">target</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;rose.png&#39;</span><span class="p">)</span>
<span class="n">hsvt</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">target</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2HSV</span><span class="p">)</span>

<span class="c1"># calculating object histogram</span>
<span class="n">roihist</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">calcHist</span><span class="p">([</span><span class="n">hsv</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="kc">None</span><span class="p">,</span> <span class="p">[</span><span class="mi">180</span><span class="p">,</span> <span class="mi">256</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">180</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">256</span><span class="p">]</span> <span class="p">)</span>

<span class="c1"># normalize histogram and apply backprojection</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">roihist</span><span class="p">,</span><span class="n">roihist</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">NORM_MINMAX</span><span class="p">)</span>
<span class="n">dst</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">calcBackProject</span><span class="p">([</span><span class="n">hsvt</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">roihist</span><span class="p">,[</span><span class="mi">0</span><span class="p">,</span><span class="mi">180</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">256</span><span class="p">],</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Now convolute with circular disc</span>
<span class="n">disc</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">getStructuringElement</span><span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">MORPH_ELLIPSE</span><span class="p">,(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">filter2D</span><span class="p">(</span><span class="n">dst</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">disc</span><span class="p">,</span><span class="n">dst</span><span class="p">)</span>

<span class="c1"># threshold and binary AND</span>
<span class="n">ret</span><span class="p">,</span><span class="n">thresh</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">threshold</span><span class="p">(</span><span class="n">dst</span><span class="p">,</span><span class="mi">50</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">thresh</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">merge</span><span class="p">((</span><span class="n">thresh</span><span class="p">,</span><span class="n">thresh</span><span class="p">,</span><span class="n">thresh</span><span class="p">))</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">bitwise_and</span><span class="p">(</span><span class="n">target</span><span class="p">,</span><span class="n">thresh</span><span class="p">)</span>

<span class="n">res</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">target</span><span class="p">,</span><span class="n">thresh</span><span class="p">,</span><span class="n">res</span><span class="p">))</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">imwrite</span><span class="p">(</span><span class="s1">&#39;res.jpg&#39;</span><span class="p">,</span><span class="n">res</span><span class="p">)</span>
</pre></div>
</div>
<p>Below is one example I worked with. I used the region inside blue rectangle as sample object and I wanted to extract the full ground.</p>
<blockquote>
<div><img alt="Histogram Backprojection in OpenCV" class="align-center" src="_images/backproject_opencv.jpg" />
</div></blockquote>
</div>
<div class="section" id="additional-resources">
<h6>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h6>
<ol class="arabic simple">
<li>&#8220;Indexing via color histograms&#8221;, Swain, Michael J. , Third international conference on computer vision,1990.</li>
</ol>
</div>
<div class="section" id="exercises">
<h6>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h6>
</div>
</div>
</div>
</div>
<span id="document-py_tutorials/py_imgproc/py_transforms/py_table_of_contents_transforms/py_table_of_contents_transforms"></span><div class="section" id="image-transforms-in-opencv">
<span id="table-of-content-transforms"></span><h4>Image Transforms in OpenCV<a class="headerlink" href="#image-transforms-in-opencv" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><a class="reference internal" href="index.html#fourier-transform"><span class="std std-ref">Fourier Transform</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="16%" />
<col width="84%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/transform_fourier.jpg"><img alt="transform_1" src="_images/transform_fourier.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Learn to find the Fourier Transform of images</td>
</tr>
</tbody>
</table>
</div></blockquote>
<div class="toctree-wrapper compound">
<span id="document-py_tutorials/py_imgproc/py_transforms/py_fourier_transform/py_fourier_transform"></span><div class="section" id="fourier-transform">
<span id="id1"></span><h5>Fourier Transform<a class="headerlink" href="#fourier-transform" title="Permalink to this headline">¶</a></h5>
<div class="section" id="goal">
<h6>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h6>
<dl class="docutils">
<dt>In this section, we will learn</dt>
<dd><ul class="first last simple">
<li>To find the Fourier Transform of images using OpenCV</li>
<li>To utilize the FFT functions available in Numpy</li>
<li>Some applications of Fourier Transform</li>
<li>We will see following functions : <strong>cv2.dft()</strong>, <strong>cv2.idft()</strong> etc</li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="theory">
<h6>Theory<a class="headerlink" href="#theory" title="Permalink to this headline">¶</a></h6>
<p>Fourier Transform is used to analyze the frequency characteristics of various filters. For images, <strong>2D Discrete Fourier Transform (DFT)</strong> is used to find the frequency domain. A fast algorithm called <strong>Fast Fourier Transform (FFT)</strong> is used for calculation of DFT. Details about these can be found in any image processing or signal processing textbooks. Please see <a class="reference internal" href="#additional-resources">Additional Resources</a> section.</p>
<p>For a sinusoidal signal, <img class="math" src="_images/math/0ece7e4c0413ba0b6db531b0160b88f79553df37.png" alt="x(t) = A \sin(2 \pi ft)"/>, we can say <img class="math" src="_images/math/0001d02b63ede2fe3219e05a7cd09c82ae6298b6.png" alt="f"/> is the frequency of signal, and if its frequency domain is taken, we can see a spike at <img class="math" src="_images/math/0001d02b63ede2fe3219e05a7cd09c82ae6298b6.png" alt="f"/>. If signal is sampled to form a discrete signal, we get the same frequency domain, but is periodic in the range <img class="math" src="_images/math/a1e7f52ec7d364f52ff7bb98a49d28a3ccce5756.png" alt="[- \pi, \pi]"/> or <img class="math" src="_images/math/afd1cc9f6085210dc7aea0e591b90df796cff56c.png" alt="[0,2\pi]"/> (or <img class="math" src="_images/math/2efe13038e0b745725e05c174d88172d3b57de17.png" alt="[0,N]"/> for N-point DFT). You can consider an image as a signal which is sampled in two directions. So taking fourier transform in both X and Y directions gives you the frequency representation of image.</p>
<p>More intuitively, for the sinusoidal signal, if the amplitude varies so fast in short time, you can say it is a high frequency signal. If it varies slowly, it is a low frequency signal. You can extend the same idea to images. Where does the amplitude varies drastically in images ? At the edge points, or noises. So we can say, edges and noises are high frequency contents in an image. If there is no much changes in amplitude, it is a low frequency component. ( Some links are added to <a class="reference internal" href="#additional-resources">Additional Resources</a> which explains frequency transform intuitively with examples).</p>
<p>Now we will see how to find the Fourier Transform.</p>
</div>
<div class="section" id="fourier-transform-in-numpy">
<h6>Fourier Transform in Numpy<a class="headerlink" href="#fourier-transform-in-numpy" title="Permalink to this headline">¶</a></h6>
<p>First we will see how to find Fourier Transform using Numpy. Numpy has an FFT package to do this. <strong>np.fft.fft2()</strong> provides us the frequency transform which will be a complex array. Its first argument is the input image, which is grayscale. Second argument is optional which decides the size of output array. If it is greater than size of input image, input image is padded with zeros before calculation of FFT. If it is less than input image, input image will be cropped. If no arguments passed, Output array size will be same as input.</p>
<p>Now once you got the result, zero frequency component (DC component) will be at top left corner. If you want to bring it to center, you need to shift the result by <img class="math" src="_images/math/302d1509aac2b5425d39c6a24aecd993e8c86d4b.png" alt="\frac{N}{2}"/> in both the directions. This is simply done by the function, <strong>np.fft.fftshift()</strong>. (It is more easier to analyze). Once you found the frequency transform, you can find the magnitude spectrum.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;messi5.jpg&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">fft2</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="n">fshift</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">fftshift</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
<span class="n">magnitude_spectrum</span> <span class="o">=</span> <span class="mi">20</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">fshift</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Input Image&#39;</span><span class="p">),</span> <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([]),</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">magnitude_spectrum</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Magnitude Spectrum&#39;</span><span class="p">),</span> <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([]),</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Result look like below:</p>
<blockquote>
<div><img alt="Magnitude Spectrum" class="align-center" src="_images/fft1.jpg" />
</div></blockquote>
<p>See, You can see more whiter region at the center showing low frequency content is more.</p>
<p>So you found the frequency transform Now you can do some operations in frequency domain, like high pass filtering and reconstruct the image, ie find inverse DFT. For that you simply remove the low frequencies by masking with a rectangular window of size 60x60. Then apply the inverse shift using <strong>np.fft.ifftshift()</strong> so that DC component again come at the top-left corner. Then find inverse FFT using <strong>np.ifft2()</strong> function. The result, again, will be a complex number. You can take its absolute value.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">rows</span><span class="p">,</span> <span class="n">cols</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">shape</span>
<span class="n">crow</span><span class="p">,</span><span class="n">ccol</span> <span class="o">=</span> <span class="n">rows</span><span class="o">/</span><span class="mi">2</span> <span class="p">,</span> <span class="n">cols</span><span class="o">/</span><span class="mi">2</span>
<span class="n">fshift</span><span class="p">[</span><span class="n">crow</span><span class="o">-</span><span class="mi">30</span><span class="p">:</span><span class="n">crow</span><span class="o">+</span><span class="mi">30</span><span class="p">,</span> <span class="n">ccol</span><span class="o">-</span><span class="mi">30</span><span class="p">:</span><span class="n">ccol</span><span class="o">+</span><span class="mi">30</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">f_ishift</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">ifftshift</span><span class="p">(</span><span class="n">fshift</span><span class="p">)</span>
<span class="n">img_back</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">ifft2</span><span class="p">(</span><span class="n">f_ishift</span><span class="p">)</span>
<span class="n">img_back</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">img_back</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">131</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Input Image&#39;</span><span class="p">),</span> <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([]),</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">132</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img_back</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Image after HPF&#39;</span><span class="p">),</span> <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([]),</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img_back</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Result in JET&#39;</span><span class="p">),</span> <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([]),</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Result look like below:</p>
<blockquote>
<div><img alt="High Pass Filtering" class="align-center" src="_images/fft2.jpg" />
</div></blockquote>
<p>The result shows High Pass Filtering is an edge detection operation. This is what we have seen in Image Gradients chapter. This also shows that most of the image data is present in the Low frequency region of the spectrum. Anyway we have seen how to find DFT, IDFT etc in Numpy. Now let&#8217;s see how to do it in OpenCV.</p>
<p>If you closely watch the result, especially the last image in JET color, you can see some artifacts (One instance I have marked in red arrow). It shows some ripple like structures there, and it is called <strong>ringing effects</strong>. It is caused by the rectangular window we used for masking. This mask is converted to sinc shape which causes this problem. So rectangular windows is not used for filtering. Better option is Gaussian Windows.</p>
</div>
<div class="section" id="fourier-transform-in-opencv">
<h6>Fourier Transform in OpenCV<a class="headerlink" href="#fourier-transform-in-opencv" title="Permalink to this headline">¶</a></h6>
<p>OpenCV provides the functions <strong>cv2.dft()</strong> and <strong>cv2.idft()</strong> for this. It returns the same result as previous, but with two channels. First channel will have the real part of the result and second channel will have the imaginary part of the result. The input image should be converted to np.float32 first. We will see how to do it.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;messi5.jpg&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>

<span class="n">dft</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">dft</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">img</span><span class="p">),</span><span class="n">flags</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">DFT_COMPLEX_OUTPUT</span><span class="p">)</span>
<span class="n">dft_shift</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">fftshift</span><span class="p">(</span><span class="n">dft</span><span class="p">)</span>

<span class="n">magnitude_spectrum</span> <span class="o">=</span> <span class="mi">20</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">magnitude</span><span class="p">(</span><span class="n">dft_shift</span><span class="p">[:,:,</span><span class="mi">0</span><span class="p">],</span><span class="n">dft_shift</span><span class="p">[:,:,</span><span class="mi">1</span><span class="p">]))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Input Image&#39;</span><span class="p">),</span> <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([]),</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">magnitude_spectrum</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Magnitude Spectrum&#39;</span><span class="p">),</span> <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([]),</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">You can also use <strong>cv2.cartToPolar()</strong> which returns both magnitude and phase in a single shot</p>
</div>
<p>So, now we have to do inverse DFT. In previous session, we created a HPF, this time we will see how to remove high frequency contents in the image, ie we apply LPF to image. It actually blurs the image. For this, we create a mask first with high value (1) at low frequencies, ie we pass the LF content, and 0 at HF region.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">rows</span><span class="p">,</span> <span class="n">cols</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">shape</span>
<span class="n">crow</span><span class="p">,</span><span class="n">ccol</span> <span class="o">=</span> <span class="n">rows</span><span class="o">/</span><span class="mi">2</span> <span class="p">,</span> <span class="n">cols</span><span class="o">/</span><span class="mi">2</span>

<span class="c1"># create a mask first, center square is 1, remaining all zeros</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">rows</span><span class="p">,</span><span class="n">cols</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="n">mask</span><span class="p">[</span><span class="n">crow</span><span class="o">-</span><span class="mi">30</span><span class="p">:</span><span class="n">crow</span><span class="o">+</span><span class="mi">30</span><span class="p">,</span> <span class="n">ccol</span><span class="o">-</span><span class="mi">30</span><span class="p">:</span><span class="n">ccol</span><span class="o">+</span><span class="mi">30</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># apply mask and inverse DFT</span>
<span class="n">fshift</span> <span class="o">=</span> <span class="n">dft_shift</span><span class="o">*</span><span class="n">mask</span>
<span class="n">f_ishift</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">ifftshift</span><span class="p">(</span><span class="n">fshift</span><span class="p">)</span>
<span class="n">img_back</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">idft</span><span class="p">(</span><span class="n">f_ishift</span><span class="p">)</span>
<span class="n">img_back</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">magnitude</span><span class="p">(</span><span class="n">img_back</span><span class="p">[:,:,</span><span class="mi">0</span><span class="p">],</span><span class="n">img_back</span><span class="p">[:,:,</span><span class="mi">1</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Input Image&#39;</span><span class="p">),</span> <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([]),</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img_back</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Magnitude Spectrum&#39;</span><span class="p">),</span> <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([]),</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>See the result:</p>
<blockquote>
<div><img alt="Magnitude Spectrum" class="align-center" src="_images/fft4.jpg" />
</div></blockquote>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">As usual, OpenCV functions <strong>cv2.dft()</strong> and <strong>cv2.idft()</strong> are faster than Numpy counterparts. But Numpy functions are more user-friendly. For more details about performance issues, see below section.</p>
</div>
</div>
<div class="section" id="performance-optimization-of-dft">
<h6>Performance Optimization of DFT<a class="headerlink" href="#performance-optimization-of-dft" title="Permalink to this headline">¶</a></h6>
<p>Performance of DFT calculation is better for some array size. It is fastest when array size is power of two. The arrays whose size is a product of 2’s, 3’s, and 5’s are also processed quite efficiently. So if you are worried about the performance of your code, you can modify the size of the array to any optimal size (by padding zeros) before finding DFT. For OpenCV, you have to manually pad zeros. But for Numpy, you specify the new size of FFT calculation, and it will automatically pad zeros for you.</p>
<p>So how do we find this optimal size ? OpenCV provides a function, <strong>cv2.getOptimalDFTSize()</strong> for this. It is applicable to both <strong>cv2.dft()</strong> and <strong>np.fft.fft2()</strong>. Let&#8217;s check their performance using IPython magic command <code class="docutils literal"><span class="pre">%timeit</span></code>.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">In</span> <span class="p">[</span><span class="mi">16</span><span class="p">]:</span> <span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;messi5.jpg&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">In</span> <span class="p">[</span><span class="mi">17</span><span class="p">]:</span> <span class="n">rows</span><span class="p">,</span><span class="n">cols</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">shape</span>
<span class="n">In</span> <span class="p">[</span><span class="mi">18</span><span class="p">]:</span> <span class="nb">print</span> <span class="n">rows</span><span class="p">,</span><span class="n">cols</span>
<span class="mi">342</span> <span class="mi">548</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">19</span><span class="p">]:</span> <span class="n">nrows</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">getOptimalDFTSize</span><span class="p">(</span><span class="n">rows</span><span class="p">)</span>
<span class="n">In</span> <span class="p">[</span><span class="mi">20</span><span class="p">]:</span> <span class="n">ncols</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">getOptimalDFTSize</span><span class="p">(</span><span class="n">cols</span><span class="p">)</span>
<span class="n">In</span> <span class="p">[</span><span class="mi">21</span><span class="p">]:</span> <span class="nb">print</span> <span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span>
<span class="mi">360</span> <span class="mi">576</span>
</pre></div>
</div>
<p>See, the size (342,548) is modified to (360, 576). Now let&#8217;s pad it with zeros (for OpenCV) and find their DFT calculation performance. You can do it by creating a new big zero array and copy the data to it, or use <strong>cv2.copyMakeBorder()</strong>.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">nimg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nrows</span><span class="p">,</span><span class="n">ncols</span><span class="p">))</span>
<span class="n">nimg</span><span class="p">[:</span><span class="n">rows</span><span class="p">,:</span><span class="n">cols</span><span class="p">]</span> <span class="o">=</span> <span class="n">img</span>
</pre></div>
</div>
<p>OR:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">right</span> <span class="o">=</span> <span class="n">ncols</span> <span class="o">-</span> <span class="n">cols</span>
<span class="n">bottom</span> <span class="o">=</span> <span class="n">nrows</span> <span class="o">-</span> <span class="n">rows</span>
<span class="n">bordertype</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">BORDER_CONSTANT</span> <span class="c1">#just to avoid line breakup in PDF file</span>
<span class="n">nimg</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">copyMakeBorder</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">bottom</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">right</span><span class="p">,</span><span class="n">bordertype</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>Now we calculate the DFT performance comparison of Numpy function:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">In</span> <span class="p">[</span><span class="mi">22</span><span class="p">]:</span> <span class="o">%</span><span class="n">timeit</span> <span class="n">fft1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">fft2</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="mi">10</span> <span class="n">loops</span><span class="p">,</span> <span class="n">best</span> <span class="n">of</span> <span class="mi">3</span><span class="p">:</span> <span class="mf">40.9</span> <span class="n">ms</span> <span class="n">per</span> <span class="n">loop</span>
<span class="n">In</span> <span class="p">[</span><span class="mi">23</span><span class="p">]:</span> <span class="o">%</span><span class="n">timeit</span> <span class="n">fft2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">fft2</span><span class="p">(</span><span class="n">img</span><span class="p">,[</span><span class="n">nrows</span><span class="p">,</span><span class="n">ncols</span><span class="p">])</span>
<span class="mi">100</span> <span class="n">loops</span><span class="p">,</span> <span class="n">best</span> <span class="n">of</span> <span class="mi">3</span><span class="p">:</span> <span class="mf">10.4</span> <span class="n">ms</span> <span class="n">per</span> <span class="n">loop</span>
</pre></div>
</div>
<p>It shows a 4x speedup. Now we will try the same with OpenCV functions.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">In</span> <span class="p">[</span><span class="mi">24</span><span class="p">]:</span> <span class="o">%</span><span class="n">timeit</span> <span class="n">dft1</span><span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">dft</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">img</span><span class="p">),</span><span class="n">flags</span><span class="o">=</span><span class="n">cv2</span><span class="o">.</span><span class="n">DFT_COMPLEX_OUTPUT</span><span class="p">)</span>
<span class="mi">100</span> <span class="n">loops</span><span class="p">,</span> <span class="n">best</span> <span class="n">of</span> <span class="mi">3</span><span class="p">:</span> <span class="mf">13.5</span> <span class="n">ms</span> <span class="n">per</span> <span class="n">loop</span>
<span class="n">In</span> <span class="p">[</span><span class="mi">27</span><span class="p">]:</span> <span class="o">%</span><span class="n">timeit</span> <span class="n">dft2</span><span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">dft</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">nimg</span><span class="p">),</span><span class="n">flags</span><span class="o">=</span><span class="n">cv2</span><span class="o">.</span><span class="n">DFT_COMPLEX_OUTPUT</span><span class="p">)</span>
<span class="mi">100</span> <span class="n">loops</span><span class="p">,</span> <span class="n">best</span> <span class="n">of</span> <span class="mi">3</span><span class="p">:</span> <span class="mf">3.11</span> <span class="n">ms</span> <span class="n">per</span> <span class="n">loop</span>
</pre></div>
</div>
<p>It also shows a 4x speed-up. You can also see that OpenCV functions are around 3x faster than Numpy functions. This can be tested for inverse FFT also, and that is left as an exercise for you.</p>
</div>
<div class="section" id="why-laplacian-is-a-high-pass-filter">
<h6>Why Laplacian is a High Pass Filter?<a class="headerlink" href="#why-laplacian-is-a-high-pass-filter" title="Permalink to this headline">¶</a></h6>
<p>A similar question was asked in a forum. The question is, why Laplacian is a high pass filter? Why Sobel is a HPF? etc. And the first answer given to it was in terms of Fourier Transform. Just take the fourier transform of Laplacian for some higher size of FFT. Analyze it:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># simple averaging filter without scaling parameter</span>
<span class="n">mean_filter</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>

<span class="c1"># creating a guassian filter</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">getGaussianKernel</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">gaussian</span> <span class="o">=</span> <span class="n">x</span><span class="o">*</span><span class="n">x</span><span class="o">.</span><span class="n">T</span>

<span class="c1"># different edge detecting filters</span>
<span class="c1"># scharr in x-direction</span>
<span class="n">scharr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                   <span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">],</span>
                   <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>
<span class="c1"># sobel in x direction</span>
<span class="n">sobel_x</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                   <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                   <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="c1"># sobel in y direction</span>
<span class="n">sobel_y</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                   <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                   <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="c1"># laplacian</span>
<span class="n">laplacian</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                    <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>

<span class="n">filters</span> <span class="o">=</span> <span class="p">[</span><span class="n">mean_filter</span><span class="p">,</span> <span class="n">gaussian</span><span class="p">,</span> <span class="n">laplacian</span><span class="p">,</span> <span class="n">sobel_x</span><span class="p">,</span> <span class="n">sobel_y</span><span class="p">,</span> <span class="n">scharr</span><span class="p">]</span>
<span class="n">filter_name</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;mean_filter&#39;</span><span class="p">,</span> <span class="s1">&#39;gaussian&#39;</span><span class="p">,</span><span class="s1">&#39;laplacian&#39;</span><span class="p">,</span> <span class="s1">&#39;sobel_x&#39;</span><span class="p">,</span> \
                <span class="s1">&#39;sobel_y&#39;</span><span class="p">,</span> <span class="s1">&#39;scharr_x&#39;</span><span class="p">]</span>
<span class="n">fft_filters</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">fft2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">filters</span><span class="p">]</span>
<span class="n">fft_shift</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">fftshift</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">fft_filters</span><span class="p">]</span>
<span class="n">mag_spectrum</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">z</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">z</span> <span class="ow">in</span> <span class="n">fft_shift</span><span class="p">]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">mag_spectrum</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">cmap</span> <span class="o">=</span> <span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">filter_name</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([]),</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>See the result:</p>
<blockquote>
<div><img alt="Frequency Spectrum of different Kernels" class="align-center" src="_images/fft5.jpg" />
</div></blockquote>
<p>From image, you can see what frequency region each kernel blocks, and what region it passes. From that information, we can say why each kernel is a HPF or a LPF</p>
</div>
<div class="section" id="additional-resources">
<h6>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h6>
<ol class="arabic simple">
<li><a class="reference external" href="http://cns-alumni.bu.edu/~slehar/fourier/fourier.html">An Intuitive Explanation of Fourier Theory</a> by Steven Lehar</li>
<li><a class="reference external" href="http://homepages.inf.ed.ac.uk/rbf/HIPR2/fourier.htm">Fourier Transform</a> at HIPR</li>
<li><a class="reference external" href="http://dsp.stackexchange.com/q/1637/818">What does frequency domain denote in case of images?</a></li>
</ol>
</div>
<div class="section" id="exercises">
<h6>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h6>
</div>
</div>
</div>
</div>
<span id="document-py_tutorials/py_imgproc/py_template_matching/py_template_matching"></span><div class="section" id="template-matching">
<span id="id1"></span><h4>Template Matching<a class="headerlink" href="#template-matching" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goals">
<h5>Goals<a class="headerlink" href="#goals" title="Permalink to this headline">¶</a></h5>
<dl class="docutils">
<dt>In this chapter, you will learn</dt>
<dd><ul class="first last simple">
<li>To find objects in an image using Template Matching</li>
<li>You will see these functions : <strong>cv2.matchTemplate()</strong>, <strong>cv2.minMaxLoc()</strong></li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="theory">
<h5>Theory<a class="headerlink" href="#theory" title="Permalink to this headline">¶</a></h5>
<p>Template Matching is a method for searching and finding the location of a template image in a larger image. OpenCV comes with a function <strong>cv2.matchTemplate()</strong> for this purpose. It simply slides the template image over the input image (as in 2D convolution) and compares the template and patch of input image under the template image. Several comparison methods are implemented in OpenCV. (You can check docs for more details). It returns a grayscale image, where each pixel denotes how much does the neighbourhood of that pixel match with template.</p>
<p>If input image is of size <cite>(WxH)</cite> and template image is of size <cite>(wxh)</cite>, output image will have a size of <cite>(W-w+1, H-h+1)</cite>. Once you got the result, you can use <strong>cv2.minMaxLoc()</strong> function to find where is the maximum/minimum value. Take it as the top-left corner of rectangle and take <cite>(w,h)</cite> as width and height of the rectangle. That rectangle is your region of template.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">If you are using <code class="docutils literal"><span class="pre">cv2.TM_SQDIFF</span></code> as comparison method, minimum value gives the best match.</p>
</div>
</div>
<div class="section" id="template-matching-in-opencv">
<h5>Template Matching in OpenCV<a class="headerlink" href="#template-matching-in-opencv" title="Permalink to this headline">¶</a></h5>
<p>Here, as an example, we will search for Messi&#8217;s face in his photo. So I created a template as below:</p>
<blockquote>
<div><img alt="Template Image" class="align-center" src="_images/messi_face.jpg" />
</div></blockquote>
<p>We will try all the comparison methods so that we can see how their results look like:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;messi5.jpg&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">img2</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">template</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;template.jpg&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">w</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">template</span><span class="o">.</span><span class="n">shape</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># All the 6 methods for comparison in a list</span>
<span class="n">methods</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;cv2.TM_CCOEFF&#39;</span><span class="p">,</span> <span class="s1">&#39;cv2.TM_CCOEFF_NORMED&#39;</span><span class="p">,</span> <span class="s1">&#39;cv2.TM_CCORR&#39;</span><span class="p">,</span>
            <span class="s1">&#39;cv2.TM_CCORR_NORMED&#39;</span><span class="p">,</span> <span class="s1">&#39;cv2.TM_SQDIFF&#39;</span><span class="p">,</span> <span class="s1">&#39;cv2.TM_SQDIFF_NORMED&#39;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">meth</span> <span class="ow">in</span> <span class="n">methods</span><span class="p">:</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">img2</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">method</span> <span class="o">=</span> <span class="nb">eval</span><span class="p">(</span><span class="n">meth</span><span class="p">)</span>

    <span class="c1"># Apply template Matching</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">matchTemplate</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">template</span><span class="p">,</span><span class="n">method</span><span class="p">)</span>
    <span class="n">min_val</span><span class="p">,</span> <span class="n">max_val</span><span class="p">,</span> <span class="n">min_loc</span><span class="p">,</span> <span class="n">max_loc</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">minMaxLoc</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>

    <span class="c1"># If the method is TM_SQDIFF or TM_SQDIFF_NORMED, take minimum</span>
    <span class="k">if</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="n">cv2</span><span class="o">.</span><span class="n">TM_SQDIFF</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">TM_SQDIFF_NORMED</span><span class="p">]:</span>
        <span class="n">top_left</span> <span class="o">=</span> <span class="n">min_loc</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">top_left</span> <span class="o">=</span> <span class="n">max_loc</span>
    <span class="n">bottom_right</span> <span class="o">=</span> <span class="p">(</span><span class="n">top_left</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">w</span><span class="p">,</span> <span class="n">top_left</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">h</span><span class="p">)</span>

    <span class="n">cv2</span><span class="o">.</span><span class="n">rectangle</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">top_left</span><span class="p">,</span> <span class="n">bottom_right</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">res</span><span class="p">,</span><span class="n">cmap</span> <span class="o">=</span> <span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Matching Result&#39;</span><span class="p">),</span> <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([]),</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">cmap</span> <span class="o">=</span> <span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Detected Point&#39;</span><span class="p">),</span> <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([]),</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="n">meth</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>See the results below:</p>
<blockquote>
<div><ul class="simple">
<li>cv2.TM_CCOEFF</li>
</ul>
<img alt="Template Image" class="align-center" src="_images/template_ccoeff_1.jpg" />
<ul class="simple">
<li>cv2.TM_CCOEFF_NORMED</li>
</ul>
<img alt="Template Image" class="align-center" src="_images/template_ccoeffn_2.jpg" />
<ul class="simple">
<li>cv2.TM_CCORR</li>
</ul>
<img alt="Template Image" class="align-center" src="_images/template_ccorr_3.jpg" />
<ul class="simple">
<li>cv2.TM_CCORR_NORMED</li>
</ul>
<img alt="Template Image" class="align-center" src="_images/template_ccorrn_4.jpg" />
<ul class="simple">
<li>cv2.TM_SQDIFF</li>
</ul>
<img alt="Template Image" class="align-center" src="_images/template_sqdiff_5.jpg" />
<ul class="simple">
<li>cv2.TM_SQDIFF_NORMED</li>
</ul>
<img alt="Template Image" class="align-center" src="_images/template_sqdiffn_6.jpg" />
</div></blockquote>
<p>You can see that the result using <strong>cv2.TM_CCORR</strong> is not good as we expected.</p>
</div>
<div class="section" id="template-matching-with-multiple-objects">
<h5>Template Matching with Multiple Objects<a class="headerlink" href="#template-matching-with-multiple-objects" title="Permalink to this headline">¶</a></h5>
<p>In the previous section, we searched image for Messi&#8217;s face, which occurs only once in the image. Suppose you are searching for an object which has multiple occurances, <strong>cv2.minMaxLoc()</strong> won&#8217;t give you all the locations. In that case, we will use thresholding. So in this example, we will use a screenshot of the famous game <strong>Mario</strong> and we will find the coins in it.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">img_rgb</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;mario.png&#39;</span><span class="p">)</span>
<span class="n">img_gray</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">img_rgb</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2GRAY</span><span class="p">)</span>
<span class="n">template</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;mario_coin.png&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">w</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">template</span><span class="o">.</span><span class="n">shape</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="n">res</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">matchTemplate</span><span class="p">(</span><span class="n">img_gray</span><span class="p">,</span><span class="n">template</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">TM_CCOEFF_NORMED</span><span class="p">)</span>
<span class="n">threshold</span> <span class="o">=</span> <span class="mf">0.8</span>
<span class="n">loc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span> <span class="n">res</span> <span class="o">&gt;=</span> <span class="n">threshold</span><span class="p">)</span>
<span class="k">for</span> <span class="n">pt</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">loc</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>
    <span class="n">cv2</span><span class="o">.</span><span class="n">rectangle</span><span class="p">(</span><span class="n">img_rgb</span><span class="p">,</span> <span class="n">pt</span><span class="p">,</span> <span class="p">(</span><span class="n">pt</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">w</span><span class="p">,</span> <span class="n">pt</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">h</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">cv2</span><span class="o">.</span><span class="n">imwrite</span><span class="p">(</span><span class="s1">&#39;res.png&#39;</span><span class="p">,</span><span class="n">img_rgb</span><span class="p">)</span>
</pre></div>
</div>
<p>Result:</p>
<blockquote>
<div><img alt="Template Matching" class="align-center" src="_images/res_mario.jpg" />
</div></blockquote>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
</div>
</div>
<span id="document-py_tutorials/py_imgproc/py_houghlines/py_houghlines"></span><div class="section" id="hough-line-transform">
<span id="hough-lines"></span><h4>Hough Line Transform<a class="headerlink" href="#hough-line-transform" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goal">
<h5>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h5>
<dl class="docutils">
<dt>In this chapter,</dt>
<dd><ul class="first last simple">
<li>We will understand the concept of Hough Tranform.</li>
<li>We will see how to use it detect lines in an image.</li>
<li>We will see following functions: <strong>cv2.HoughLines()</strong>, <strong>cv2.HoughLinesP()</strong></li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="theory">
<h5>Theory<a class="headerlink" href="#theory" title="Permalink to this headline">¶</a></h5>
<p>Hough Transform is a popular technique to detect any shape, if you can represent that shape in mathematical form. It can detect the shape even if it is broken or distorted a little bit. We will see how it works for a line.</p>
<p>A line can be represented as <img class="math" src="_images/math/bc6599d4d6cf2a75b684eac43bf95359d98d177b.png" alt="y = mx+c"/> or in parametric form, as <img class="math" src="_images/math/8b16fa3dc8f81011e6879225ae665c6eec78b8e4.png" alt="\rho = x \cos \theta + y \sin \theta"/> where <img class="math" src="_images/math/f574498915fa9e02eeb5141c24835d077eba3e75.png" alt="\rho"/> is the perpendicular distance from origin to the line, and <img class="math" src="_images/math/a9cfbeb8ebee1f365919e147a79e242dcb67ee5d.png" alt="\theta"/> is the angle formed by this perpendicular line and horizontal axis measured in counter-clockwise ( That direction varies on how you represent the coordinate system. This representation is used in OpenCV). Check below image:</p>
<blockquote>
<div><a class="reference internal image-reference" href="_images/houghlines1.svg"><div align="center" class="align-center"><img alt="coordinate system" height="200pt" src="_images/houghlines1.svg" width="200pt" /></div>
</a>
</div></blockquote>
<p>So if line is passing below the origin, it will have a positive rho and angle less than 180. If it is going above the origin, instead of taking angle greater than 180, angle is taken less than 180, and rho is taken negative. Any vertical line will have 0 degree and horizontal lines will have 90 degree.</p>
<p>Now let&#8217;s see how Hough Transform works for lines. Any line can be represented in these two terms, <img class="math" src="_images/math/e3f69f1bb3159048a10a158203e519cc0609d32d.png" alt="(\rho, \theta)"/>. So first it creates a 2D array or accumulator (to hold values of two parameters) and it is set to 0 initially. Let rows denote the <img class="math" src="_images/math/f574498915fa9e02eeb5141c24835d077eba3e75.png" alt="\rho"/> and columns denote the <img class="math" src="_images/math/a9cfbeb8ebee1f365919e147a79e242dcb67ee5d.png" alt="\theta"/>. Size of array depends on the accuracy you need. Suppose you want the accuracy of angles to be 1 degree, you need 180 columns. For <img class="math" src="_images/math/f574498915fa9e02eeb5141c24835d077eba3e75.png" alt="\rho"/>, the maximum distance possible is the diagonal length of the image. So taking one pixel accuracy, number of rows can be diagonal length of the image.</p>
<p>Consider a 100x100 image with a horizontal line at the middle. Take the first point of the line. You know its (x,y) values. Now in the line equation, put the values <img class="math" src="_images/math/903f3ddc103daa3fdf6b5631d5fc46dd2ecb698c.png" alt="\theta = 0,1,2,....,180"/> and check the <img class="math" src="_images/math/f574498915fa9e02eeb5141c24835d077eba3e75.png" alt="\rho"/> you get. For every <img class="math" src="_images/math/e3f69f1bb3159048a10a158203e519cc0609d32d.png" alt="(\rho, \theta)"/> pair, you increment value by one in our accumulator in its corresponding <img class="math" src="_images/math/e3f69f1bb3159048a10a158203e519cc0609d32d.png" alt="(\rho, \theta)"/> cells. So now in accumulator, the cell (50,90) = 1 along with some other cells.</p>
<p>Now take the second point on the line. Do the same as above. Increment the the values in the cells corresponding to <img class="math" src="_images/math/e3f69f1bb3159048a10a158203e519cc0609d32d.png" alt="(\rho, \theta)"/> you got. This time, the cell (50,90) = 2. What you actually do is voting the <img class="math" src="_images/math/e3f69f1bb3159048a10a158203e519cc0609d32d.png" alt="(\rho, \theta)"/> values. You continue this process for every point on the line. At each point, the cell (50,90) will be incremented or voted up, while other cells may or may not be voted up. This way, at the end, the cell (50,90) will have maximum votes. So if you search the accumulator for maximum votes, you get the value (50,90) which says, there is a line in this image at distance 50 from origin and at angle 90 degrees. It is well shown in below animation (Image Courtesy: <a class="reference external" href="http://homepages.inf.ed.ac.uk/amos/hough.html">Amos Storkey</a> )</p>
<blockquote>
<div><img alt="Hough Transform Demo" class="align-center" src="_images/houghlinesdemo.gif" />
</div></blockquote>
<p>This is how hough transform for lines works. It is simple, and may be you can implement it using Numpy on your own. Below is an image which shows the accumulator. Bright spots at some locations denotes they are the parameters of possible lines in the image. (Image courtesy: <a class="reference external" href="http://en.wikipedia.org/wiki/Hough_transform">Wikipedia</a> )</p>
<blockquote>
<div><img alt="Hough Transform accumulator" class="align-center" src="_images/houghlines2.jpg" />
</div></blockquote>
</div>
<div class="section" id="hough-tranform-in-opencv">
<h5>Hough Tranform in OpenCV<a class="headerlink" href="#hough-tranform-in-opencv" title="Permalink to this headline">¶</a></h5>
<p>Everything explained above is encapsulated in the OpenCV function, <strong>cv2.HoughLines()</strong>. It simply returns an array of <img class="math" src="_images/math/e3f69f1bb3159048a10a158203e519cc0609d32d.png" alt="(\rho, \theta)"/> values. <img class="math" src="_images/math/f574498915fa9e02eeb5141c24835d077eba3e75.png" alt="\rho"/> is measured in pixels and <img class="math" src="_images/math/a9cfbeb8ebee1f365919e147a79e242dcb67ee5d.png" alt="\theta"/> is measured in radians. First parameter, Input image should be a binary image, so apply threshold or use canny edge detection before finding applying hough transform. Second and third parameters are <img class="math" src="_images/math/f574498915fa9e02eeb5141c24835d077eba3e75.png" alt="\rho"/> and <img class="math" src="_images/math/a9cfbeb8ebee1f365919e147a79e242dcb67ee5d.png" alt="\theta"/> accuracies respectively. Fourth argument is the <cite>threshold</cite>, which means minimum vote it should get for it to be considered as a line. Remember, number of votes depend upon number of points on the line. So it represents the minimum length of line that should be detected.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;dave.jpg&#39;</span><span class="p">)</span>
<span class="n">gray</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2GRAY</span><span class="p">)</span>
<span class="n">edges</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">Canny</span><span class="p">(</span><span class="n">gray</span><span class="p">,</span><span class="mi">50</span><span class="p">,</span><span class="mi">150</span><span class="p">,</span><span class="n">apertureSize</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span>

<span class="n">lines</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">HoughLines</span><span class="p">(</span><span class="n">edges</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">180</span><span class="p">,</span><span class="mi">200</span><span class="p">)</span>
<span class="k">for</span> <span class="n">rho</span><span class="p">,</span><span class="n">theta</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
    <span class="n">x0</span> <span class="o">=</span> <span class="n">a</span><span class="o">*</span><span class="n">rho</span>
    <span class="n">y0</span> <span class="o">=</span> <span class="n">b</span><span class="o">*</span><span class="n">rho</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">x0</span> <span class="o">+</span> <span class="mi">1000</span><span class="o">*</span><span class="p">(</span><span class="o">-</span><span class="n">b</span><span class="p">))</span>
    <span class="n">y1</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">y0</span> <span class="o">+</span> <span class="mi">1000</span><span class="o">*</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>
    <span class="n">x2</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">x0</span> <span class="o">-</span> <span class="mi">1000</span><span class="o">*</span><span class="p">(</span><span class="o">-</span><span class="n">b</span><span class="p">))</span>
    <span class="n">y2</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">y0</span> <span class="o">-</span> <span class="mi">1000</span><span class="o">*</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>

    <span class="n">cv2</span><span class="o">.</span><span class="n">line</span><span class="p">(</span><span class="n">img</span><span class="p">,(</span><span class="n">x1</span><span class="p">,</span><span class="n">y1</span><span class="p">),(</span><span class="n">x2</span><span class="p">,</span><span class="n">y2</span><span class="p">),(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">),</span><span class="mi">2</span><span class="p">)</span>

<span class="n">cv2</span><span class="o">.</span><span class="n">imwrite</span><span class="p">(</span><span class="s1">&#39;houghlines3.jpg&#39;</span><span class="p">,</span><span class="n">img</span><span class="p">)</span>
</pre></div>
</div>
<p>Check the results below:</p>
<blockquote>
<div><img alt="Hough Transform Line Detection" class="align-center" src="_images/houghlines3.jpg" />
</div></blockquote>
</div>
<div class="section" id="probabilistic-hough-transform">
<h5>Probabilistic Hough Transform<a class="headerlink" href="#probabilistic-hough-transform" title="Permalink to this headline">¶</a></h5>
<p>In the hough transform, you can see that even for a line with two arguments, it takes a lot of computation. Probabilistic Hough Transform is an optimization of Hough Transform we saw. It doesn&#8217;t take all the points into consideration, instead take only a random subset of points and that is sufficient for line detection. Just we have to decrease the threshold. See below image which compare Hough Transform and Probabilistic Hough Transform in hough space. (Image Courtesy : <a class="reference external" href="http://phdfb1.free.fr/robot/mscthesis/node14.html">Franck Bettinger&#8217;s home page</a></p>
<blockquote>
<div><img alt="Hough Transform and Probabilistic Hough Transform" class="align-center" src="_images/houghlines4.png" />
</div></blockquote>
<dl class="docutils">
<dt>OpenCV implementation is based on Robust Detection of Lines Using the Progressive Probabilistic Hough Transform by Matas, J. and Galambos, C. and Kittler, J.V.. The function used is <strong>cv2.HoughLinesP()</strong>. It has two new arguments.</dt>
<dd><ul class="first last simple">
<li><strong>minLineLength</strong> - Minimum length of line. Line segments shorter than this are rejected.</li>
<li><strong>maxLineGap</strong> - Maximum allowed gap between line segments to treat them as single line.</li>
</ul>
</dd>
</dl>
<p>Best thing is that, it directly returns the two endpoints of lines. In previous case, you got only the parameters of lines, and you had to find all the points. Here, everything is direct and simple.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;dave.jpg&#39;</span><span class="p">)</span>
<span class="n">gray</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2GRAY</span><span class="p">)</span>
<span class="n">edges</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">Canny</span><span class="p">(</span><span class="n">gray</span><span class="p">,</span><span class="mi">50</span><span class="p">,</span><span class="mi">150</span><span class="p">,</span><span class="n">apertureSize</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">minLineLength</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">maxLineGap</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">lines</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">HoughLinesP</span><span class="p">(</span><span class="n">edges</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">180</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="n">minLineLength</span><span class="p">,</span><span class="n">maxLineGap</span><span class="p">)</span>
<span class="k">for</span> <span class="n">x1</span><span class="p">,</span><span class="n">y1</span><span class="p">,</span><span class="n">x2</span><span class="p">,</span><span class="n">y2</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
    <span class="n">cv2</span><span class="o">.</span><span class="n">line</span><span class="p">(</span><span class="n">img</span><span class="p">,(</span><span class="n">x1</span><span class="p">,</span><span class="n">y1</span><span class="p">),(</span><span class="n">x2</span><span class="p">,</span><span class="n">y2</span><span class="p">),(</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span><span class="mi">2</span><span class="p">)</span>

<span class="n">cv2</span><span class="o">.</span><span class="n">imwrite</span><span class="p">(</span><span class="s1">&#39;houghlines5.jpg&#39;</span><span class="p">,</span><span class="n">img</span><span class="p">)</span>
</pre></div>
</div>
<p>See the results below:</p>
<blockquote>
<div><img alt="Probabilistic Hough Transform" class="align-center" src="_images/houghlines5.jpg" />
</div></blockquote>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
<ol class="arabic simple">
<li><a class="reference external" href="http://en.wikipedia.org/wiki/Hough_transform">Hough Transform on Wikipedia</a></li>
</ol>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
</div>
</div>
<span id="document-py_tutorials/py_imgproc/py_houghcircles/py_houghcircles"></span><div class="section" id="hough-circle-transform">
<span id="hough-circles"></span><h4>Hough Circle Transform<a class="headerlink" href="#hough-circle-transform" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goal">
<h5>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h5>
<dl class="docutils">
<dt>In this chapter,</dt>
<dd><ul class="first last simple">
<li>We will learn to use Hough Transform to find circles in an image.</li>
<li>We will see these functions: <strong>cv2.HoughCircles()</strong></li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="theory">
<h5>Theory<a class="headerlink" href="#theory" title="Permalink to this headline">¶</a></h5>
<p>A circle is represented mathematically as <img class="math" src="_images/math/a59e83d016322a7b1e888b67eb77a7a3112493c2.png" alt="(x-x_{center})^2 + (y - y_{center})^2 = r^2"/> where <img class="math" src="_images/math/7a9595fd342dc86f3547a19cc4537322e3a4dedc.png" alt="(x_{center},y_{center})"/> is the center of the circle, and <img class="math" src="_images/math/2ede365ad144ab396916ec60458da03860803078.png" alt="r"/> is the radius of the circle. From equation, we can see we have 3 parameters, so we need a 3D accumulator for hough transform, which would be highly ineffective. So OpenCV uses more trickier method, <strong>Hough Gradient Method</strong> which uses the gradient information of edges.</p>
<p>The function we use here is <strong>cv2.HoughCircles()</strong>. It has plenty of arguments which are well explained in the documentation. So we directly go to the code.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;opencv_logo.png&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">medianBlur</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="n">cimg</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_GRAY2BGR</span><span class="p">)</span>

<span class="n">circles</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">HoughCircles</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">HOUGH_GRADIENT</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span>
                            <span class="n">param1</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span><span class="n">param2</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span><span class="n">minRadius</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">maxRadius</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">circles</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">uint16</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">around</span><span class="p">(</span><span class="n">circles</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">circles</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]:</span>
    <span class="c1"># draw the outer circle</span>
    <span class="n">cv2</span><span class="o">.</span><span class="n">circle</span><span class="p">(</span><span class="n">cimg</span><span class="p">,(</span><span class="n">i</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">i</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span><span class="n">i</span><span class="p">[</span><span class="mi">2</span><span class="p">],(</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span><span class="mi">2</span><span class="p">)</span>
    <span class="c1"># draw the center of the circle</span>
    <span class="n">cv2</span><span class="o">.</span><span class="n">circle</span><span class="p">(</span><span class="n">cimg</span><span class="p">,(</span><span class="n">i</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">i</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span><span class="mi">2</span><span class="p">,(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">),</span><span class="mi">3</span><span class="p">)</span>

<span class="n">cv2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s1">&#39;detected circles&#39;</span><span class="p">,</span><span class="n">cimg</span><span class="p">)</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>
</pre></div>
</div>
<p>Result is shown below:</p>
<blockquote>
<div><img alt="Hough Circles" class="align-center" src="_images/houghcircles2.jpg" />
</div></blockquote>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
</div>
</div>
<span id="document-py_tutorials/py_imgproc/py_watershed/py_watershed"></span><div class="section" id="image-segmentation-with-watershed-algorithm">
<span id="watershed"></span><h4>Image Segmentation with Watershed Algorithm<a class="headerlink" href="#image-segmentation-with-watershed-algorithm" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goal">
<h5>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h5>
<dl class="docutils">
<dt>In this chapter,</dt>
<dd><ul class="first last simple">
<li>We will learn to use marker-based image segmentation using watershed algorithm</li>
<li>We will see: <strong>cv2.watershed()</strong></li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="theory">
<h5>Theory<a class="headerlink" href="#theory" title="Permalink to this headline">¶</a></h5>
<p>Any grayscale image can be viewed as a topographic surface where high intensity denotes peaks and hills while low intensity denotes valleys. You start filling every isolated valleys (local minima) with different colored water (labels). As the water rises, depending on the peaks (gradients) nearby, water from different valleys, obviously with different colors will start to merge. To avoid that, you build barriers in the locations where water merges. You continue the work of filling water and building barriers until all the peaks are under water. Then the barriers you created gives you the segmentation result. This is the &#8220;philosophy&#8221; behind the watershed. You can visit the <a class="reference external" href="http://cmm.ensmp.fr/~beucher/wtshed.html">CMM webpage on watershed</a> to understand it with the help of some animations.</p>
<p>But this approach gives you oversegmented result due to noise or any other irregularities in the image. So OpenCV implemented a marker-based watershed algorithm where you specify which are all valley points are to be merged and which are not. It is an interactive image segmentation. What we do is to give different labels for our object we know. Label the region which we are sure of being the foreground or object with one color (or intensity), label the region which we are sure of being background or non-object with another color and finally the region which we are not sure of anything, label it with 0. That is our marker. Then apply watershed algorithm. Then our marker will be updated with the labels we gave, and the boundaries of objects will have a value of -1.</p>
</div>
<div class="section" id="code">
<h5>Code<a class="headerlink" href="#code" title="Permalink to this headline">¶</a></h5>
<p>Below we will see an example on how to use the Distance Transform along with watershed to segment mutually touching objects.</p>
<p>Consider the coins image below, the coins are touching each other. Even if you threshold it, it will be touching each other.</p>
<blockquote>
<div><img alt="Coins" class="align-center" src="_images/water_coins.jpg" />
</div></blockquote>
<p>We start with finding an approximate estimate of the coins. For that, we can use the Otsu&#8217;s binarization.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;coins.png&#39;</span><span class="p">)</span>
<span class="n">gray</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2GRAY</span><span class="p">)</span>
<span class="n">ret</span><span class="p">,</span> <span class="n">thresh</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">threshold</span><span class="p">(</span><span class="n">gray</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">THRESH_BINARY_INV</span><span class="o">+</span><span class="n">cv2</span><span class="o">.</span><span class="n">THRESH_OTSU</span><span class="p">)</span>
</pre></div>
</div>
<p>Result:</p>
<blockquote>
<div><img alt="Thresholding" class="align-center" src="_images/water_thresh.jpg" />
</div></blockquote>
<p>Now we need to remove any small white noises in the image. For that we can use morphological opening. To remove any small holes in the object, we can use morphological closing. So, now we know for sure that region near to center of objects are foreground and region much away from the object are background. Only region we are not sure is the boundary region of coins.</p>
<p>So we need to extract the area which we are sure they are coins. Erosion removes the boundary pixels. So whatever remaining, we can be sure it is coin. That would work if objects were not touching each other. But since they are touching each other, another good option would be to find the distance transform and apply a proper threshold. Next we need to find the area which we are sure they are not coins. For that, we dilate the result. Dilation increases object boundary to background. This way, we can make sure whatever region in background in result is really a background, since boundary region is removed. See the image below.</p>
<blockquote>
<div><img alt="Foreground and Background" class="align-center" src="_images/water_fgbg.jpg" />
</div></blockquote>
<p>The remaining regions are those which we don&#8217;t have any idea, whether it is coins or background. Watershed algorithm should find it. These areas are normally around the boundaries of coins where foreground and background meet (Or even two different coins meet). We call it border. It can be obtained from subtracting sure_fg area from sure_bg area.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="c1"># noise removal</span>
<span class="n">kernel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="n">opening</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">morphologyEx</span><span class="p">(</span><span class="n">thresh</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">MORPH_OPEN</span><span class="p">,</span><span class="n">kernel</span><span class="p">,</span> <span class="n">iterations</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># sure background area</span>
<span class="n">sure_bg</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">dilate</span><span class="p">(</span><span class="n">opening</span><span class="p">,</span><span class="n">kernel</span><span class="p">,</span><span class="n">iterations</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="c1"># Finding sure foreground area</span>
<span class="n">dist_transform</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">distanceTransform</span><span class="p">(</span><span class="n">opening</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">DIST_L2</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="n">ret</span><span class="p">,</span> <span class="n">sure_fg</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">threshold</span><span class="p">(</span><span class="n">dist_transform</span><span class="p">,</span><span class="mf">0.7</span><span class="o">*</span><span class="n">dist_transform</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Finding unknown region</span>
<span class="n">sure_fg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">(</span><span class="n">sure_fg</span><span class="p">)</span>
<span class="n">unknown</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">subtract</span><span class="p">(</span><span class="n">sure_bg</span><span class="p">,</span><span class="n">sure_fg</span><span class="p">)</span>
</pre></div>
</div>
<p>See the result. In the thresholded image, we get some regions of coins which we are sure of coins and they are detached now. (In some cases, you may be interested in only foreground segmentation, not in separating the mutually touching objects. In that case, you need not use distance transform, just erosion is sufficient. Erosion is just another method to extract sure foreground area, that&#8217;s all.)</p>
<blockquote>
<div><img alt="Distance Transform" class="align-center" src="_images/water_dt.jpg" />
</div></blockquote>
<p>Now we know for sure which are region of coins, which are background and all. So we create marker (it is an array of same size as that of original image, but with int32 datatype) and label the regions inside it. The regions we know for sure (whether foreground or background) are labelled with any positive integers, but different integers, and the area we don&#8217;t know for sure are just left as zero. For this we use <strong>cv2.connectedComponents()</strong>. It labels background of the image with 0, then other objects are labelled with integers starting from 1.</p>
<p>But we know that if background is marked with 0, watershed will consider it as unknown area. So we want to mark it with different integer. Instead, we will mark unknown region, defined by <code class="docutils literal"><span class="pre">unknown</span></code>, with 0.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="c1"># Marker labelling</span>
<span class="n">ret</span><span class="p">,</span> <span class="n">markers</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">connectedComponents</span><span class="p">(</span><span class="n">sure_fg</span><span class="p">)</span>

<span class="c1"># Add one to all labels so that sure background is not 0, but 1</span>
<span class="n">markers</span> <span class="o">=</span> <span class="n">markers</span><span class="o">+</span><span class="mi">1</span>

<span class="c1"># Now, mark the region of unknown with zero</span>
<span class="n">markers</span><span class="p">[</span><span class="n">unknown</span><span class="o">==</span><span class="mi">255</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</pre></div>
</div>
<p>See the result shown in JET colormap. The dark blue region shows unknown region. Sure coins are colored with different values. Remaining area which are sure background are shown in lighter blue compared to unknown region.</p>
<blockquote>
<div><img alt="Marker Image" class="align-center" src="_images/water_marker.jpg" />
</div></blockquote>
<p>Now our marker is ready. It is time for final step, apply watershed. Then marker image will be modified. The boundary region will be marked with -1.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">markers</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">watershed</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">markers</span><span class="p">)</span>
<span class="n">img</span><span class="p">[</span><span class="n">markers</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p>See the result below. For some coins, the region where they touch are segmented properly and for some, they are not.</p>
<blockquote>
<div><img alt="Result" class="align-center" src="_images/water_result.jpg" />
</div></blockquote>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
<ol class="arabic simple">
<li>CMM page on <a class="reference external" href="http://cmm.ensmp.fr/~beucher/wtshed.html">Watershed Tranformation</a></li>
</ol>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
<ol class="arabic simple">
<li>OpenCV samples has an interactive sample on watershed segmentation, <cite>watershed.py</cite>. Run it, Enjoy it, then learn it.</li>
</ol>
</div>
</div>
<span id="document-py_tutorials/py_imgproc/py_grabcut/py_grabcut"></span><div class="section" id="interactive-foreground-extraction-using-grabcut-algorithm">
<span id="grabcut"></span><h4>Interactive Foreground Extraction using GrabCut Algorithm<a class="headerlink" href="#interactive-foreground-extraction-using-grabcut-algorithm" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goal">
<h5>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h5>
<dl class="docutils">
<dt>In this chapter</dt>
<dd><ul class="first last simple">
<li>We will see GrabCut algorithm to extract foreground in images</li>
<li>We will create an interactive application for this.</li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="theory">
<h5>Theory<a class="headerlink" href="#theory" title="Permalink to this headline">¶</a></h5>
<p>GrabCut algorithm was designed by Carsten Rother, Vladimir Kolmogorov &amp; Andrew Blake from Microsoft Research Cambridge, UK. in their paper, <a class="reference external" href="http://dl.acm.org/citation.cfm?id=1015720">&#8220;GrabCut&#8221;: interactive foreground extraction using iterated graph cuts</a> . An algorithm was needed for foreground extraction with minimal user interaction, and the result was GrabCut.</p>
<p>How it works from user point of view ? Initially user draws a rectangle around the foreground region (foreground region shoule be completely inside the rectangle). Then algorithm segments it iteratively to get the best result. Done. But in some cases, the segmentation won&#8217;t be fine, like, it may have marked some foreground region as background and vice versa. In that case, user need to do fine touch-ups. Just give some strokes on the images where some faulty results are there. Strokes basically says <em>&#8220;Hey, this region should be foreground, you marked it background, correct it in next iteration&#8221;</em> or its opposite for background. Then in the next iteration, you get better results.</p>
<p>See the image below. First player and football is enclosed in a blue rectangle. Then some final touchups with white strokes (denoting foreground) and black strokes (denoting background) is made. And we get a nice result.</p>
<blockquote>
<div><img alt="GrabCut in Action" class="align-center" src="_images/grabcut_output1.jpg" />
</div></blockquote>
<p>So what happens in background ?</p>
<blockquote>
<div><ul class="simple">
<li>User inputs the rectangle. Everything outside this rectangle will be taken as sure background (That is the reason it is mentioned before that your rectangle should include all the objects). Everything inside rectangle is unknown. Similarly any user input specifying foreground and background are considered as hard-labelling which means they won&#8217;t change in the process.</li>
<li>Computer does an initial labelling depeding on the data we gave. It labels the foreground and background pixels (or it hard-labels)</li>
<li>Now a Gaussian Mixture Model(GMM) is used to model the foreground and background.</li>
<li>Depending on the data we gave, GMM learns and create new pixel distribution. That is, the unknown pixels are labelled either probable foreground or probable background depending on its relation with the other hard-labelled pixels in terms of color statistics (It is just like clustering).</li>
<li>A graph is built from this pixel distribution. Nodes in the graphs are pixels. Additional two nodes are added, <strong>Source node</strong> and <strong>Sink node</strong>. Every foreground pixel is connected to Source node and every background pixel is connected to Sink node.</li>
<li>The weights of edges connecting pixels to source node/end node are defined by the probability of a pixel being foreground/background. The weights between the pixels are defined by the edge information or pixel similarity. If there is a large difference in pixel color, the edge between them will get a low weight.</li>
<li>Then a mincut algorithm is used to segment the graph. It cuts the graph into two separating source node and sink node with minimum cost function. The cost function is the sum of all weights of the edges that are cut. After the cut, all the pixels connected to Source node become foreground and those connected to Sink node become background.</li>
<li>The process is continued until the classification converges.</li>
</ul>
</div></blockquote>
<p>It is illustrated in below image (Image Courtesy: <a class="reference external" href="http://www.cs.ru.ac.za/research/g02m1682/">http://www.cs.ru.ac.za/research/g02m1682/</a>)</p>
<blockquote>
<div><img alt="Simplified Diagram of GrabCut Algorithm" class="align-center" src="_images/grabcut.jpg" />
</div></blockquote>
</div>
<div class="section" id="demo">
<h5>Demo<a class="headerlink" href="#demo" title="Permalink to this headline">¶</a></h5>
<p>Now we go for grabcut algorithm with OpenCV. OpenCV has the function, <strong>cv2.grabCut()</strong> for this. We will see its arguments first:</p>
<blockquote>
<div><ul class="simple">
<li><em>img</em> - Input image</li>
<li><em>mask</em> - It is a mask image where we specify which areas are background, foreground or probable background/foreground etc. It is done by the following flags, <strong>cv2.GC_BGD, cv2.GC_FGD, cv2.GC_PR_BGD, cv2.GC_PR_FGD</strong>, or simply pass 0,1,2,3 to image.</li>
<li><em>rect</em> - It is the coordinates of a rectangle which includes the foreground object in the format (x,y,w,h)</li>
<li><em>bdgModel</em>, <em>fgdModel</em> - These are arrays used by the algorithm internally. You just create two np.float64 type zero arrays of size (1,65).</li>
<li><em>iterCount</em> - Number of iterations the algorithm should run.</li>
<li><em>mode</em> - It should be <strong>cv2.GC_INIT_WITH_RECT</strong> or <strong>cv2.GC_INIT_WITH_MASK</strong> or combined which decides whether we are drawing rectangle or final touchup strokes.</li>
</ul>
</div></blockquote>
<p>First let&#8217;s see with rectangular mode. We load the image, create a similar mask image. We create <em>fgdModel</em> and <em>bgdModel</em>. We give the rectangle parameters. It&#8217;s all straight-forward. Let the algorithm run for 5 iterations. Mode should be <em>cv2.GC_INIT_WITH_RECT</em> since we are using rectangle. Then run the grabcut. It modifies the mask image. In the new mask image, pixels will be marked with four flags denoting background/foreground as specified above. So we modify the mask such that all 0-pixels and 2-pixels are put to 0 (ie background) and all 1-pixels and 3-pixels are put to 1(ie foreground pixels). Now our final mask is ready. Just multiply it with input image to get the segmented image.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;messi5.jpg&#39;</span><span class="p">)</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>

<span class="n">bgdModel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">65</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="n">fgdModel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">65</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

<span class="n">rect</span> <span class="o">=</span> <span class="p">(</span><span class="mi">50</span><span class="p">,</span><span class="mi">50</span><span class="p">,</span><span class="mi">450</span><span class="p">,</span><span class="mi">290</span><span class="p">)</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">grabCut</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">mask</span><span class="p">,</span><span class="n">rect</span><span class="p">,</span><span class="n">bgdModel</span><span class="p">,</span><span class="n">fgdModel</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">GC_INIT_WITH_RECT</span><span class="p">)</span>

<span class="n">mask2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">((</span><span class="n">mask</span><span class="o">==</span><span class="mi">2</span><span class="p">)</span><span class="o">|</span><span class="p">(</span><span class="n">mask</span><span class="o">==</span><span class="mi">0</span><span class="p">),</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;uint8&#39;</span><span class="p">)</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="o">*</span><span class="n">mask2</span><span class="p">[:,:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(),</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>See the results below:</p>
<blockquote>
<div><img alt="Segmentation in rect mode" class="align-center" src="_images/grabcut_rect.jpg" />
</div></blockquote>
<p>Oops, Messi&#8217;s hair is gone. <em>Who likes Messi without his hair?</em> We need to bring it back. So we will give there a fine touchup with 1-pixel (sure foreground). At the same time, Some part of ground has come to picture which we don&#8217;t want, and also some logo. We need to remove them. There we give some 0-pixel touchup (sure background). So we modify our resulting mask in previous case as we told now.</p>
<p><em>What I actually did is that, I opened input image in paint application and added another layer to the image. Using brush tool in the paint, I marked missed foreground (hair, shoes, ball etc) with white and unwanted background (like logo, ground etc) with black on this new layer. Then filled remaining background with gray. Then loaded that mask image in OpenCV, edited original mask image we got with corresponding values in newly added mask image. Check the code below:</em></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="c1"># newmask is the mask image I manually labelled</span>
<span class="n">newmask</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;newmask.png&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># whereever it is marked white (sure foreground), change mask=1</span>
<span class="c1"># whereever it is marked black (sure background), change mask=0</span>
<span class="n">mask</span><span class="p">[</span><span class="n">newmask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">mask</span><span class="p">[</span><span class="n">newmask</span> <span class="o">==</span> <span class="mi">255</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">mask</span><span class="p">,</span> <span class="n">bgdModel</span><span class="p">,</span> <span class="n">fgdModel</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">grabCut</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">mask</span><span class="p">,</span><span class="kc">None</span><span class="p">,</span><span class="n">bgdModel</span><span class="p">,</span><span class="n">fgdModel</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">GC_INIT_WITH_MASK</span><span class="p">)</span>

<span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">((</span><span class="n">mask</span><span class="o">==</span><span class="mi">2</span><span class="p">)</span><span class="o">|</span><span class="p">(</span><span class="n">mask</span><span class="o">==</span><span class="mi">0</span><span class="p">),</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;uint8&#39;</span><span class="p">)</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="o">*</span><span class="n">mask</span><span class="p">[:,:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(),</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>See the result below:</p>
<blockquote>
<div><img alt="Segmentation in mask mode" class="align-center" src="_images/grabcut_mask.jpg" />
</div></blockquote>
<p>So that&#8217;s it. Here instead of initializing in rect mode, you can directly go into mask mode. Just mark the rectangle area in mask image with 2-pixel or 3-pixel (probable background/foreground). Then mark our sure_foreground with 1-pixel as we did in second example. Then directly apply the grabCut function with mask mode.</p>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
<ol class="arabic simple">
<li>OpenCV samples contain a sample <code class="docutils literal"><span class="pre">grabcut.py</span></code> which is an interactive tool using grabcut. Check it. Also watch this <a class="reference external" href="http://www.youtube.com/watch?v=kAwxLTDDAwU">youtube video</a> on how to use it.</li>
<li>Here, you can make this into a interactive sample with drawing rectangle and strokes with mouse, create trackbar to adjust stroke width etc.</li>
</ol>
</div>
</div>
</div>
</div>
<span id="document-py_tutorials/py_feature2d/py_table_of_contents_feature2d/py_table_of_contents_feature2d"></span><div class="section" id="feature-detection-and-description">
<span id="py-table-of-content-feature2d"></span><h3>Feature Detection and Description<a class="headerlink" href="#feature-detection-and-description" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference internal" href="index.html#features-meaning"><span class="std std-ref">Understanding Features</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="11%" />
<col width="89%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/features_icon.jpg"><img alt="f2d_1" src="_images/features_icon.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>What are the main features in an image? How can finding those features be useful to us?</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#harris-corners"><span class="std std-ref">Harris Corner Detection</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="16%" />
<col width="84%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/harris_icon.jpg"><img alt="f2d_2" src="_images/harris_icon.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Okay, Corners are good features? But how do we find them?</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#shi-tomasi"><span class="std std-ref">Shi-Tomasi Corner Detector &amp; Good Features to Track</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="17%" />
<col width="83%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/shi_icon.jpg"><img alt="f2d_3" src="_images/shi_icon.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>We will look into Shi-Tomasi corner detection</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#sift-intro"><span class="std std-ref">Introduction to SIFT (Scale-Invariant Feature Transform)</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="6%" />
<col width="94%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/sift_icon.jpg"><img alt="f2d_4" src="_images/sift_icon.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Harris corner detector is not good enough when scale of image changes. Lowe developed a breakthrough method to find scale-invariant features and it is called SIFT</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#surf"><span class="std std-ref">Introduction to SURF (Speeded-Up Robust Features)</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/surf_icon.jpg"><img alt="f2d_5" src="_images/surf_icon.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>SIFT is really good, but not fast enough, so people came up with a speeded-up version called SURF.</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#fast"><span class="std std-ref">FAST Algorithm for Corner Detection</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="5%" />
<col width="95%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/fast_icon.jpg"><img alt="f2d_06" src="_images/fast_icon.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>All the above feature detection methods are good in some way. But they are not fast enough to work in real-time applications like SLAM. There comes the FAST algorithm, which is really &#8220;FAST&#8221;.</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#brief"><span class="std std-ref">BRIEF (Binary Robust Independent Elementary Features)</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="3%" />
<col width="97%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/brief.jpg"><img alt="f2d_07" src="_images/brief.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>SIFT uses a feature descriptor with 128 floating point numbers. Consider thousands of such features. It takes lots of memory and more time for matching. We can compress it to make it faster. But still we have to calculate it first. There comes BRIEF which gives the shortcut to find binary descriptors with less memory, faster matching, still higher recognition rate.</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#orb"><span class="std std-ref">ORB (Oriented FAST and Rotated BRIEF)</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="4%" />
<col width="96%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/orb.jpg"><img alt="f2d_08" src="_images/orb.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>SIFT and SURF are good in what they do, but what if you have to pay a few dollars every year to use them in your applications? Yeah, they are patented!!! To solve that problem, OpenCV devs came up with a new &#8220;FREE&#8221; alternative to SIFT &amp; SURF, and that is ORB.</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#matcher"><span class="std std-ref">Feature Matching</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="5%" />
<col width="95%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/matching.jpg"><img alt="f2d_09" src="_images/matching.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>We know a great deal about feature detectors and descriptors. It is time to learn how to match different descriptors. OpenCV provides two techniques, Brute-Force matcher and FLANN based matcher.</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#feature-homography"><span class="std std-ref">Feature Matching + Homography to find Objects</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="9%" />
<col width="91%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/homography_icon.jpg"><img alt="f2d_10" src="_images/homography_icon.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Now we know about feature matching. Let&#8217;s mix it up with <cite>calib3d</cite> module to find objects in a complex image.</td>
</tr>
</tbody>
</table>
</div></blockquote>
<div class="toctree-wrapper compound">
<span id="document-py_tutorials/py_feature2d/py_features_meaning/py_features_meaning"></span><div class="section" id="understanding-features">
<span id="features-meaning"></span><h4>Understanding Features<a class="headerlink" href="#understanding-features" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goal">
<h5>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h5>
<p>In this chapter, we will just try to understand what are features, why are they important, why corners are important etc.</p>
</div>
<div class="section" id="explanation">
<h5>Explanation<a class="headerlink" href="#explanation" title="Permalink to this headline">¶</a></h5>
<p>Most of you will have played the jigsaw puzzle games. You get a lot of small pieces of a images, where you need to assemble them correctly to form a big real image. <strong>The question is, how you do it?</strong> What about the projecting the same theory to a computer program so that computer can play jigsaw puzzles? If the computer can play jigsaw puzzles, why can&#8217;t we give a lot of real-life images of a good natural scenery to computer and tell it to stitch all those images to a big single image? If the computer can stitch several natural images to one, what about giving a lot of pictures of a building or any structure and tell computer to create a 3D model out of it?</p>
<p>Well, the questions and imaginations continue. But it all depends on the most basic question? How do you play jigsaw puzzles? How do you arrange lots of scrambled image pieces into a big single image? How can you stitch a lot of natural images to a single image?</p>
<p>The answer is, we are looking for specific patterns or specific features which are unique, which can be easily tracked, which can be easily compared. If we go for a definition of such a feature, we may find it difficult to express it in words, but we know what are they. If some one asks you to point out one good feature which can be compared across several images, you can point out one. That is why, even small children can simply play these games. We search for these features in an image, we find them, we find the same features in other images, we align them. That&#8217;s it. (In jigsaw puzzle, we look more into continuity of different images). All these abilities are present in us inherently.</p>
<p>So our one basic question expands to more in number, but becomes more specific. <strong>What are these features?</strong>. <em>(The answer should be understandable to a computer also.)</em></p>
<p>Well, it is difficult to say how humans find these features. It is already programmed in our brain. But if we look deep into some pictures and search for different patterns, we will find something interesting. For example, take below image:</p>
<blockquote>
<div><img alt="Understanding features" class="align-center" src="_images/feature_building.jpg" />
</div></blockquote>
<p>Image is very simple. At the top of image, six small image patches are given. Question for you is to find the exact location of these patches in the original image. How many correct results you can find ?</p>
<p>A and B are flat surfaces, and they are spread in a lot of area. It is difficult to find the exact location of these patches.</p>
<p>C and D are much more simpler. They are edges of the building. You can find an approximate location, but exact location is still difficult. It is because, along the edge, it is same everywhere. Normal to the edge, it is different. So edge is much more better feature compared to flat area, but not good enough (It is good in jigsaw puzzle for comparing continuity of edges).</p>
<p>Finally, E and F are some corners of the building. And they can be easily found out. Because at corners, wherever you move this patch, it will look different. So they can be considered as a good feature. So now we move into more simpler (and widely used image) for better understanding.</p>
<blockquote>
<div><img alt="Features" class="align-center" src="_images/feature_simple.png" />
</div></blockquote>
<p>Just like above, blue patch is flat area and difficult to find and track. Wherever you move the blue patch, it looks the same. For black patch, it is an edge. If you move it in vertical direction (i.e. along the gradient) it changes. Put along the edge (parallel to edge), it looks the same. And for red patch, it is a corner. Wherever you move the patch, it looks different, means it is unique. So basically, corners are considered to be good features in an image. (Not just corners, in some cases blobs are considered good features).</p>
<p>So now we answered our question, &#8220;what are these features?&#8221;. But next question arises. How do we find them? Or how do we find the corners?. That also we answered in an intuitive way, i.e., look for the regions in images which have maximum variation when moved (by a small amount) in all regions around it. This would be projected into computer language in coming chapters. So finding these image features is called <strong>Feature Detection</strong>.</p>
<p>So we found the features in image (Assume you did it). Once you found it, you should find the same in the other images. What we do? We take a region around the feature, we explain it in our own words, like &#8220;upper part is blue sky, lower part is building region, on that building there are some glasses etc&#8221; and you search for the same area in other images. Basically, you are describing the feature. Similar way, computer also should describe the region around the feature so that it can find it in other images. So called description is called <strong>Feature Description</strong>. Once you have the features and its description, you can find same features in all images and align them, stitch them or do whatever you want.</p>
<p>So in this module, we are looking to different algorithms in OpenCV to find features, describe them, match them etc.</p>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
</div>
</div>
<span id="document-py_tutorials/py_feature2d/py_features_harris/py_features_harris"></span><div class="section" id="harris-corner-detection">
<span id="harris-corners"></span><h4>Harris Corner Detection<a class="headerlink" href="#harris-corner-detection" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goal">
<h5>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h5>
<p>In this chapter,</p>
<blockquote>
<div><ul class="simple">
<li>We will understand the concepts behind Harris Corner Detection.</li>
<li>We will see the functions: <strong>cv2.cornerHarris()</strong>, <strong>cv2.cornerSubPix()</strong></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="theory">
<h5>Theory<a class="headerlink" href="#theory" title="Permalink to this headline">¶</a></h5>
<p>In last chapter, we saw that corners are regions in the image with large variation in intensity in all the directions. One early attempt to find these corners was done by <strong>Chris Harris &amp; Mike Stephens</strong> in their paper <strong>A Combined Corner and Edge Detector</strong> in 1988, so now it is called Harris Corner Detector. He took this simple idea to a mathematical form. It basically finds the difference in intensity for a displacement of <img class="math" src="_images/math/2d6bf630586c698ccb735d756e4a4f0674d5ff68.png" alt="(u,v)"/> in all directions. This is expressed as below:</p>
<div class="math">
<p><img src="_images/math/1948f2850c912ce3394b61ba99b546f1bf7a6adc.png" alt="E(u,v) = \sum_{x,y} \underbrace{w(x,y)}_\text{window function} \, [\underbrace{I(x+u,y+v)}_\text{shifted intensity}-\underbrace{I(x,y)}_\text{intensity}]^2"/></p>
</div><p>Window function is either a rectangular window or gaussian window which gives weights to pixels underneath.</p>
<p>We have to maximize this function <img class="math" src="_images/math/8472aae63e1c53ef00aa0a02f1f02a7444b9ef0d.png" alt="E(u,v)"/> for corner detection. That means, we have to maximize the second term. Applying Taylor Expansion to above equation and using some mathematical steps (please refer any standard text books you like for full derivation), we get the final equation as:</p>
<div class="math">
<p><img src="_images/math/3aea48f44ebcc36dda8055c842e209d8886d7a80.png" alt="E(u,v) \approx \begin{bmatrix} u &amp; v \end{bmatrix} M \begin{bmatrix} u \\ v \end{bmatrix}"/></p>
</div><p>where</p>
<div class="math">
<p><img src="_images/math/a33b6840bcc7c240425581fe5e39e58d868f9893.png" alt="M = \sum_{x,y} w(x,y) \begin{bmatrix}I_x I_x &amp; I_x I_y \\
                                     I_x I_y &amp; I_y I_y \end{bmatrix}"/></p>
</div><p>Here, <img class="math" src="_images/math/4d7b6fb7cc597603f842805f2b2920a9e15c837a.png" alt="I_x"/> and <img class="math" src="_images/math/1d4141aba2bbc4d02a35fd500e1b3c70dd7d0000.png" alt="I_y"/> are image derivatives in x and y directions respectively. (Can be easily found out using <strong>cv2.Sobel()</strong>).</p>
<p>Then comes the main part. After this, they created a score, basically an equation, which will determine if a window can contain a corner or not.</p>
<div class="math">
<p><img src="_images/math/8499c5fd1fb4572f17432043824392ba87e027e6.png" alt="R = det(M) - k(trace(M))^2"/></p>
</div><dl class="docutils">
<dt>where</dt>
<dd><ul class="first last simple">
<li><img class="math" src="_images/math/cf85281c57f753fa4f883455cc85bed7cdbfaab3.png" alt="det(M) = \lambda_1 \lambda_2"/></li>
<li><img class="math" src="_images/math/1eb05457094cbdec3f8d3e505b87a184e3ed8121.png" alt="trace(M) = \lambda_1 + \lambda_2"/></li>
<li><img class="math" src="_images/math/18cbd415b1a8e3f19977c5d04d046d41c585c7de.png" alt="\lambda_1"/> and <img class="math" src="_images/math/2bc2d2b207f861ff1c70724ebb8a9cd6831c0d52.png" alt="\lambda_2"/> are the eigen values of M</li>
</ul>
</dd>
</dl>
<p>So the values of these eigen values decide whether a region is corner, edge or flat.</p>
<blockquote>
<div><ul class="simple">
<li>When <img class="math" src="_images/math/79730c21f1c6e4ff7b8fd1b682f0d337a96bc4a9.png" alt="|R|"/> is small, which happens when <img class="math" src="_images/math/18cbd415b1a8e3f19977c5d04d046d41c585c7de.png" alt="\lambda_1"/> and <img class="math" src="_images/math/2bc2d2b207f861ff1c70724ebb8a9cd6831c0d52.png" alt="\lambda_2"/> are small, the region is flat.</li>
<li>When <img class="math" src="_images/math/71931a13baba6be6844d6561f2a1335426de3e16.png" alt="R&lt;0"/>, which happens when <img class="math" src="_images/math/288b695b2f1b07fac29a1f75bfa6f3c80e1a5885.png" alt="\lambda_1 &gt;&gt; \lambda_2"/> or vice versa, the region is edge.</li>
<li>When <img class="math" src="_images/math/9d86170e7de539c0ff999de09621ee0c7b6c8ed0.png" alt="R"/> is large, which happens when <img class="math" src="_images/math/18cbd415b1a8e3f19977c5d04d046d41c585c7de.png" alt="\lambda_1"/> and <img class="math" src="_images/math/2bc2d2b207f861ff1c70724ebb8a9cd6831c0d52.png" alt="\lambda_2"/> are large and <img class="math" src="_images/math/24e92e18a8e748f48baae814ccd28e427d6d1b81.png" alt="\lambda_1 \sim \lambda_2"/>, the region is a corner.</li>
</ul>
</div></blockquote>
<p>It can be represented in a nice picture as follows:</p>
<blockquote>
<div><img alt="Classification of Image Points" class="align-center" src="_images/harris_region.jpg" />
</div></blockquote>
<p>So the result of Harris Corner Detection is a grayscale image with these scores. Thresholding for a suitable give you the corners in the image. We will do it with a simple image.</p>
</div>
<div class="section" id="harris-corner-detector-in-opencv">
<h5>Harris Corner Detector in OpenCV<a class="headerlink" href="#harris-corner-detector-in-opencv" title="Permalink to this headline">¶</a></h5>
<p>OpenCV has the function <strong>cv2.cornerHarris()</strong> for this purpose. Its arguments are :</p>
<blockquote>
<div><ul class="simple">
<li><strong>img</strong> - Input image, it should be grayscale and float32 type.</li>
<li><strong>blockSize</strong> - It is the size of neighbourhood considered for corner detection</li>
<li><strong>ksize</strong> - Aperture parameter of Sobel derivative used.</li>
<li><strong>k</strong> - Harris detector free parameter in the equation.</li>
</ul>
</div></blockquote>
<p>See the example below:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">filename</span> <span class="o">=</span> <span class="s1">&#39;chessboard.jpg&#39;</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
<span class="n">gray</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2GRAY</span><span class="p">)</span>

<span class="n">gray</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">gray</span><span class="p">)</span>
<span class="n">dst</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cornerHarris</span><span class="p">(</span><span class="n">gray</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mf">0.04</span><span class="p">)</span>

<span class="c1">#result is dilated for marking the corners, not important</span>
<span class="n">dst</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">dilate</span><span class="p">(</span><span class="n">dst</span><span class="p">,</span><span class="kc">None</span><span class="p">)</span>

<span class="c1"># Threshold for an optimal value, it may vary depending on the image.</span>
<span class="n">img</span><span class="p">[</span><span class="n">dst</span><span class="o">&gt;</span><span class="mf">0.01</span><span class="o">*</span><span class="n">dst</span><span class="o">.</span><span class="n">max</span><span class="p">()]</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">]</span>

<span class="n">cv2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s1">&#39;dst&#39;</span><span class="p">,</span><span class="n">img</span><span class="p">)</span>
<span class="k">if</span> <span class="n">cv2</span><span class="o">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">&amp;</span> <span class="mh">0xff</span> <span class="o">==</span> <span class="mi">27</span><span class="p">:</span>
    <span class="n">cv2</span><span class="o">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>
</pre></div>
</div>
<p>Below are the three results:</p>
<blockquote>
<div><img alt="Harris Corner Detection" class="align-center" src="_images/harris_result.jpg" />
</div></blockquote>
</div>
<div class="section" id="corner-with-subpixel-accuracy">
<h5>Corner with SubPixel Accuracy<a class="headerlink" href="#corner-with-subpixel-accuracy" title="Permalink to this headline">¶</a></h5>
<p>Sometimes, you may need to find the corners with maximum accuracy. OpenCV comes with a function <strong>cv2.cornerSubPix()</strong> which further refines the corners detected with sub-pixel accuracy. Below is an example. As usual, we need to find the harris corners first. Then we pass the centroids of these corners (There may be a bunch of pixels at a corner, we take their centroid) to refine them. Harris corners are marked in red pixels and refined corners are marked in green pixels. For this function, we have to define the criteria when to stop the iteration. We stop it after a specified number of iteration or a certain accuracy is achieved, whichever occurs first. We also need to define the size of neighbourhood it would search for corners.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">filename</span> <span class="o">=</span> <span class="s1">&#39;chessboard2.jpg&#39;</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
<span class="n">gray</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2GRAY</span><span class="p">)</span>

<span class="c1"># find Harris corners</span>
<span class="n">gray</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">gray</span><span class="p">)</span>
<span class="n">dst</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cornerHarris</span><span class="p">(</span><span class="n">gray</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mf">0.04</span><span class="p">)</span>
<span class="n">dst</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">dilate</span><span class="p">(</span><span class="n">dst</span><span class="p">,</span><span class="kc">None</span><span class="p">)</span>
<span class="n">ret</span><span class="p">,</span> <span class="n">dst</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">threshold</span><span class="p">(</span><span class="n">dst</span><span class="p">,</span><span class="mf">0.01</span><span class="o">*</span><span class="n">dst</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">dst</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">(</span><span class="n">dst</span><span class="p">)</span>

<span class="c1"># find centroids</span>
<span class="n">ret</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">stats</span><span class="p">,</span> <span class="n">centroids</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">connectedComponentsWithStats</span><span class="p">(</span><span class="n">dst</span><span class="p">)</span>

<span class="c1"># define the criteria to stop and refine the corners</span>
<span class="n">criteria</span> <span class="o">=</span> <span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">TERM_CRITERIA_EPS</span> <span class="o">+</span> <span class="n">cv2</span><span class="o">.</span><span class="n">TERM_CRITERIA_MAX_ITER</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">)</span>
<span class="n">corners</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cornerSubPix</span><span class="p">(</span><span class="n">gray</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">centroids</span><span class="p">),(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">),(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span><span class="n">criteria</span><span class="p">)</span>

<span class="c1"># Now draw them</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">centroids</span><span class="p">,</span><span class="n">corners</span><span class="p">))</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">int0</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
<span class="n">img</span><span class="p">[</span><span class="n">res</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">res</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]]</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">]</span>
<span class="n">img</span><span class="p">[</span><span class="n">res</span><span class="p">[:,</span><span class="mi">3</span><span class="p">],</span><span class="n">res</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>

<span class="n">cv2</span><span class="o">.</span><span class="n">imwrite</span><span class="p">(</span><span class="s1">&#39;subpixel5.png&#39;</span><span class="p">,</span><span class="n">img</span><span class="p">)</span>
</pre></div>
</div>
<p>Below is the result, where some important locations are shown in zoomed window to visualize:</p>
<blockquote>
<div><img alt="Corner Detection with SubPixel Accuracy" class="align-center" src="_images/subpixel3.png" />
</div></blockquote>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
</div>
</div>
<span id="document-py_tutorials/py_feature2d/py_shi_tomasi/py_shi_tomasi"></span><div class="section" id="shi-tomasi-corner-detector-good-features-to-track">
<span id="shi-tomasi"></span><h4>Shi-Tomasi Corner Detector &amp; Good Features to Track<a class="headerlink" href="#shi-tomasi-corner-detector-good-features-to-track" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goal">
<h5>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h5>
<p>In this chapter,</p>
<blockquote>
<div><ul class="simple">
<li>We will learn about the another corner detector: Shi-Tomasi Corner Detector</li>
<li>We will see the function: <strong>cv2.goodFeaturesToTrack()</strong></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="theory">
<h5>Theory<a class="headerlink" href="#theory" title="Permalink to this headline">¶</a></h5>
<p>In last chapter, we saw Harris Corner Detector. Later in 1994, J. Shi and C. Tomasi made a small modification to it in their paper <strong>Good Features to Track</strong> which shows better results compared to Harris Corner Detector. The scoring function in Harris Corner Detector was given by:</p>
<div class="math">
<p><img src="_images/math/0a3ea39f0c903da7c210e2912e3c51805597f23a.png" alt="R = \lambda_1 \lambda_2 - k(\lambda_1+\lambda_2)^2"/></p>
</div><p>Instead of this, Shi-Tomasi proposed:</p>
<div class="math">
<p><img src="_images/math/1d9c5f834abe53109bc6cfea3557294bb38ba935.png" alt="R = min(\lambda_1, \lambda_2)"/></p>
</div><p>If it is a greater than a threshold value, it is considered as a corner. If we plot it in <img class="math" src="_images/math/8ab50472b609751c806617ea8d0d9864c89c1e19.png" alt="\lambda_1 - \lambda_2"/> space as we did in Harris Corner Detector, we get an image as below:</p>
<blockquote>
<div><img alt="Shi-Tomasi Corner Space" class="align-center" src="_images/shitomasi_space.png" />
</div></blockquote>
<p>From the figure, you can see that only when <img class="math" src="_images/math/18cbd415b1a8e3f19977c5d04d046d41c585c7de.png" alt="\lambda_1"/> and <img class="math" src="_images/math/2bc2d2b207f861ff1c70724ebb8a9cd6831c0d52.png" alt="\lambda_2"/> are above a minimum value, <img class="math" src="_images/math/ccd2c26c6a199e722b8a6bee922d32657e0ceee2.png" alt="\lambda_{min}"/>, it is conidered as a corner(green region).</p>
</div>
<div class="section" id="code">
<h5>Code<a class="headerlink" href="#code" title="Permalink to this headline">¶</a></h5>
<p>OpenCV has a function, <strong>cv2.goodFeaturesToTrack()</strong>. It finds N strongest corners in the image by Shi-Tomasi method (or Harris Corner Detection, if you specify it). As usual, image should be a grayscale image. Then you specify number of corners you want to find. Then you specify the quality level, which is a value between 0-1, which denotes the minimum quality of corner below which everyone is rejected. Then we provide the minimum euclidean distance between corners detected.</p>
<p>With all these informations, the function finds corners in the image. All corners below quality level are rejected. Then it sorts the remaining corners based on quality in the descending order. Then function takes first strongest corner, throws away all the nearby corners in the range of minimum distance and returns N strongest corners.</p>
<p>In below example, we will try to find 25 best corners:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;simple.jpg&#39;</span><span class="p">)</span>
<span class="n">gray</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2GRAY</span><span class="p">)</span>

<span class="n">corners</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">goodFeaturesToTrack</span><span class="p">(</span><span class="n">gray</span><span class="p">,</span><span class="mi">25</span><span class="p">,</span><span class="mf">0.01</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">corners</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">int0</span><span class="p">(</span><span class="n">corners</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">corners</span><span class="p">:</span>
    <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="n">i</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="n">cv2</span><span class="o">.</span><span class="n">circle</span><span class="p">(</span><span class="n">img</span><span class="p">,(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">),</span><span class="mi">3</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>See the result below:</p>
<blockquote>
<div><img alt="Shi-Tomasi Corners" class="align-center" src="_images/shitomasi_block1.jpg" />
</div></blockquote>
<p>This function is more appropriate for tracking. We will see that when its time comes.</p>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
</div>
</div>
<span id="document-py_tutorials/py_feature2d/py_sift_intro/py_sift_intro"></span><div class="section" id="introduction-to-sift-scale-invariant-feature-transform">
<span id="sift-intro"></span><h4>Introduction to SIFT (Scale-Invariant Feature Transform)<a class="headerlink" href="#introduction-to-sift-scale-invariant-feature-transform" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goal">
<h5>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h5>
<dl class="docutils">
<dt>In this chapter,</dt>
<dd><ul class="first last simple">
<li>We will learn about the concepts of SIFT algorithm</li>
<li>We will learn to find SIFT Keypoints and Descriptors.</li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="theory">
<h5>Theory<a class="headerlink" href="#theory" title="Permalink to this headline">¶</a></h5>
<p>In last couple of chapters, we saw some corner detectors like Harris etc. They are rotation-invariant, which means, even if the image is rotated, we can find the same corners. It is obvious because corners remain corners in rotated image also. But what about scaling? A corner may not be a corner if the image is scaled. For example, check a simple image below. A corner in a small image within a small window is flat when it is zoomed in the same window. So Harris corner is not scale invariant.</p>
<blockquote>
<div><img alt="Scale-Invariance" class="align-center" src="_images/sift_scale_invariant.jpg" />
</div></blockquote>
<p>So, in 2004, <strong>D.Lowe</strong>, University of British Columbia, came up with a new algorithm, Scale Invariant Feature Transform (SIFT) in his paper, <strong>Distinctive Image Features from Scale-Invariant Keypoints</strong>, which extract keypoints and compute its descriptors. <em>(This paper is easy to understand and considered to be best material available on SIFT. So this explanation is just a short summary of this paper)</em>.</p>
<p>There are mainly four steps involved in SIFT algorithm. We will see them one-by-one.</p>
<div class="section" id="scale-space-extrema-detection">
<h6>1. Scale-space Extrema Detection<a class="headerlink" href="#scale-space-extrema-detection" title="Permalink to this headline">¶</a></h6>
<p>From the image above, it is obvious that we can&#8217;t use the same window to detect keypoints with different scale. It is OK with small corner. But to detect larger corners we need larger windows. For this, scale-space filtering is used. In it, Laplacian of Gaussian is found for the image with various <img class="math" src="_images/math/2298cf1485084afe72757a9c8483af49a138d81f.png" alt="\sigma"/> values. LoG acts as a blob detector which detects blobs in various sizes due to change in <img class="math" src="_images/math/2298cf1485084afe72757a9c8483af49a138d81f.png" alt="\sigma"/>. In short, <img class="math" src="_images/math/2298cf1485084afe72757a9c8483af49a138d81f.png" alt="\sigma"/> acts as a scaling parameter. For eg, in the above image, gaussian kernel with low <img class="math" src="_images/math/2298cf1485084afe72757a9c8483af49a138d81f.png" alt="\sigma"/> gives high value for small corner while guassian kernel with high <img class="math" src="_images/math/2298cf1485084afe72757a9c8483af49a138d81f.png" alt="\sigma"/> fits well for larger corner. So, we can find the local maxima across the scale and space which gives us a list of <img class="math" src="_images/math/e794caf43ad872e6b7a1bba6c1ed0bd2fa58c231.png" alt="(x,y,\sigma)"/> values which means there is a potential keypoint at (x,y) at <img class="math" src="_images/math/2298cf1485084afe72757a9c8483af49a138d81f.png" alt="\sigma"/> scale.</p>
<p>But this LoG is a little costly, so SIFT algorithm uses Difference of Gaussians which is an approximation of LoG. Difference of Gaussian is obtained as the difference of Gaussian blurring of an image with two different <img class="math" src="_images/math/2298cf1485084afe72757a9c8483af49a138d81f.png" alt="\sigma"/>, let it be <img class="math" src="_images/math/2298cf1485084afe72757a9c8483af49a138d81f.png" alt="\sigma"/> and <img class="math" src="_images/math/65c868ead1d249f57d017e9e4c290d7f4770ebf9.png" alt="k\sigma"/>. This process is done for different octaves of the image in Gaussian Pyramid. It is represented in below image:</p>
<blockquote>
<div><img alt="Difference of Gaussian" class="align-center" src="_images/sift_dog.jpg" />
</div></blockquote>
<p>Once this DoG are found, images are searched for local extrema over scale and space. For eg, one pixel in an image is compared with its 8 neighbours as well as 9 pixels in next scale and 9 pixels in previous scales. If it is a local extrema, it is a potential keypoint. It basically means that keypoint is best represented in that scale. It is shown in below image:</p>
<blockquote>
<div><img alt="Difference of Gaussian" class="align-center" src="_images/sift_local_extrema.jpg" />
</div></blockquote>
<p>Regarding different parameters, the paper gives some empirical data which can be summarized as, number of octaves = 4, number of scale levels = 5, initial <img class="math" src="_images/math/f86e886ab2712cd539f7e0cfbc7e39851b7964f6.png" alt="\sigma=1.6"/>, <img class="math" src="_images/math/f5833de2909fd83d1fc9d0370fa5ee3355674809.png" alt="k=\sqrt{2}"/> etc as optimal values.</p>
</div>
<div class="section" id="keypoint-localization">
<h6>2. Keypoint Localization<a class="headerlink" href="#keypoint-localization" title="Permalink to this headline">¶</a></h6>
<p>Once potential keypoints locations are found, they have to be refined to get more accurate results. They used Taylor series expansion of scale space to get more accurate location of extrema, and if the intensity at this extrema is less than a threshold value (0.03 as per the paper), it is rejected. This threshold is called <strong>contrastThreshold</strong> in OpenCV</p>
<p>DoG has higher response for edges, so edges also need to be removed. For this, a concept similar to Harris corner detector is used. They used a 2x2 Hessian matrix (H) to compute the pricipal curvature. We know from Harris corner detector that for edges, one eigen value is larger than the other. So here they used a simple function,</p>
<p>If this ratio is greater than a threshold, called <strong>edgeThreshold</strong> in OpenCV, that keypoint is discarded. It is given as 10 in paper.</p>
<p>So it eliminates any low-contrast keypoints and edge keypoints and what remains is strong interest points.</p>
</div>
<div class="section" id="orientation-assignment">
<h6>3. Orientation Assignment<a class="headerlink" href="#orientation-assignment" title="Permalink to this headline">¶</a></h6>
<p>Now an orientation is assigned to each keypoint to achieve invariance to image rotation. A neigbourhood is taken around the keypoint location depending on the scale, and the gradient magnitude and direction is calculated in that region. An orientation histogram with 36 bins covering 360 degrees is created. (It is weighted by gradient magnitude and gaussian-weighted circular window with <img class="math" src="_images/math/2298cf1485084afe72757a9c8483af49a138d81f.png" alt="\sigma"/> equal to 1.5 times the scale of keypoint. The highest peak in the histogram is taken and any peak above 80% of it is also considered to calculate the orientation. It creates keypoints with same location and scale, but different directions. It contribute to stability of matching.</p>
</div>
<div class="section" id="keypoint-descriptor">
<h6>4. Keypoint Descriptor<a class="headerlink" href="#keypoint-descriptor" title="Permalink to this headline">¶</a></h6>
<p>Now keypoint descriptor is created. A 16x16 neighbourhood around the keypoint is taken. It is devided into 16 sub-blocks of 4x4 size. For each sub-block, 8 bin orientation histogram is created. So a total of 128 bin values are available. It is represented as a vector to form keypoint descriptor. In addition to this, several measures are taken to achieve robustness against illumination changes, rotation etc.</p>
</div>
<div class="section" id="keypoint-matching">
<h6>5. Keypoint Matching<a class="headerlink" href="#keypoint-matching" title="Permalink to this headline">¶</a></h6>
<p>Keypoints between two images are matched by identifying their nearest neighbours. But in some cases, the second closest-match may be very near to the first. It may happen due to noise or some other reasons. In that case, ratio of closest-distance to second-closest distance is taken. If it is greater than 0.8, they are rejected. It eliminaters around 90% of false matches while discards only 5% correct matches, as per the paper.</p>
<p>So this is a summary of SIFT algorithm. For more details and understanding, reading the original paper is highly recommended. Remember one thing, this algorithm is patented. So this algorithm is included in Non-free module in OpenCV.</p>
</div>
</div>
<div class="section" id="sift-in-opencv">
<h5>SIFT in OpenCV<a class="headerlink" href="#sift-in-opencv" title="Permalink to this headline">¶</a></h5>
<p>So now let&#8217;s see SIFT functionalities available in OpenCV. Let&#8217;s start with keypoint detection and draw them. First we have to construct a SIFT object. We can pass different parameters to it which are optional and they are well explained in docs.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;home.jpg&#39;</span><span class="p">)</span>
<span class="n">gray</span><span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2GRAY</span><span class="p">)</span>

<span class="n">sift</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">SIFT</span><span class="p">()</span>
<span class="n">kp</span> <span class="o">=</span> <span class="n">sift</span><span class="o">.</span><span class="n">detect</span><span class="p">(</span><span class="n">gray</span><span class="p">,</span><span class="kc">None</span><span class="p">)</span>

<span class="n">img</span><span class="o">=</span><span class="n">cv2</span><span class="o">.</span><span class="n">drawKeypoints</span><span class="p">(</span><span class="n">gray</span><span class="p">,</span><span class="n">kp</span><span class="p">)</span>

<span class="n">cv2</span><span class="o">.</span><span class="n">imwrite</span><span class="p">(</span><span class="s1">&#39;sift_keypoints.jpg&#39;</span><span class="p">,</span><span class="n">img</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>sift.detect()</strong> function finds the keypoint in the images. You can pass a mask if you want to search only a part of image. Each keypoint is a special structure which has many attributes like its (x,y) coordinates, size of the meaningful neighbourhood, angle which specifies its orientation, response that specifies strength of keypoints etc.</p>
<p>OpenCV also provides <strong>cv2.drawKeyPoints()</strong> function which draws the small circles on the locations of keypoints. If you pass a flag, <strong>cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS</strong> to it, it will draw a circle with size of keypoint and it will even show its orientation. See below example.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">img</span><span class="o">=</span><span class="n">cv2</span><span class="o">.</span><span class="n">drawKeypoints</span><span class="p">(</span><span class="n">gray</span><span class="p">,</span><span class="n">kp</span><span class="p">,</span><span class="n">flags</span><span class="o">=</span><span class="n">cv2</span><span class="o">.</span><span class="n">DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS</span><span class="p">)</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">imwrite</span><span class="p">(</span><span class="s1">&#39;sift_keypoints.jpg&#39;</span><span class="p">,</span><span class="n">img</span><span class="p">)</span>
</pre></div>
</div>
<p>See the two results below:</p>
<blockquote>
<div><img alt="SIFT Keypoints" class="align-center" src="_images/sift_keypoints.jpg" />
</div></blockquote>
<p>Now to calculate the descriptor, OpenCV provides two methods.</p>
<ol class="arabic simple">
<li>Since you already found keypoints, you can call <strong>sift.compute()</strong> which computes the descriptors from the keypoints we have found. Eg: <code class="docutils literal"><span class="pre">kp,des</span> <span class="pre">=</span> <span class="pre">sift.compute(gray,kp)</span></code></li>
<li>If you didn&#8217;t find keypoints, directly find keypoints and descriptors in a single step with the function, <strong>sift.detectAndCompute()</strong>.</li>
</ol>
<p>We will see the second method:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">sift</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">SIFT</span><span class="p">()</span>
<span class="n">kp</span><span class="p">,</span> <span class="n">des</span> <span class="o">=</span> <span class="n">sift</span><span class="o">.</span><span class="n">detectAndCompute</span><span class="p">(</span><span class="n">gray</span><span class="p">,</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
<p>Here kp will be a list of keypoints and des is a numpy array of shape <img class="math" src="_images/math/fae33102db976e0e6798937a940f26d53b3c97d2.png" alt="Number\_of\_Keypoints \times 128"/>.</p>
<p>So we got keypoints, descriptors etc. Now we want to see how to match keypoints in different images. That we will learn in coming chapters.</p>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
</div>
</div>
<span id="document-py_tutorials/py_feature2d/py_surf_intro/py_surf_intro"></span><div class="section" id="introduction-to-surf-speeded-up-robust-features">
<span id="surf"></span><h4>Introduction to SURF (Speeded-Up Robust Features)<a class="headerlink" href="#introduction-to-surf-speeded-up-robust-features" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goal">
<h5>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h5>
<dl class="docutils">
<dt>In this chapter,</dt>
<dd><ul class="first last simple">
<li>We will see the basics of SURF</li>
<li>We will see SURF functionalities in OpenCV</li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="theory">
<h5>Theory<a class="headerlink" href="#theory" title="Permalink to this headline">¶</a></h5>
<p>In last chapter, we saw SIFT for keypoint detection and description. But it was comparatively slow and people needed more speeded-up version. In 2006, three people, Bay, H., Tuytelaars, T. and Van Gool, L, published another paper, &#8220;SURF: Speeded Up Robust Features&#8221; which introduced a new algorithm called SURF. As name suggests, it is a speeded-up version of SIFT.</p>
<p>In SIFT, Lowe approximated Laplacian of Gaussian with Difference of Gaussian for finding scale-space. SURF goes a little further and approximates LoG with Box Filter. Below image shows a demonstration of such an approximation. One big advantage of this approximation is that, convolution with box filter can be easily calculated with the help of integral images. And it can be done in parallel for different scales. Also the SURF rely on determinant of Hessian matrix for both scale and location.</p>
<blockquote>
<div><img alt="Box Filter approximation of Laplacian" class="align-center" src="_images/surf_boxfilter.jpg" />
</div></blockquote>
<p>For orientation assignment, SURF uses wavelet responses in horizontal and vertical direction for a neighbourhood of size 6s. Adequate guassian weights are also applied to it. Then they are plotted in a space as given in below image.  The dominant orientation is estimated by calculating the sum of all responses within a sliding orientation window of angle 60 degrees. Interesting thing is that, wavelet response can be found out using integral images very easily at any scale. For many applications, rotation invariance is not required, so no need of finding this orientation, which speeds up the process. SURF provides such a functionality called Upright-SURF or U-SURF. It improves speed and is robust upto <img class="math" src="_images/math/7e20ab7d0ba6233d83535b56fba5ad0b87f39c4a.png" alt="\pm 15^{\circ}"/>. OpenCV supports both, depending upon the flag, <strong>upright</strong>. If it is 0, orientation is calculated. If it is 1, orientation is not calculated and it is more faster.</p>
<blockquote>
<div><img alt="Orientation Assignment in SURF" class="align-center" src="_images/surf_orientation.jpg" />
</div></blockquote>
<p>For feature description, SURF uses Wavelet responses in horizontal and vertical direction (again, use of integral images makes things easier). A neighbourhood of size 20sX20s is taken around the keypoint where s is the size. It is divided into 4x4 subregions. For each subregion, horizontal and vertical wavelet responses are taken and a vector is formed like this, <img class="math" src="_images/math/d0155d0b0cd01830f093a4830c900ea748b50abd.png" alt="v=( \sum{d_x}, \sum{d_y}, \sum{|d_x|}, \sum{|d_y|})"/>. This when represented as a vector gives SURF feature descriptor with total 64 dimensions. Lower the dimension, higher the speed of computation and matching, but provide better distinctiveness of features.</p>
<p>For more distinctiveness, SURF feature descriptor has an extended 128 dimension version. The sums of <img class="math" src="_images/math/5f348c3bf9ef6a6c8fca69aa052b80ddc6ff8ad2.png" alt="d_x"/> and <img class="math" src="_images/math/bc8ead0d6b126b13de51d9ec5d011faa03853d18.png" alt="|d_x|"/> are computed  separately for <img class="math" src="_images/math/cd6bddeb6571cf99113ddd5d4529e68e760f573f.png" alt="d_y &lt; 0"/> and <img class="math" src="_images/math/c24cf9a8bf2129edd5faa59483ead0824e1fb6a2.png" alt="d_y \geq 0"/>. Similarly, the sums of <img class="math" src="_images/math/7b7f1c01e226c2f2e7e6c8f739988e8039af5252.png" alt="d_y"/> and <img class="math" src="_images/math/eec657398329b911726fb6dfbb7e19920ce123a1.png" alt="|d_y|"/> are split
up according to the sign of <img class="math" src="_images/math/5f348c3bf9ef6a6c8fca69aa052b80ddc6ff8ad2.png" alt="d_x"/> , thereby doubling the number of features. It doesn&#8217;t add much computation complexity. OpenCV supports both by setting the value of flag <strong>extended</strong> with 0 and 1 for 64-dim and 128-dim respectively (default is 128-dim)</p>
<p>Another important improvement is the use of sign of Laplacian (trace of Hessian Matrix) for underlying interest point. It adds no computation cost since it is already computed during detection. The sign of the Laplacian distinguishes bright blobs on dark backgrounds from the reverse situation. In the matching stage, we only compare features if they have the same type of contrast (as shown in image below). This minimal information allows for faster matching, without reducing the descriptor&#8217;s performance.</p>
<blockquote>
<div><img alt="Fast Indexing for Matching" class="align-center" src="_images/surf_matching.jpg" />
</div></blockquote>
<p>In short, SURF adds a lot of features to improve the speed in every step. Analysis shows it is 3 times faster than SIFT while performance is comparable to SIFT. SURF is good at handling images with blurring and rotation, but not good at handling viewpoint change and illumination change.</p>
</div>
<div class="section" id="surf-in-opencv">
<h5>SURF in OpenCV<a class="headerlink" href="#surf-in-opencv" title="Permalink to this headline">¶</a></h5>
<p>OpenCV provides SURF functionalities just like SIFT. You initiate a SURF object with some optional conditions like 64/128-dim descriptors, Upright/Normal SURF etc. All the details are well explained in docs. Then as we did in SIFT, we can use SURF.detect(), SURF.compute() etc for finding keypoints and descriptors.</p>
<p>First we will see a simple demo on how to find SURF keypoints and descriptors and draw it. All examples are shown in Python terminal since it is just same as SIFT only.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;fly.png&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>

<span class="go"># Create SURF object. You can specify params here or later.</span>
<span class="go"># Here I set Hessian Threshold to 400</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">surf</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">SURF</span><span class="p">(</span><span class="mi">400</span><span class="p">)</span>

<span class="go"># Find keypoints and descriptors directly</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kp</span><span class="p">,</span> <span class="n">des</span> <span class="o">=</span> <span class="n">surf</span><span class="o">.</span><span class="n">detectAndCompute</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="kc">None</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="nb">len</span><span class="p">(</span><span class="n">kp</span><span class="p">)</span>
<span class="go"> 699</span>
</pre></div>
</div>
<p>1199 keypoints is too much to show in a picture. We reduce it to some 50 to draw it on an image. While matching, we may need all those features, but not now. So we increase the Hessian Threshold.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="c1"># Check present Hessian threshold</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="n">surf</span><span class="o">.</span><span class="n">hessianThreshold</span>
<span class="mf">400.0</span>

<span class="c1"># We set it to some 50000. Remember, it is just for representing in picture.</span>
<span class="c1"># In actual cases, it is better to have a value 300-500</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">surf</span><span class="o">.</span><span class="n">hessianThreshold</span> <span class="o">=</span> <span class="mi">50000</span>

<span class="c1"># Again compute keypoints and check its number.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">kp</span><span class="p">,</span> <span class="n">des</span> <span class="o">=</span> <span class="n">surf</span><span class="o">.</span><span class="n">detectAndCompute</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="kc">None</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="nb">len</span><span class="p">(</span><span class="n">kp</span><span class="p">)</span>
<span class="mi">47</span>
</pre></div>
</div>
<p>It is less than 50. Let&#8217;s draw it on the image.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">img2</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">drawKeypoints</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">kp</span><span class="p">,</span><span class="kc">None</span><span class="p">,(</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span><span class="mi">4</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img2</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>See the result below. You can see that SURF is more like a blob detector. It detects the white blobs on wings of butterfly. You can test it with other images.</p>
<blockquote>
<div><img alt="SURF Keypoints with Orientation" class="align-center" src="_images/surf_kp1.jpg" />
</div></blockquote>
<p>Now I want to apply U-SURF, so that it won&#8217;t find the orientation.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="c1"># Check upright flag, if it False, set it to True</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="n">surf</span><span class="o">.</span><span class="n">upright</span>
<span class="kc">False</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">surf</span><span class="o">.</span><span class="n">upright</span> <span class="o">=</span> <span class="kc">True</span>

<span class="c1"># Recompute the feature points and draw it</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">kp</span> <span class="o">=</span> <span class="n">surf</span><span class="o">.</span><span class="n">detect</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="kc">None</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">img2</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">drawKeypoints</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">kp</span><span class="p">,</span><span class="kc">None</span><span class="p">,(</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span><span class="mi">4</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img2</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>See the results below. All the orientations are shown in same direction. It is more faster than previous. If you are working on cases where orientation is not a problem (like panorama stitching) etc, this is more better.</p>
<blockquote>
<div><img alt="Upright-SURF" class="align-center" src="_images/surf_kp2.jpg" />
</div></blockquote>
<p>Finally we check the descriptor size and change it to 128 if it is only 64-dim.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="c1"># Find size of descriptor</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="n">surf</span><span class="o">.</span><span class="n">descriptorSize</span><span class="p">()</span>
<span class="mi">64</span>

<span class="c1"># That means flag, &quot;extended&quot; is False.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">surf</span><span class="o">.</span><span class="n">extended</span>
 <span class="kc">False</span>

<span class="c1"># So we make it to True to get 128-dim descriptors.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">surf</span><span class="o">.</span><span class="n">extended</span> <span class="o">=</span> <span class="kc">True</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">kp</span><span class="p">,</span> <span class="n">des</span> <span class="o">=</span> <span class="n">surf</span><span class="o">.</span><span class="n">detectAndCompute</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="kc">None</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="n">surf</span><span class="o">.</span><span class="n">descriptorSize</span><span class="p">()</span>
<span class="mi">128</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="n">des</span><span class="o">.</span><span class="n">shape</span>
<span class="p">(</span><span class="mi">47</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
</pre></div>
</div>
<p>Remaining part is matching which we will do in another chapter.</p>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
</div>
</div>
<span id="document-py_tutorials/py_feature2d/py_fast/py_fast"></span><div class="section" id="fast-algorithm-for-corner-detection">
<span id="fast"></span><h4>FAST Algorithm for Corner Detection<a class="headerlink" href="#fast-algorithm-for-corner-detection" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goal">
<h5>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h5>
<dl class="docutils">
<dt>In this chapter,</dt>
<dd><ul class="first last simple">
<li>We will understand the basics of FAST algorithm</li>
<li>We will find corners using OpenCV functionalities for FAST algorithm.</li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="theory">
<h5>Theory<a class="headerlink" href="#theory" title="Permalink to this headline">¶</a></h5>
<p>We saw several feature detectors and many of them are really good. But when looking from a real-time application point of view, they are not fast enough. One best example would be SLAM (Simultaneous Localization and Mapping) mobile robot which have limited computational resources.</p>
<p>As a solution to this, FAST (Features from Accelerated Segment Test) algorithm was proposed by Edward Rosten and Tom Drummond in their paper &#8220;Machine learning for high-speed corner detection&#8221; in 2006 (Later revised it in 2010). A basic summary of the algorithm is presented below. Refer original paper for more details (All the images are taken from original paper).</p>
<div class="section" id="feature-detection-using-fast">
<h6>Feature Detection using FAST<a class="headerlink" href="#feature-detection-using-fast" title="Permalink to this headline">¶</a></h6>
<ol class="arabic">
<li><p class="first">Select a pixel <img class="math" src="_images/math/3eca8557203e86160952e1c0f735f7417f3285b1.png" alt="p"/> in the image which is to be identified as an interest point or not. Let its intensity be <img class="math" src="_images/math/b5ad6718bfc849d18daea7cac20b339542279cf6.png" alt="I_p"/>.</p>
</li>
<li><p class="first">Select appropriate threshold value <img class="math" src="_images/math/ef9270877405055756d345facd044e4ab297f858.png" alt="t"/>.</p>
</li>
<li><p class="first">Consider a circle of 16 pixels around the pixel under test. (See the image below)</p>
<blockquote>
<div><img alt="A corner in the image" class="align-center" src="_images/fast_speedtest.jpg" />
</div></blockquote>
</li>
<li><p class="first">Now the pixel <img class="math" src="_images/math/3eca8557203e86160952e1c0f735f7417f3285b1.png" alt="p"/> is a corner if there exists a set of <img class="math" src="_images/math/413f8a8e40062a9090d9d50b88bc7b551b314c26.png" alt="n"/> contiguous pixels in the circle (of 16 pixels) which are all brighter than <img class="math" src="_images/math/9d7b7117a739a96f996b556de9614623f79db7e0.png" alt="I_p + t"/>, or all darker than <img class="math" src="_images/math/d610a2bb2a77d1b970821b66f264b387292baaa9.png" alt="I_p − t"/>. (Shown as white dash lines in the above image). <img class="math" src="_images/math/413f8a8e40062a9090d9d50b88bc7b551b314c26.png" alt="n"/> was chosen to be 12.</p>
</li>
<li><p class="first">A <strong>high-speed test</strong> was proposed to exclude a large number of non-corners. This test examines only the four pixels at 1, 9, 5 and 13 (First 1 and 9 are tested if they are too brighter or darker. If so, then checks 5 and 13). If <img class="math" src="_images/math/3eca8557203e86160952e1c0f735f7417f3285b1.png" alt="p"/> is a corner, then at least three of these must all be brighter than <img class="math" src="_images/math/9d7b7117a739a96f996b556de9614623f79db7e0.png" alt="I_p + t"/> or darker than <img class="math" src="_images/math/d610a2bb2a77d1b970821b66f264b387292baaa9.png" alt="I_p − t"/>. If neither of these is the case, then <img class="math" src="_images/math/3eca8557203e86160952e1c0f735f7417f3285b1.png" alt="p"/> cannot be a corner. The full segment test criterion can then be applied to the passed candidates by examining all pixels in the circle. This detector in itself exhibits high performance, but there are several weaknesses:</p>
<blockquote>
<div><ul class="simple">
<li>It does not reject as many candidates for n &lt; 12.</li>
<li>The choice of pixels is not optimal because its efficiency depends on ordering of the questions and distribution of corner appearances.</li>
<li>Results of high-speed tests are thrown away.</li>
<li>Multiple features are detected adjacent to one another.</li>
</ul>
</div></blockquote>
</li>
</ol>
<p>First 3 points are addressed with a machine learning approach. Last one is addressed using non-maximal suppression.</p>
</div>
<div class="section" id="machine-learning-a-corner-detector">
<h6>Machine Learning a Corner Detector<a class="headerlink" href="#machine-learning-a-corner-detector" title="Permalink to this headline">¶</a></h6>
<ol class="arabic">
<li><p class="first">Select a set of images for training (preferably from the target application domain)</p>
</li>
<li><p class="first">Run FAST algorithm in every images to find feature points.</p>
</li>
<li><p class="first">For every feature point, store the 16 pixels around it as a vector. Do it for all the images to get feature vector <img class="math" src="_images/math/f48b617185733d8dd6712643f1ab17c736661a06.png" alt="P"/>.</p>
</li>
<li><p class="first">Each pixel (say <img class="math" src="_images/math/188c175aac0a8a9c22499336711b5d7256407254.png" alt="x"/>) in these 16 pixels can have one of the following three states:</p>
<blockquote>
<div><img alt="FAST equation" class="align-center" src="_images/fast_eqns.jpg" />
</div></blockquote>
</li>
<li><p class="first">Depending on these states, the feature vector <img class="math" src="_images/math/f48b617185733d8dd6712643f1ab17c736661a06.png" alt="P"/> is subdivided into 3 subsets, <img class="math" src="_images/math/8da43f60293e922bb2bc13fa4f6e269279060df2.png" alt="P_d"/>, <img class="math" src="_images/math/ed9f4c9fc87ae987b59a3aa3760298ab588192b5.png" alt="P_s"/>, <img class="math" src="_images/math/1edfaf11fc49f475c1571e8e75c58c701ca10418.png" alt="P_b"/>.</p>
</li>
<li><p class="first">Define a new boolean variable, <img class="math" src="_images/math/fdd885ab223e162af552c760a6a9929cda3ac17f.png" alt="K_p"/>, which is true if <img class="math" src="_images/math/3eca8557203e86160952e1c0f735f7417f3285b1.png" alt="p"/> is a corner and false otherwise.</p>
</li>
<li><p class="first">Use the ID3 algorithm (decision tree classifier) to query each subset using the variable <img class="math" src="_images/math/fdd885ab223e162af552c760a6a9929cda3ac17f.png" alt="K_p"/> for the knowledge about the true class. It selects the <img class="math" src="_images/math/188c175aac0a8a9c22499336711b5d7256407254.png" alt="x"/> which yields the most information about whether the candidate pixel is a corner, measured by the entropy of <img class="math" src="_images/math/fdd885ab223e162af552c760a6a9929cda3ac17f.png" alt="K_p"/>.</p>
</li>
<li><p class="first">This is recursively applied to all the subsets until its entropy is zero.</p>
</li>
<li><p class="first">The decision tree so created is used for fast detection in other images.</p>
</li>
</ol>
</div>
<div class="section" id="non-maximal-suppression">
<h6>Non-maximal Suppression<a class="headerlink" href="#non-maximal-suppression" title="Permalink to this headline">¶</a></h6>
<p>Detecting multiple interest points in adjacent locations is another problem. It is solved by using Non-maximum Suppression.</p>
<ol class="arabic simple">
<li>Compute a score function, <img class="math" src="_images/math/c99df7a209495334da442b1ec998abaabfa320d8.png" alt="V"/> for all the detected feature points. <img class="math" src="_images/math/c99df7a209495334da442b1ec998abaabfa320d8.png" alt="V"/> is the sum of absolute difference between <img class="math" src="_images/math/3eca8557203e86160952e1c0f735f7417f3285b1.png" alt="p"/> and 16 surrounding pixels values.</li>
<li>Consider two adjacent keypoints and compute their <img class="math" src="_images/math/c99df7a209495334da442b1ec998abaabfa320d8.png" alt="V"/> values.</li>
<li>Discard the one with lower <img class="math" src="_images/math/c99df7a209495334da442b1ec998abaabfa320d8.png" alt="V"/> value.</li>
</ol>
</div>
<div class="section" id="summary">
<h6>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h6>
<p>It is several times faster than other existing corner detectors.</p>
<p>But it is not robust to high levels of noise. It is dependant on a threshold.</p>
</div>
</div>
<div class="section" id="fast-feature-detector-in-opencv">
<h5>FAST Feature Detector in OpenCV<a class="headerlink" href="#fast-feature-detector-in-opencv" title="Permalink to this headline">¶</a></h5>
<p>It is called as any other feature detector in OpenCV. If you want, you can specify the threshold, whether non-maximum suppression to be applied or not, the neighborhood to be used etc.</p>
<p>For the neighborhood, three flags are defined, <code class="docutils literal"><span class="pre">cv2.FAST_FEATURE_DETECTOR_TYPE_5_8</span></code>, <code class="docutils literal"><span class="pre">cv2.FAST_FEATURE_DETECTOR_TYPE_7_12</span></code> and  <code class="docutils literal"><span class="pre">cv2.FAST_FEATURE_DETECTOR_TYPE_9_16</span></code>. Below is a simple code on how to detect and draw the FAST feature points.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;simple.jpg&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Initiate FAST object with default values</span>
<span class="n">fast</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">FastFeatureDetector</span><span class="p">()</span>

<span class="c1"># find and draw the keypoints</span>
<span class="n">kp</span> <span class="o">=</span> <span class="n">fast</span><span class="o">.</span><span class="n">detect</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="kc">None</span><span class="p">)</span>
<span class="n">img2</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">drawKeypoints</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">kp</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="p">(</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span>

<span class="c1"># Print all default params</span>
<span class="nb">print</span> <span class="s2">&quot;Threshold: &quot;</span><span class="p">,</span> <span class="n">fast</span><span class="o">.</span><span class="n">getInt</span><span class="p">(</span><span class="s1">&#39;threshold&#39;</span><span class="p">)</span>
<span class="nb">print</span> <span class="s2">&quot;nonmaxSuppression: &quot;</span><span class="p">,</span> <span class="n">fast</span><span class="o">.</span><span class="n">getBool</span><span class="p">(</span><span class="s1">&#39;nonmaxSuppression&#39;</span><span class="p">)</span>
<span class="nb">print</span> <span class="s2">&quot;neighborhood: &quot;</span><span class="p">,</span> <span class="n">fast</span><span class="o">.</span><span class="n">getInt</span><span class="p">(</span><span class="s1">&#39;type&#39;</span><span class="p">)</span>
<span class="nb">print</span> <span class="s2">&quot;Total Keypoints with nonmaxSuppression: &quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">kp</span><span class="p">)</span>

<span class="n">cv2</span><span class="o">.</span><span class="n">imwrite</span><span class="p">(</span><span class="s1">&#39;fast_true.png&#39;</span><span class="p">,</span><span class="n">img2</span><span class="p">)</span>

<span class="c1"># Disable nonmaxSuppression</span>
<span class="n">fast</span><span class="o">.</span><span class="n">setBool</span><span class="p">(</span><span class="s1">&#39;nonmaxSuppression&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">kp</span> <span class="o">=</span> <span class="n">fast</span><span class="o">.</span><span class="n">detect</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="kc">None</span><span class="p">)</span>

<span class="nb">print</span> <span class="s2">&quot;Total Keypoints without nonmaxSuppression: &quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">kp</span><span class="p">)</span>

<span class="n">img3</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">drawKeypoints</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">kp</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="p">(</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span>

<span class="n">cv2</span><span class="o">.</span><span class="n">imwrite</span><span class="p">(</span><span class="s1">&#39;fast_false.png&#39;</span><span class="p">,</span><span class="n">img3</span><span class="p">)</span>
</pre></div>
</div>
<p>See the results. First image shows FAST with nonmaxSuppression and second one without nonmaxSuppression:</p>
<blockquote>
<div><img alt="FAST Keypoints" class="align-center" src="_images/fast_kp.jpg" />
</div></blockquote>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
<ol class="arabic simple">
<li>Edward Rosten and Tom Drummond, “Machine learning for high speed corner detection” in 9th European Conference on Computer Vision, vol. 1, 2006, pp. 430–443.</li>
<li>Edward Rosten, Reid Porter, and Tom Drummond, &#8220;Faster and better: a machine learning approach to corner detection&#8221; in IEEE Trans. Pattern Analysis and Machine Intelligence, 2010, vol 32, pp. 105-119.</li>
</ol>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
</div>
</div>
<span id="document-py_tutorials/py_feature2d/py_brief/py_brief"></span><div class="section" id="brief-binary-robust-independent-elementary-features">
<span id="brief"></span><h4>BRIEF (Binary Robust Independent Elementary Features)<a class="headerlink" href="#brief-binary-robust-independent-elementary-features" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goal">
<h5>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h5>
<dl class="docutils">
<dt>In this chapter</dt>
<dd><ul class="first last simple">
<li>We will see the basics of BRIEF algorithm</li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="theory">
<h5>Theory<a class="headerlink" href="#theory" title="Permalink to this headline">¶</a></h5>
<p>We know SIFT uses 128-dim vector for descriptors. Since it is using floating point numbers, it takes basically 512 bytes. Similarly SURF also takes minimum of 256 bytes (for 64-dim). Creating such a vector for thousands of features takes a lot of memory which are not feasible for resouce-constraint applications especially for embedded systems. Larger the memory, longer the time it takes for matching.</p>
<p>But all these dimensions may not be needed for actual matching. We can compress it using several methods like PCA, LDA etc. Even other methods like hashing using LSH (Locality Sensitive Hashing) is used to convert these SIFT descriptors in floating point numbers to binary strings. These binary strings are used to match features using Hamming distance. This provides better speed-up because finding hamming distance is just applying XOR and bit count, which are very fast in modern CPUs with SSE instructions. But here, we need to find the descriptors first, then only we can apply hashing, which doesn&#8217;t solve our initial problem on memory.</p>
<p>BRIEF comes into picture at this moment. It provides a shortcut to find the binary strings directly without finding descriptors. It takes smoothened image patch and selects a set of <img class="math" src="_images/math/1a1bf67a033cf1fac2e2cae40771b49186fe75f4.png" alt="n_d"/> (x,y) location pairs in an unique way (explained in paper). Then some pixel intensity comparisons are done on these location pairs. For eg, let first location pairs be <img class="math" src="_images/math/3eca8557203e86160952e1c0f735f7417f3285b1.png" alt="p"/> and <img class="math" src="_images/math/23f1b45408e5b4130c0f940fcbfcec54492cbdcd.png" alt="q"/>. If <img class="math" src="_images/math/7fc27b31a2f636c8ffcb3a30d7acf41a249752d9.png" alt="I(p) &lt; I(q)"/>, then its result is 1, else it is 0. This is applied for all the <img class="math" src="_images/math/1a1bf67a033cf1fac2e2cae40771b49186fe75f4.png" alt="n_d"/> location pairs to get a <img class="math" src="_images/math/1a1bf67a033cf1fac2e2cae40771b49186fe75f4.png" alt="n_d"/>-dimensional bitstring.</p>
<p>This <img class="math" src="_images/math/1a1bf67a033cf1fac2e2cae40771b49186fe75f4.png" alt="n_d"/> can be 128, 256 or 512. OpenCV supports all of these, but by default, it would be 256 (OpenCV represents it in bytes. So the values will be 16, 32 and 64). So once you get this, you can use Hamming Distance to match these descriptors.</p>
<p>One important point is that BRIEF is a feature descriptor, it doesn&#8217;t provide any method to find the features. So you will have to use any other feature detectors like SIFT, SURF etc. The paper recommends to use CenSurE which is a fast detector and BRIEF works even slightly better for CenSurE points than for SURF points.</p>
<p>In short, BRIEF is a faster method feature descriptor calculation and matching. It also provides high recognition rate unless there is large in-plane rotation.</p>
</div>
<div class="section" id="brief-in-opencv">
<h5>BRIEF in OpenCV<a class="headerlink" href="#brief-in-opencv" title="Permalink to this headline">¶</a></h5>
<p>Below code shows the computation of BRIEF descriptors with the help of CenSurE detector. (CenSurE detector is called STAR detector in OpenCV)</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;simple.jpg&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Initiate STAR detector</span>
<span class="n">star</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">FeatureDetector_create</span><span class="p">(</span><span class="s2">&quot;STAR&quot;</span><span class="p">)</span>

<span class="c1"># Initiate BRIEF extractor</span>
<span class="n">brief</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">DescriptorExtractor_create</span><span class="p">(</span><span class="s2">&quot;BRIEF&quot;</span><span class="p">)</span>

<span class="c1"># find the keypoints with STAR</span>
<span class="n">kp</span> <span class="o">=</span> <span class="n">star</span><span class="o">.</span><span class="n">detect</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="kc">None</span><span class="p">)</span>

<span class="c1"># compute the descriptors with BRIEF</span>
<span class="n">kp</span><span class="p">,</span> <span class="n">des</span> <span class="o">=</span> <span class="n">brief</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">kp</span><span class="p">)</span>

<span class="nb">print</span> <span class="n">brief</span><span class="o">.</span><span class="n">getInt</span><span class="p">(</span><span class="s1">&#39;bytes&#39;</span><span class="p">)</span>
<span class="nb">print</span> <span class="n">des</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<p>The function <code class="docutils literal"><span class="pre">brief.getInt('bytes')</span></code> gives the <img class="math" src="_images/math/1a1bf67a033cf1fac2e2cae40771b49186fe75f4.png" alt="n_d"/> size used in bytes. By default it is 32. Next one is matching, which will be done in another chapter.</p>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
<ol class="arabic simple">
<li>Michael Calonder, Vincent Lepetit, Christoph Strecha, and Pascal Fua, &#8220;BRIEF: Binary Robust Independent Elementary Features&#8221;, 11th European Conference on Computer Vision (ECCV), Heraklion, Crete. LNCS Springer, September 2010.</li>
<li>LSH (Locality Sensitive Hasing) at wikipedia.</li>
</ol>
</div>
</div>
<span id="document-py_tutorials/py_feature2d/py_orb/py_orb"></span><div class="section" id="orb-oriented-fast-and-rotated-brief">
<span id="orb"></span><h4>ORB (Oriented FAST and Rotated BRIEF)<a class="headerlink" href="#orb-oriented-fast-and-rotated-brief" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goal">
<h5>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h5>
<dl class="docutils">
<dt>In this chapter,</dt>
<dd><ul class="first last simple">
<li>We will see the basics of ORB</li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="theory">
<h5>Theory<a class="headerlink" href="#theory" title="Permalink to this headline">¶</a></h5>
<p>As an OpenCV enthusiast, the most important thing about the ORB is that it came from &#8220;OpenCV Labs&#8221;. This algorithm was brought up by Ethan Rublee, Vincent Rabaud, Kurt Konolige and Gary R. Bradski in their paper <strong>ORB: An efficient alternative to SIFT or SURF</strong> in 2011. As the title says, it is a good alternative to SIFT and SURF in computation cost, matching performance and mainly the patents. Yes, SIFT and SURF are patented and you are supposed to pay them for its use. But ORB is not !!!</p>
<p>ORB is basically a fusion of FAST keypoint detector and BRIEF descriptor with many modifications to enhance the performance. First it use FAST to find keypoints, then apply Harris corner measure to find top N points among them. It also use pyramid to produce multiscale-features. But one problem is that, FAST doesn&#8217;t compute the orientation. So what about rotation invariance? Authors came up with following modification.</p>
<p>It computes the intensity weighted centroid of the patch with located corner at center. The direction of the vector from this corner point to centroid gives the orientation. To improve the rotation invariance, moments are computed with x and y which should be in a circular region of radius <img class="math" src="_images/math/2ede365ad144ab396916ec60458da03860803078.png" alt="r"/>, where <img class="math" src="_images/math/2ede365ad144ab396916ec60458da03860803078.png" alt="r"/> is the size of the patch.</p>
<p>Now for descriptors, ORB use BRIEF descriptors. But we have already seen that BRIEF performs poorly with rotation. So what ORB does is to &#8220;steer&#8221; BRIEF according to the orientation of keypoints. For any feature set of <img class="math" src="_images/math/413f8a8e40062a9090d9d50b88bc7b551b314c26.png" alt="n"/> binary tests at location
<img class="math" src="_images/math/87efe7e04dfd6ce8180e15f5ecfdb2e8048761b1.png" alt="(x_i, y_i)"/>, define a <img class="math" src="_images/math/3975a215da61827de92e0aca6894ecbff0fc6d2e.png" alt="2 \times n"/> matrix, <img class="math" src="_images/math/11a85f3c69ae6702cb1d99d3de451913b8f84c04.png" alt="S"/> which contains the coordinates of these pixels. Then using the orientation of patch, <img class="math" src="_images/math/a9cfbeb8ebee1f365919e147a79e242dcb67ee5d.png" alt="\theta"/>, its rotation matrix is found and rotates the <img class="math" src="_images/math/11a85f3c69ae6702cb1d99d3de451913b8f84c04.png" alt="S"/> to get steered(rotated) version <img class="math" src="_images/math/d50645f91c3ec2b614261bac98c8a21b4e6d22e1.png" alt="S_\theta"/>.</p>
<p>ORB discretize the angle to increments of <img class="math" src="_images/math/dd38a81e192a5491d1c638d22e7514571e2648ed.png" alt="2 \pi /30"/> (12 degrees), and construct a lookup table of precomputed BRIEF patterns. As long as the keypoint orientation <img class="math" src="_images/math/a9cfbeb8ebee1f365919e147a79e242dcb67ee5d.png" alt="\theta"/> is consistent across views, the correct set of points <img class="math" src="_images/math/d50645f91c3ec2b614261bac98c8a21b4e6d22e1.png" alt="S_\theta"/> will be used to compute its descriptor.</p>
<p>BRIEF has an important property that each bit feature has a large variance and a mean near 0.5. But once it is oriented along keypoint direction, it loses this property and become more distributed. High variance makes a feature more discriminative, since it responds differentially to inputs. Another desirable property is to have the tests uncorrelated, since then each test will contribute to the result. To resolve all these, ORB runs a greedy search among all possible binary tests to find the ones that have both high variance and means close to 0.5, as well as being uncorrelated. The result is called <strong>rBRIEF</strong>.</p>
<p>For descriptor matching, multi-probe LSH which improves on the traditional LSH, is used. The paper says ORB is much faster than SURF and SIFT and ORB descriptor works better than SURF. ORB is a good choice in low-power devices for panorama stitching etc.</p>
</div>
<div class="section" id="orb-in-opencv">
<h5>ORB in OpenCV<a class="headerlink" href="#orb-in-opencv" title="Permalink to this headline">¶</a></h5>
<p>As usual, we have to create an ORB object with the function, <strong>cv2.ORB()</strong> or using feature2d common interface. It has a number of optional parameters. Most useful ones are <code class="docutils literal"><span class="pre">nFeatures</span></code> which denotes maximum number of features to be retained (by default 500), <code class="docutils literal"><span class="pre">scoreType</span></code> which denotes whether Harris score or FAST score to rank the features (by default, Harris score) etc. Another parameter, <code class="docutils literal"><span class="pre">WTA_K</span></code> decides number of points that produce each element of the oriented BRIEF descriptor. By default it is two, ie selects two points at a time. In that case, for matching, <code class="docutils literal"><span class="pre">NORM_HAMMING</span></code> distance is used. If WTA_K is 3 or 4, which takes 3 or 4 points to produce BRIEF descriptor, then matching distance is defined by <code class="docutils literal"><span class="pre">NORM_HAMMING2</span></code>.</p>
<p>Below is a simple code which shows the use of ORB.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;simple.jpg&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Initiate STAR detector</span>
<span class="n">orb</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">ORB</span><span class="p">()</span>

<span class="c1"># find the keypoints with ORB</span>
<span class="n">kp</span> <span class="o">=</span> <span class="n">orb</span><span class="o">.</span><span class="n">detect</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="kc">None</span><span class="p">)</span>

<span class="c1"># compute the descriptors with ORB</span>
<span class="n">kp</span><span class="p">,</span> <span class="n">des</span> <span class="o">=</span> <span class="n">orb</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">kp</span><span class="p">)</span>

<span class="c1"># draw only keypoints location,not size and orientation</span>
<span class="n">img2</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">drawKeypoints</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">kp</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="n">flags</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img2</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>See the result below:</p>
<blockquote>
<div><img alt="ORB Keypoints" class="align-center" src="_images/orb_kp.jpg" />
</div></blockquote>
<p>ORB feature matching, we will do in another chapter.</p>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
<ol class="arabic simple">
<li>Ethan Rublee, Vincent Rabaud, Kurt Konolige, Gary R. Bradski: ORB: An efficient alternative to SIFT or SURF. ICCV 2011: 2564-2571.</li>
</ol>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
</div>
</div>
<span id="document-py_tutorials/py_feature2d/py_matcher/py_matcher"></span><div class="section" id="feature-matching">
<span id="matcher"></span><h4>Feature Matching<a class="headerlink" href="#feature-matching" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goal">
<h5>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h5>
<dl class="docutils">
<dt>In this chapter</dt>
<dd><ul class="first last simple">
<li>We will see how to match features in one image with others.</li>
<li>We will use the Brute-Force matcher and FLANN Matcher in OpenCV</li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="basics-of-brute-force-matcher">
<h5>Basics of Brute-Force Matcher<a class="headerlink" href="#basics-of-brute-force-matcher" title="Permalink to this headline">¶</a></h5>
<p>Brute-Force matcher is simple. It takes the descriptor of one feature in first set and is matched with all other features in second set using some distance calculation. And the closest one is returned.</p>
<p>For BF matcher, first we have to create the BFMatcher object using <strong>cv2.BFMatcher()</strong>. It takes two optional params. First one is <code class="docutils literal"><span class="pre">normType</span></code>. It specifies the distance measurement to be used. By default, it is <code class="docutils literal"><span class="pre">cv2.NORM_L2</span></code>. It is good for SIFT, SURF etc (<code class="docutils literal"><span class="pre">cv2.NORM_L1</span></code> is also there). For binary string based descriptors like ORB, BRIEF, BRISK etc, <code class="docutils literal"><span class="pre">cv2.NORM_HAMMING</span></code> should be used, which used Hamming distance as measurement. If ORB is using <code class="docutils literal"><span class="pre">VTA_K</span> <span class="pre">==</span> <span class="pre">3</span> <span class="pre">or</span> <span class="pre">4</span></code>, <code class="docutils literal"><span class="pre">cv2.NORM_HAMMING2</span></code> should be used.</p>
<p>Second param is boolean variable, <code class="docutils literal"><span class="pre">crossCheck</span></code> which is false by default. If it is true, Matcher returns only those matches with value (i,j) such that i-th descriptor in set A has j-th descriptor in set B as the best match and vice-versa. That is, the two features in both sets should match each other. It provides consistant result, and is a good alternative to ratio test proposed by D.Lowe in SIFT paper.</p>
<p>Once it is created, two important methods are <em>BFMatcher.match()</em> and <em>BFMatcher.knnMatch()</em>. First one returns the best match. Second method returns <cite>k</cite> best matches where k is specified by the user. It may be useful when we need to do additional work on that.</p>
<p>Like we used cv2.drawKeypoints() to draw keypoints, <strong>cv2.drawMatches()</strong> helps us to draw the matches. It stacks two images horizontally and draw lines from first image to second image showing best matches. There is also <strong>cv2.drawMatchesKnn</strong> which draws all the k best matches. If k=2, it will draw two match-lines for each keypoint. So we have to pass a mask if we want to selectively draw it.</p>
<p>Let&#8217;s see one example for each of SURF and ORB (Both use different distance measurements).</p>
<div class="section" id="brute-force-matching-with-orb-descriptors">
<h6>Brute-Force Matching with ORB Descriptors<a class="headerlink" href="#brute-force-matching-with-orb-descriptors" title="Permalink to this headline">¶</a></h6>
<p>Here, we will see a simple example on how to match features between two images. In this case, I have a queryImage and a trainImage. We will try to find the queryImage in trainImage using feature matching. ( The images are <code class="docutils literal"><span class="pre">/samples/c/box.png</span></code> and <code class="docutils literal"><span class="pre">/samples/c/box_in_scene.png</span></code>)</p>
<p>We are using SIFT descriptors to match features. So let&#8217;s start with loading images, finding descriptors etc.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">img1</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;box.png&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>          <span class="c1"># queryImage</span>
<span class="n">img2</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;box_in_scene.png&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># trainImage</span>

<span class="c1"># Initiate SIFT detector</span>
<span class="n">orb</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">ORB</span><span class="p">()</span>

<span class="c1"># find the keypoints and descriptors with SIFT</span>
<span class="n">kp1</span><span class="p">,</span> <span class="n">des1</span> <span class="o">=</span> <span class="n">orb</span><span class="o">.</span><span class="n">detectAndCompute</span><span class="p">(</span><span class="n">img1</span><span class="p">,</span><span class="kc">None</span><span class="p">)</span>
<span class="n">kp2</span><span class="p">,</span> <span class="n">des2</span> <span class="o">=</span> <span class="n">orb</span><span class="o">.</span><span class="n">detectAndCompute</span><span class="p">(</span><span class="n">img2</span><span class="p">,</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
<p>Next we create a BFMatcher object with distance measurement <code class="docutils literal"><span class="pre">cv2.NORM_HAMMING</span></code> (since we are using ORB) and <code class="docutils literal"><span class="pre">crossCheck</span></code> is switched on for better results. Then we use Matcher.match() method to get the best matches in two images. We sort them in ascending order of their distances so that best matches (with low distance) come to front. Then we draw only first 10 matches (Just for sake of visibility. You can increase it as you like)</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="c1"># create BFMatcher object</span>
<span class="n">bf</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">BFMatcher</span><span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">NORM_HAMMING</span><span class="p">,</span> <span class="n">crossCheck</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Match descriptors.</span>
<span class="n">matches</span> <span class="o">=</span> <span class="n">bf</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">des1</span><span class="p">,</span><span class="n">des2</span><span class="p">)</span>

<span class="c1"># Sort them in the order of their distance.</span>
<span class="n">matches</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">matches</span><span class="p">,</span> <span class="n">key</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="n">x</span><span class="o">.</span><span class="n">distance</span><span class="p">)</span>

<span class="c1"># Draw first 10 matches.</span>
<span class="n">img3</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">drawMatches</span><span class="p">(</span><span class="n">img1</span><span class="p">,</span><span class="n">kp1</span><span class="p">,</span><span class="n">img2</span><span class="p">,</span><span class="n">kp2</span><span class="p">,</span><span class="n">matches</span><span class="p">[:</span><span class="mi">10</span><span class="p">],</span> <span class="n">flags</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img3</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Below is the result I got:</p>
<blockquote>
<div><img alt="ORB Feature Matching with Brute-Force" class="align-center" src="_images/matcher_result1.jpg" />
</div></blockquote>
</div>
<div class="section" id="what-is-this-matcher-object">
<h6>What is this Matcher Object?<a class="headerlink" href="#what-is-this-matcher-object" title="Permalink to this headline">¶</a></h6>
<p>The result of <code class="docutils literal"><span class="pre">matches</span> <span class="pre">=</span> <span class="pre">bf.match(des1,des2)</span></code> line is a list of DMatch objects. This DMatch object has following attributes:</p>
<blockquote>
<div><ul class="simple">
<li><code class="docutils literal"><span class="pre">DMatch.distance</span></code> - Distance between descriptors. The lower, the better it is.</li>
<li><code class="docutils literal"><span class="pre">DMatch.trainIdx</span></code> - Index of the descriptor in train descriptors</li>
<li><code class="docutils literal"><span class="pre">DMatch.queryIdx</span></code> - Index of the descriptor in query descriptors</li>
<li><code class="docutils literal"><span class="pre">DMatch.imgIdx</span></code>   - Index of the train image.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="brute-force-matching-with-sift-descriptors-and-ratio-test">
<h6>Brute-Force Matching with SIFT Descriptors and Ratio Test<a class="headerlink" href="#brute-force-matching-with-sift-descriptors-and-ratio-test" title="Permalink to this headline">¶</a></h6>
<p>This time, we will use <code class="docutils literal"><span class="pre">BFMatcher.knnMatch()</span></code> to get k best matches. In this example, we will take k=2 so that we can apply ratio test explained by D.Lowe in his paper.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">img1</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;box.png&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>          <span class="c1"># queryImage</span>
<span class="n">img2</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;box_in_scene.png&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># trainImage</span>

<span class="c1"># Initiate SIFT detector</span>
<span class="n">sift</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">SIFT</span><span class="p">()</span>

<span class="c1"># find the keypoints and descriptors with SIFT</span>
<span class="n">kp1</span><span class="p">,</span> <span class="n">des1</span> <span class="o">=</span> <span class="n">sift</span><span class="o">.</span><span class="n">detectAndCompute</span><span class="p">(</span><span class="n">img1</span><span class="p">,</span><span class="kc">None</span><span class="p">)</span>
<span class="n">kp2</span><span class="p">,</span> <span class="n">des2</span> <span class="o">=</span> <span class="n">sift</span><span class="o">.</span><span class="n">detectAndCompute</span><span class="p">(</span><span class="n">img2</span><span class="p">,</span><span class="kc">None</span><span class="p">)</span>

<span class="c1"># BFMatcher with default params</span>
<span class="n">bf</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">BFMatcher</span><span class="p">()</span>
<span class="n">matches</span> <span class="o">=</span> <span class="n">bf</span><span class="o">.</span><span class="n">knnMatch</span><span class="p">(</span><span class="n">des1</span><span class="p">,</span><span class="n">des2</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Apply ratio test</span>
<span class="n">good</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">m</span><span class="p">,</span><span class="n">n</span> <span class="ow">in</span> <span class="n">matches</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">distance</span> <span class="o">&lt;</span> <span class="mf">0.75</span><span class="o">*</span><span class="n">n</span><span class="o">.</span><span class="n">distance</span><span class="p">:</span>
        <span class="n">good</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">m</span><span class="p">])</span>

<span class="c1"># cv2.drawMatchesKnn expects list of lists as matches.</span>
<span class="n">img3</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">drawMatchesKnn</span><span class="p">(</span><span class="n">img1</span><span class="p">,</span><span class="n">kp1</span><span class="p">,</span><span class="n">img2</span><span class="p">,</span><span class="n">kp2</span><span class="p">,</span><span class="n">good</span><span class="p">,</span><span class="n">flags</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img3</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>See the result below:</p>
<blockquote>
<div><img alt="SIFT Descriptor with ratio test" class="align-center" src="_images/matcher_result2.jpg" />
</div></blockquote>
</div>
</div>
<div class="section" id="flann-based-matcher">
<h5>FLANN based Matcher<a class="headerlink" href="#flann-based-matcher" title="Permalink to this headline">¶</a></h5>
<p>FLANN stands for Fast Library for Approximate Nearest Neighbors. It contains a collection of algorithms optimized for fast nearest neighbor search in large datasets and for high dimensional features. It works more faster than BFMatcher for large datasets. We will see the second example with FLANN based matcher.</p>
<p>For FLANN based matcher, we need to pass two dictionaries which specifies the algorithm to be used, its related parameters etc. First one is IndexParams. For various algorithms, the information to be passed is explained in FLANN docs. As a summary, for algorithms like SIFT, SURF etc. you can pass following:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">index_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">algorithm</span> <span class="o">=</span> <span class="n">FLANN_INDEX_KDTREE</span><span class="p">,</span> <span class="n">trees</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
<p>While using ORB, you can pass the following. The commented values are recommended as per the docs, but it didn&#8217;t provide required results in some cases. Other values worked fine.:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">index_params</span><span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">algorithm</span> <span class="o">=</span> <span class="n">FLANN_INDEX_LSH</span><span class="p">,</span>
                   <span class="n">table_number</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span> <span class="c1"># 12</span>
                   <span class="n">key_size</span> <span class="o">=</span> <span class="mi">12</span><span class="p">,</span>     <span class="c1"># 20</span>
                   <span class="n">multi_probe_level</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="c1">#2</span>
</pre></div>
</div>
<p>Second dictionary is the SearchParams. It specifies the number of times the trees in the index should be recursively traversed. Higher values gives better precision, but also takes more time. If you want to change the value, pass <code class="docutils literal"><span class="pre">search_params</span> <span class="pre">=</span> <span class="pre">dict(checks=100)</span></code>.</p>
<p>With these informations, we are good to go.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">img1</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;box.png&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>          <span class="c1"># queryImage</span>
<span class="n">img2</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;box_in_scene.png&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># trainImage</span>

<span class="c1"># Initiate SIFT detector</span>
<span class="n">sift</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">SIFT</span><span class="p">()</span>

<span class="c1"># find the keypoints and descriptors with SIFT</span>
<span class="n">kp1</span><span class="p">,</span> <span class="n">des1</span> <span class="o">=</span> <span class="n">sift</span><span class="o">.</span><span class="n">detectAndCompute</span><span class="p">(</span><span class="n">img1</span><span class="p">,</span><span class="kc">None</span><span class="p">)</span>
<span class="n">kp2</span><span class="p">,</span> <span class="n">des2</span> <span class="o">=</span> <span class="n">sift</span><span class="o">.</span><span class="n">detectAndCompute</span><span class="p">(</span><span class="n">img2</span><span class="p">,</span><span class="kc">None</span><span class="p">)</span>

<span class="c1"># FLANN parameters</span>
<span class="n">FLANN_INDEX_KDTREE</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">index_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">algorithm</span> <span class="o">=</span> <span class="n">FLANN_INDEX_KDTREE</span><span class="p">,</span> <span class="n">trees</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">search_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">checks</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>   <span class="c1"># or pass empty dictionary</span>

<span class="n">flann</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">FlannBasedMatcher</span><span class="p">(</span><span class="n">index_params</span><span class="p">,</span><span class="n">search_params</span><span class="p">)</span>

<span class="n">matches</span> <span class="o">=</span> <span class="n">flann</span><span class="o">.</span><span class="n">knnMatch</span><span class="p">(</span><span class="n">des1</span><span class="p">,</span><span class="n">des2</span><span class="p">,</span><span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Need to draw only good matches, so create a mask</span>
<span class="n">matchesMask</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">matches</span><span class="p">))]</span>

<span class="c1"># ratio test as per Lowe&#39;s paper</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,(</span><span class="n">m</span><span class="p">,</span><span class="n">n</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">matches</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">distance</span> <span class="o">&lt;</span> <span class="mf">0.7</span><span class="o">*</span><span class="n">n</span><span class="o">.</span><span class="n">distance</span><span class="p">:</span>
        <span class="n">matchesMask</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>

<span class="n">draw_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">matchColor</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span>
                   <span class="n">singlePointColor</span> <span class="o">=</span> <span class="p">(</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span>
                   <span class="n">matchesMask</span> <span class="o">=</span> <span class="n">matchesMask</span><span class="p">,</span>
                   <span class="n">flags</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

<span class="n">img3</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">drawMatchesKnn</span><span class="p">(</span><span class="n">img1</span><span class="p">,</span><span class="n">kp1</span><span class="p">,</span><span class="n">img2</span><span class="p">,</span><span class="n">kp2</span><span class="p">,</span><span class="n">matches</span><span class="p">,</span><span class="kc">None</span><span class="p">,</span><span class="o">**</span><span class="n">draw_params</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img3</span><span class="p">,),</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>See the result below:</p>
<blockquote>
<div><img alt="FLANN based matching" class="align-center" src="_images/matcher_flann.jpg" />
</div></blockquote>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
</div>
</div>
<span id="document-py_tutorials/py_feature2d/py_feature_homography/py_feature_homography"></span><div class="section" id="feature-matching-homography-to-find-objects">
<span id="feature-homography"></span><h4>Feature Matching + Homography to find Objects<a class="headerlink" href="#feature-matching-homography-to-find-objects" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goal">
<h5>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h5>
<dl class="docutils">
<dt>In this chapter,</dt>
<dd><ul class="first last simple">
<li>We will mix up the feature matching and findHomography from calib3d module to find known objects in a complex image.</li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="basics">
<h5>Basics<a class="headerlink" href="#basics" title="Permalink to this headline">¶</a></h5>
<p>So what we did in last session? We used a queryImage, found some feature points in it, we took another trainImage, found the features in that image too and we found the best matches among them. In short, we found locations of some parts of an object in another cluttered image. This information is sufficient to find the object exactly on the trainImage.</p>
<p>For that, we can use a function from calib3d module, ie <strong>cv2.findHomography()</strong>. If we pass the set of points from both the images, it will find the perpective transformation of that object. Then we can use <strong>cv2.perspectiveTransform()</strong> to find the object. It needs atleast four correct points to find the transformation.</p>
<p>We have seen that there can be some possible errors while matching which may affect the result. To solve this problem, algorithm uses RANSAC or LEAST_MEDIAN (which can be decided by the flags). So good matches which provide correct estimation are called inliers and remaining are called outliers. <strong>cv2.findHomography()</strong> returns a mask which specifies the inlier and outlier points.</p>
<p>So let&#8217;s do it !!!</p>
</div>
<div class="section" id="code">
<h5>Code<a class="headerlink" href="#code" title="Permalink to this headline">¶</a></h5>
<p>First, as usual, let&#8217;s find SIFT features in images and apply the ratio test to find the best matches.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">MIN_MATCH_COUNT</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">img1</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;box.png&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>          <span class="c1"># queryImage</span>
<span class="n">img2</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;box_in_scene.png&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># trainImage</span>

<span class="c1"># Initiate SIFT detector</span>
<span class="n">sift</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">SIFT</span><span class="p">()</span>

<span class="c1"># find the keypoints and descriptors with SIFT</span>
<span class="n">kp1</span><span class="p">,</span> <span class="n">des1</span> <span class="o">=</span> <span class="n">sift</span><span class="o">.</span><span class="n">detectAndCompute</span><span class="p">(</span><span class="n">img1</span><span class="p">,</span><span class="kc">None</span><span class="p">)</span>
<span class="n">kp2</span><span class="p">,</span> <span class="n">des2</span> <span class="o">=</span> <span class="n">sift</span><span class="o">.</span><span class="n">detectAndCompute</span><span class="p">(</span><span class="n">img2</span><span class="p">,</span><span class="kc">None</span><span class="p">)</span>

<span class="n">FLANN_INDEX_KDTREE</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">index_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">algorithm</span> <span class="o">=</span> <span class="n">FLANN_INDEX_KDTREE</span><span class="p">,</span> <span class="n">trees</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">search_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">checks</span> <span class="o">=</span> <span class="mi">50</span><span class="p">)</span>

<span class="n">flann</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">FlannBasedMatcher</span><span class="p">(</span><span class="n">index_params</span><span class="p">,</span> <span class="n">search_params</span><span class="p">)</span>

<span class="n">matches</span> <span class="o">=</span> <span class="n">flann</span><span class="o">.</span><span class="n">knnMatch</span><span class="p">(</span><span class="n">des1</span><span class="p">,</span><span class="n">des2</span><span class="p">,</span><span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># store all the good matches as per Lowe&#39;s ratio test.</span>
<span class="n">good</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">m</span><span class="p">,</span><span class="n">n</span> <span class="ow">in</span> <span class="n">matches</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">distance</span> <span class="o">&lt;</span> <span class="mf">0.7</span><span class="o">*</span><span class="n">n</span><span class="o">.</span><span class="n">distance</span><span class="p">:</span>
        <span class="n">good</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
</pre></div>
</div>
<p>Now we set a condition that atleast 10 matches (defined by MIN_MATCH_COUNT) are to be there to find the object. Otherwise simply show a message saying not enough matches are present.</p>
<p>If enough matches are found, we extract the locations of matched keypoints in both the images. They are passed to find the perpective transformation. Once we get this 3x3 transformation matrix, we use it to transform the corners of queryImage to corresponding points in trainImage. Then we draw it.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">good</span><span class="p">)</span><span class="o">&gt;</span><span class="n">MIN_MATCH_COUNT</span><span class="p">:</span>
    <span class="n">src_pts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">([</span> <span class="n">kp1</span><span class="p">[</span><span class="n">m</span><span class="o">.</span><span class="n">queryIdx</span><span class="p">]</span><span class="o">.</span><span class="n">pt</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">good</span> <span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">dst_pts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">([</span> <span class="n">kp2</span><span class="p">[</span><span class="n">m</span><span class="o">.</span><span class="n">trainIdx</span><span class="p">]</span><span class="o">.</span><span class="n">pt</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">good</span> <span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">M</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">findHomography</span><span class="p">(</span><span class="n">src_pts</span><span class="p">,</span> <span class="n">dst_pts</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">RANSAC</span><span class="p">,</span><span class="mf">5.0</span><span class="p">)</span>
    <span class="n">matchesMask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

    <span class="n">h</span><span class="p">,</span><span class="n">w</span> <span class="o">=</span> <span class="n">img1</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">pts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">([</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="n">h</span><span class="o">-</span><span class="mi">1</span><span class="p">],[</span><span class="n">w</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">h</span><span class="o">-</span><span class="mi">1</span><span class="p">],[</span><span class="n">w</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">dst</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">perspectiveTransform</span><span class="p">(</span><span class="n">pts</span><span class="p">,</span><span class="n">M</span><span class="p">)</span>

    <span class="n">img2</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">polylines</span><span class="p">(</span><span class="n">img2</span><span class="p">,[</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">(</span><span class="n">dst</span><span class="p">)],</span><span class="kc">True</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">LINE_AA</span><span class="p">)</span>

<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span> <span class="s2">&quot;Not enough matches are found - </span><span class="si">%d</span><span class="s2">/</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">good</span><span class="p">),</span><span class="n">MIN_MATCH_COUNT</span><span class="p">)</span>
    <span class="n">matchesMask</span> <span class="o">=</span> <span class="kc">None</span>
</pre></div>
</div>
<p>Finally we draw our inliers (if successfully found the object) or matching keypoints (if failed).</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">draw_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">matchColor</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="c1"># draw matches in green color</span>
                   <span class="n">singlePointColor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                   <span class="n">matchesMask</span> <span class="o">=</span> <span class="n">matchesMask</span><span class="p">,</span> <span class="c1"># draw only inliers</span>
                   <span class="n">flags</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">img3</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">drawMatches</span><span class="p">(</span><span class="n">img1</span><span class="p">,</span><span class="n">kp1</span><span class="p">,</span><span class="n">img2</span><span class="p">,</span><span class="n">kp2</span><span class="p">,</span><span class="n">good</span><span class="p">,</span><span class="kc">None</span><span class="p">,</span><span class="o">**</span><span class="n">draw_params</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img3</span><span class="p">,</span> <span class="s1">&#39;gray&#39;</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>See the result below. Object is marked in white color in cluttered image:</p>
<blockquote>
<div><img alt="Finding object with feature homography" class="align-center" src="_images/homography_findobj.jpg" />
</div></blockquote>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
</div>
</div>
</div>
</div>
<span id="document-py_tutorials/py_video/py_table_of_contents_video/py_table_of_contents_video"></span><div class="section" id="video-analysis">
<span id="py-table-of-content-video"></span><h3>Video Analysis<a class="headerlink" href="#video-analysis" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference internal" href="index.html#meanshift"><span class="std std-ref">Meanshift and Camshift</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="5%" />
<col width="95%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/camshift.jpg"><img alt="vdo_1" src="_images/camshift.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>We have already seen an example of color-based tracking. It is simpler. This time, we see much more better algorithms like &#8220;Meanshift&#8221;, and its upgraded version, &#8220;Camshift&#8221; to find and track them.</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#lucas-kanade"><span class="std std-ref">Optical Flow</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="9%" />
<col width="91%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/opticalflow.jpeg"><img alt="vdo_2" src="_images/opticalflow.jpeg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Now let&#8217;s discuss an important concept, &#8220;Optical Flow&#8221;, which is related to videos and has many applications.</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#background-subtraction"><span class="std std-ref">Background Subtraction</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="6%" />
<col width="94%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/background.jpg"><img alt="vdo_b" src="_images/background.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>In several applications, we need to extract foreground for further operations like object tracking. Background Subtraction is a well-known method in those cases.</td>
</tr>
</tbody>
</table>
</div></blockquote>
<div class="toctree-wrapper compound">
<span id="document-py_tutorials/py_video/py_meanshift/py_meanshift"></span><div class="section" id="meanshift-and-camshift">
<span id="meanshift"></span><h4>Meanshift and Camshift<a class="headerlink" href="#meanshift-and-camshift" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goal">
<h5>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h5>
<p>In this chapter,</p>
<blockquote>
<div><ul class="simple">
<li>We will learn about Meanshift and Camshift algorithms to find and track objects in videos.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="id1">
<h5>Meanshift<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h5>
<p>The intuition behind the meanshift is simple. Consider you have a set of points. (It can be a pixel distribution like histogram backprojection). You are given a small window ( may be a circle) and you have to move that window to the area of maximum pixel density (or maximum number of points). It is illustrated in the simple image given below:</p>
<blockquote>
<div><img alt="Intuition behind meanshift" class="align-center" src="_images/meanshift_basics.jpg" />
</div></blockquote>
<p>The initial window is shown in blue circle with the name &#8220;C1&#8221;. Its original center is marked in blue rectangle, named &#8220;C1_o&#8221;. But if you find the centroid of the points inside that window, you will get the point &#8220;C1_r&#8221; (marked in small blue circle) which is the real centroid of window. Surely they don&#8217;t match. So move your window such that circle of the new window matches with previous centroid. Again find the new centroid. Most probably, it won&#8217;t match. So move it again, and continue the iterations such that center of window and its centroid falls on the same location (or with a small desired error). So finally what you obtain is a window with maximum pixel distribution. It is marked with green circle, named &#8220;C2&#8221;. As you can see in image, it has maximum number of points. The whole process is demonstrated on a static image below:</p>
<blockquote>
<div><img alt="Meanshift on static image" class="align-center" src="_images/meanshift_face.gif" />
</div></blockquote>
<p>So we normally pass the histogram backprojected image and initial target location. When the object moves, obviously the movement is reflected in histogram backprojected image. As a result, meanshift algorithm moves our window to the new location with maximum density.</p>
<div class="section" id="meanshift-in-opencv">
<h6>Meanshift in OpenCV<a class="headerlink" href="#meanshift-in-opencv" title="Permalink to this headline">¶</a></h6>
<p>To use meanshift in OpenCV, first we need to setup the target, find its histogram so that we can backproject the target on each frame for calculation of meanshift. We also need to provide initial location of window. For histogram, only Hue is considered here. Also, to avoid false values due to low light, low light values are discarded using <strong>cv2.inRange()</strong> function.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cv2</span>

<span class="n">cap</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">VideoCapture</span><span class="p">(</span><span class="s1">&#39;slow.flv&#39;</span><span class="p">)</span>

<span class="c1"># take first frame of the video</span>
<span class="n">ret</span><span class="p">,</span><span class="n">frame</span> <span class="o">=</span> <span class="n">cap</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>

<span class="c1"># setup initial location of window</span>
<span class="n">r</span><span class="p">,</span><span class="n">h</span><span class="p">,</span><span class="n">c</span><span class="p">,</span><span class="n">w</span> <span class="o">=</span> <span class="mi">250</span><span class="p">,</span><span class="mi">90</span><span class="p">,</span><span class="mi">400</span><span class="p">,</span><span class="mi">125</span>  <span class="c1"># simply hardcoded the values</span>
<span class="n">track_window</span> <span class="o">=</span> <span class="p">(</span><span class="n">c</span><span class="p">,</span><span class="n">r</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="n">h</span><span class="p">)</span>

<span class="c1"># set up the ROI for tracking</span>
<span class="n">roi</span> <span class="o">=</span> <span class="n">frame</span><span class="p">[</span><span class="n">r</span><span class="p">:</span><span class="n">r</span><span class="o">+</span><span class="n">h</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span><span class="n">c</span><span class="o">+</span><span class="n">w</span><span class="p">]</span>
<span class="n">hsv_roi</span> <span class="o">=</span>  <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2HSV</span><span class="p">)</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">inRange</span><span class="p">(</span><span class="n">hsv_roi</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">((</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">60.</span><span class="p">,</span><span class="mf">32.</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">((</span><span class="mf">180.</span><span class="p">,</span><span class="mf">255.</span><span class="p">,</span><span class="mf">255.</span><span class="p">)))</span>
<span class="n">roi_hist</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">calcHist</span><span class="p">([</span><span class="n">hsv_roi</span><span class="p">],[</span><span class="mi">0</span><span class="p">],</span><span class="n">mask</span><span class="p">,[</span><span class="mi">180</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">180</span><span class="p">])</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">roi_hist</span><span class="p">,</span><span class="n">roi_hist</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">NORM_MINMAX</span><span class="p">)</span>

<span class="c1"># Setup the termination criteria, either 10 iteration or move by atleast 1 pt</span>
<span class="n">term_crit</span> <span class="o">=</span> <span class="p">(</span> <span class="n">cv2</span><span class="o">.</span><span class="n">TERM_CRITERIA_EPS</span> <span class="o">|</span> <span class="n">cv2</span><span class="o">.</span><span class="n">TERM_CRITERIA_COUNT</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1</span> <span class="p">)</span>

<span class="k">while</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">ret</span> <span class="p">,</span><span class="n">frame</span> <span class="o">=</span> <span class="n">cap</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">ret</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">hsv</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2HSV</span><span class="p">)</span>
        <span class="n">dst</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">calcBackProject</span><span class="p">([</span><span class="n">hsv</span><span class="p">],[</span><span class="mi">0</span><span class="p">],</span><span class="n">roi_hist</span><span class="p">,[</span><span class="mi">0</span><span class="p">,</span><span class="mi">180</span><span class="p">],</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># apply meanshift to get the new location</span>
        <span class="n">ret</span><span class="p">,</span> <span class="n">track_window</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">meanShift</span><span class="p">(</span><span class="n">dst</span><span class="p">,</span> <span class="n">track_window</span><span class="p">,</span> <span class="n">term_crit</span><span class="p">)</span>

        <span class="c1"># Draw it on image</span>
        <span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="n">h</span> <span class="o">=</span> <span class="n">track_window</span>
        <span class="n">img2</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">rectangle</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">),</span> <span class="p">(</span><span class="n">x</span><span class="o">+</span><span class="n">w</span><span class="p">,</span><span class="n">y</span><span class="o">+</span><span class="n">h</span><span class="p">),</span> <span class="mi">255</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">cv2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s1">&#39;img2&#39;</span><span class="p">,</span><span class="n">img2</span><span class="p">)</span>

        <span class="n">k</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">60</span><span class="p">)</span> <span class="o">&amp;</span> <span class="mh">0xff</span>
        <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">27</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">cv2</span><span class="o">.</span><span class="n">imwrite</span><span class="p">(</span><span class="nb">chr</span><span class="p">(</span><span class="n">k</span><span class="p">)</span><span class="o">+</span><span class="s2">&quot;.jpg&quot;</span><span class="p">,</span><span class="n">img2</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="k">break</span>

<span class="n">cv2</span><span class="o">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>
<span class="n">cap</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>
</pre></div>
</div>
<p>Three frames in a video I used is given below:</p>
<blockquote>
<div><img alt="Meanshift result" class="align-center" src="_images/meanshift_result.jpg" />
</div></blockquote>
</div>
</div>
<div class="section" id="camshift">
<h5>Camshift<a class="headerlink" href="#camshift" title="Permalink to this headline">¶</a></h5>
<p>Did you closely watch the last result? There is a problem. Our window always has the same size when car is farther away and it is very close to camera. That is not good. We need to adapt the window size with size and rotation of the target. Once again, the solution came from &#8220;OpenCV Labs&#8221; and it is called CAMshift (Continuously Adaptive Meanshift) published by Gary Bradsky in his paper &#8220;Computer Vision Face Tracking for Use in a Perceptual User Interface&#8221; in 1988.</p>
<p>It applies meanshift first. Once meanshift converges, it updates the size of the window as, <img class="math" src="_images/math/0fbb56c526714253028e7134678e25237b2550f7.png" alt="s = 2 \times \sqrt{\frac{M_{00}}{256}}"/>. It also calculates the orientation of best fitting ellipse to it. Again it applies the meanshift with new scaled search window and previous window location. The process is continued until required accuracy is met.</p>
<blockquote>
<div><img alt="Meanshift on static image" class="align-center" src="_images/camshift_face.gif" />
</div></blockquote>
<div class="section" id="camshift-in-opencv">
<h6>Camshift in OpenCV<a class="headerlink" href="#camshift-in-opencv" title="Permalink to this headline">¶</a></h6>
<p>It is almost same as meanshift, but it returns a rotated rectangle (that is our result) and box parameters (used to be passed as search window in next iteration). See the code below:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cv2</span>

<span class="n">cap</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">VideoCapture</span><span class="p">(</span><span class="s1">&#39;slow.flv&#39;</span><span class="p">)</span>

<span class="c1"># take first frame of the video</span>
<span class="n">ret</span><span class="p">,</span><span class="n">frame</span> <span class="o">=</span> <span class="n">cap</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>

<span class="c1"># setup initial location of window</span>
<span class="n">r</span><span class="p">,</span><span class="n">h</span><span class="p">,</span><span class="n">c</span><span class="p">,</span><span class="n">w</span> <span class="o">=</span> <span class="mi">250</span><span class="p">,</span><span class="mi">90</span><span class="p">,</span><span class="mi">400</span><span class="p">,</span><span class="mi">125</span>  <span class="c1"># simply hardcoded the values</span>
<span class="n">track_window</span> <span class="o">=</span> <span class="p">(</span><span class="n">c</span><span class="p">,</span><span class="n">r</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="n">h</span><span class="p">)</span>

<span class="c1"># set up the ROI for tracking</span>
<span class="n">roi</span> <span class="o">=</span> <span class="n">frame</span><span class="p">[</span><span class="n">r</span><span class="p">:</span><span class="n">r</span><span class="o">+</span><span class="n">h</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span><span class="n">c</span><span class="o">+</span><span class="n">w</span><span class="p">]</span>
<span class="n">hsv_roi</span> <span class="o">=</span>  <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2HSV</span><span class="p">)</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">inRange</span><span class="p">(</span><span class="n">hsv_roi</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">((</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">60.</span><span class="p">,</span><span class="mf">32.</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">((</span><span class="mf">180.</span><span class="p">,</span><span class="mf">255.</span><span class="p">,</span><span class="mf">255.</span><span class="p">)))</span>
<span class="n">roi_hist</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">calcHist</span><span class="p">([</span><span class="n">hsv_roi</span><span class="p">],[</span><span class="mi">0</span><span class="p">],</span><span class="n">mask</span><span class="p">,[</span><span class="mi">180</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">180</span><span class="p">])</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">roi_hist</span><span class="p">,</span><span class="n">roi_hist</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">NORM_MINMAX</span><span class="p">)</span>

<span class="c1"># Setup the termination criteria, either 10 iteration or move by atleast 1 pt</span>
<span class="n">term_crit</span> <span class="o">=</span> <span class="p">(</span> <span class="n">cv2</span><span class="o">.</span><span class="n">TERM_CRITERIA_EPS</span> <span class="o">|</span> <span class="n">cv2</span><span class="o">.</span><span class="n">TERM_CRITERIA_COUNT</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1</span> <span class="p">)</span>

<span class="k">while</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">ret</span> <span class="p">,</span><span class="n">frame</span> <span class="o">=</span> <span class="n">cap</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">ret</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">hsv</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2HSV</span><span class="p">)</span>
        <span class="n">dst</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">calcBackProject</span><span class="p">([</span><span class="n">hsv</span><span class="p">],[</span><span class="mi">0</span><span class="p">],</span><span class="n">roi_hist</span><span class="p">,[</span><span class="mi">0</span><span class="p">,</span><span class="mi">180</span><span class="p">],</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># apply meanshift to get the new location</span>
        <span class="n">ret</span><span class="p">,</span> <span class="n">track_window</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">CamShift</span><span class="p">(</span><span class="n">dst</span><span class="p">,</span> <span class="n">track_window</span><span class="p">,</span> <span class="n">term_crit</span><span class="p">)</span>

        <span class="c1"># Draw it on image</span>
        <span class="n">pts</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">boxPoints</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span>
        <span class="n">pts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">int0</span><span class="p">(</span><span class="n">pts</span><span class="p">)</span>
        <span class="n">img2</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">polylines</span><span class="p">(</span><span class="n">frame</span><span class="p">,[</span><span class="n">pts</span><span class="p">],</span><span class="kc">True</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">cv2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s1">&#39;img2&#39;</span><span class="p">,</span><span class="n">img2</span><span class="p">)</span>

        <span class="n">k</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">60</span><span class="p">)</span> <span class="o">&amp;</span> <span class="mh">0xff</span>
        <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">27</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">cv2</span><span class="o">.</span><span class="n">imwrite</span><span class="p">(</span><span class="nb">chr</span><span class="p">(</span><span class="n">k</span><span class="p">)</span><span class="o">+</span><span class="s2">&quot;.jpg&quot;</span><span class="p">,</span><span class="n">img2</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="k">break</span>

<span class="n">cv2</span><span class="o">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>
<span class="n">cap</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>
</pre></div>
</div>
<p>Three frames of the result is shown below:</p>
<blockquote>
<div><img alt="Camshift result" class="align-center" src="_images/camshift_result.jpg" />
</div></blockquote>
</div>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
<ol class="arabic simple">
<li>French Wikipedia page on <a class="reference external" href="http://fr.wikipedia.org/wiki/Camshift">Camshift</a>. (The two animations are taken from here)</li>
<li>Bradski, G.R., &#8220;Real time face and object tracking as a component of a perceptual user interface,&#8221; Applications of Computer Vision, 1998. WACV &#8216;98. Proceedings., Fourth IEEE Workshop on , vol., no., pp.214,219, 19-21 Oct 1998</li>
</ol>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
<ol class="arabic simple">
<li>OpenCV comes with a Python sample on interactive demo of camshift. Use it, hack it, understand it.</li>
</ol>
</div>
</div>
<span id="document-py_tutorials/py_video/py_lucas_kanade/py_lucas_kanade"></span><div class="section" id="optical-flow">
<span id="lucas-kanade"></span><h4>Optical Flow<a class="headerlink" href="#optical-flow" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goal">
<h5>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h5>
<dl class="docutils">
<dt>In this chapter,</dt>
<dd><ul class="first last simple">
<li>We will understand the concepts of optical flow and its estimation using Lucas-Kanade method.</li>
<li>We will use functions like <strong>cv2.calcOpticalFlowPyrLK()</strong> to track feature points in a video.</li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="id1">
<h5>Optical Flow<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h5>
<p>Optical flow is the pattern of apparent motion of image objects between two consecutive frames caused by the movemement of object or camera. It is 2D vector field where each vector is a displacement vector showing the movement of points from first frame to second. Consider the image below (Image Courtesy: <a class="reference external" href="http://en.wikipedia.org/wiki/Optical_flow">Wikipedia article on Optical Flow</a>).</p>
<blockquote>
<div><img alt="Optical Flow" class="align-center" src="_images/optical_flow_basic1.jpg" />
</div></blockquote>
<p>It shows a ball moving in 5 consecutive frames. The arrow shows its displacement vector. Optical flow has many applications in areas like :</p>
<blockquote>
<div><ul class="simple">
<li>Structure from Motion</li>
<li>Video Compression</li>
<li>Video Stabilization ...</li>
</ul>
</div></blockquote>
<p>Optical flow works on several assumptions:</p>
<ol class="arabic simple">
<li>The pixel intensities of an object do not change between consecutive frames.</li>
<li>Neighbouring pixels have similar motion.</li>
</ol>
<p>Consider a pixel <img class="math" src="_images/math/3ae7383ac625185fe7a9ad24a0809e24bb29f9d1.png" alt="I(x,y,t)"/> in first frame (Check a new dimension, time, is added here. Earlier we were working with images only, so no need of time). It moves by distance <img class="math" src="_images/math/51a8c6889fb27a3c66f190b0d68352879fc61e30.png" alt="(dx,dy)"/> in next frame taken after <img class="math" src="_images/math/e35a5a8243aee90500ee06e96f4d04caa7b3fc79.png" alt="dt"/> time. So since those pixels are the same and intensity does not change, we can say,</p>
<div class="math">
<p><img src="_images/math/43e877f3529e83cce1616c9b24304b8902198624.png" alt="I(x,y,t) = I(x+dx, y+dy, t+dt)"/></p>
</div><p>Then take taylor series approximation of right-hand side, remove common terms and divide by <img class="math" src="_images/math/e35a5a8243aee90500ee06e96f4d04caa7b3fc79.png" alt="dt"/> to get the following equation:</p>
<div class="math">
<p><img src="_images/math/e885c66e31fec403a4b4221aac79b278414a780a.png" alt="f_x u + f_y v + f_t = 0 \;"/></p>
</div><p>where:</p>
<div class="math">
<p><img src="_images/math/c8b31bac0a4f265066e3cd733e3e498a558a5fde.png" alt="f_x = \frac{\partial f}{\partial x} \; ; \; f_y = \frac{\partial f}{\partial x}

u = \frac{dx}{dt} \; ; \; v = \frac{dy}{dt}"/></p>
</div><p>Above equation is called Optical Flow equation. In it, we can find <img class="math" src="_images/math/ecd7ad5ef3169e077aba97a2c40a7dbdafdcea35.png" alt="f_x"/> and <img class="math" src="_images/math/591beba71a9df4860dd00e83ba8b2a1cba89bee8.png" alt="f_y"/>, they are image gradients. Similarly <img class="math" src="_images/math/20d1c1977235793ab400eba6b6e71fb8b0020df3.png" alt="f_t"/> is the gradient along time. But <img class="math" src="_images/math/2d6bf630586c698ccb735d756e4a4f0674d5ff68.png" alt="(u,v)"/> is unknown. We cannot solve this one equation with two unknown variables. So several methods are provided to solve this problem and one of them is Lucas-Kanade.</p>
<div class="section" id="lucas-kanade-method">
<h6>Lucas-Kanade method<a class="headerlink" href="#lucas-kanade-method" title="Permalink to this headline">¶</a></h6>
<p>We have seen an assumption before, that all the neighbouring pixels will have similar motion. Lucas-Kanade method takes a 3x3 patch around the point. So all the 9 points have the same motion. We can find <img class="math" src="_images/math/491b3be96f4a20bae94b120bf3339548d6afa501.png" alt="(f_x, f_y, f_t)"/> for these 9 points. So now our problem becomes solving 9 equations with two unknown variables which is over-determined. A better solution is obtained with least square fit method. Below is the final solution which is two equation-two unknown problem and solve to get the solution.</p>
<div class="math">
<p><img src="_images/math/d7205af6381c68716c76d4bd919bf81428f5a760.png" alt="\begin{bmatrix} u \\ v \end{bmatrix} =
\begin{bmatrix}
    \sum_{i}{f_{x_i}}^2  &amp;  \sum_{i}{f_{x_i} f_{y_i} } \\
    \sum_{i}{f_{x_i} f_{y_i}} &amp; \sum_{i}{f_{y_i}}^2
\end{bmatrix}^{-1}
\begin{bmatrix}
    - \sum_{i}{f_{x_i} f_{t_i}} \\
    - \sum_{i}{f_{y_i} f_{t_i}}
\end{bmatrix}"/></p>
</div><p>( Check similarity of inverse matrix with Harris corner detector. It denotes that corners are better points to be tracked.)</p>
<p>So from user point of view, idea is simple, we give some points to track, we receive the optical flow vectors of those points. But again there are some problems. Until now, we were dealing with small motions. So it fails when there is large motion. So again we go for pyramids. When we go up in the pyramid, small motions are removed and large motions becomes small motions. So applying Lucas-Kanade there, we get optical flow along with the scale.</p>
</div>
</div>
<div class="section" id="lucas-kanade-optical-flow-in-opencv">
<h5>Lucas-Kanade Optical Flow in OpenCV<a class="headerlink" href="#lucas-kanade-optical-flow-in-opencv" title="Permalink to this headline">¶</a></h5>
<p>OpenCV provides all these in a single function, <strong>cv2.calcOpticalFlowPyrLK()</strong>. Here, we create a simple application which tracks some points in a video. To decide the points, we use <strong>cv2.goodFeaturesToTrack()</strong>. We take the first frame, detect some Shi-Tomasi corner points in it, then we iteratively track those points using Lucas-Kanade optical flow. For the function <strong>cv2.calcOpticalFlowPyrLK()</strong> we pass the previous frame, previous points and next frame. It returns next points along with some status numbers which has a value of 1 if next point is found, else zero. We  iteratively pass these next points as previous points in next step. See the code below:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cv2</span>

<span class="n">cap</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">VideoCapture</span><span class="p">(</span><span class="s1">&#39;slow.flv&#39;</span><span class="p">)</span>

<span class="c1"># params for ShiTomasi corner detection</span>
<span class="n">feature_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span> <span class="n">maxCorners</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
                       <span class="n">qualityLevel</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span>
                       <span class="n">minDistance</span> <span class="o">=</span> <span class="mi">7</span><span class="p">,</span>
                       <span class="n">blockSize</span> <span class="o">=</span> <span class="mi">7</span> <span class="p">)</span>

<span class="c1"># Parameters for lucas kanade optical flow</span>
<span class="n">lk_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span> <span class="n">winSize</span>  <span class="o">=</span> <span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">15</span><span class="p">),</span>
                  <span class="n">maxLevel</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
                  <span class="n">criteria</span> <span class="o">=</span> <span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">TERM_CRITERIA_EPS</span> <span class="o">|</span> <span class="n">cv2</span><span class="o">.</span><span class="n">TERM_CRITERIA_COUNT</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">0.03</span><span class="p">))</span>

<span class="c1"># Create some random colors</span>
<span class="n">color</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,(</span><span class="mi">100</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>

<span class="c1"># Take first frame and find corners in it</span>
<span class="n">ret</span><span class="p">,</span> <span class="n">old_frame</span> <span class="o">=</span> <span class="n">cap</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
<span class="n">old_gray</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">old_frame</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2GRAY</span><span class="p">)</span>
<span class="n">p0</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">goodFeaturesToTrack</span><span class="p">(</span><span class="n">old_gray</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">feature_params</span><span class="p">)</span>

<span class="c1"># Create a mask image for drawing purposes</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">old_frame</span><span class="p">)</span>

<span class="k">while</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">ret</span><span class="p">,</span><span class="n">frame</span> <span class="o">=</span> <span class="n">cap</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
    <span class="n">frame_gray</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2GRAY</span><span class="p">)</span>

    <span class="c1"># calculate optical flow</span>
    <span class="n">p1</span><span class="p">,</span> <span class="n">st</span><span class="p">,</span> <span class="n">err</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">calcOpticalFlowPyrLK</span><span class="p">(</span><span class="n">old_gray</span><span class="p">,</span> <span class="n">frame_gray</span><span class="p">,</span> <span class="n">p0</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">lk_params</span><span class="p">)</span>

    <span class="c1"># Select good points</span>
    <span class="n">good_new</span> <span class="o">=</span> <span class="n">p1</span><span class="p">[</span><span class="n">st</span><span class="o">==</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">good_old</span> <span class="o">=</span> <span class="n">p0</span><span class="p">[</span><span class="n">st</span><span class="o">==</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># draw the tracks</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,(</span><span class="n">new</span><span class="p">,</span><span class="n">old</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">good_new</span><span class="p">,</span><span class="n">good_old</span><span class="p">)):</span>
        <span class="n">a</span><span class="p">,</span><span class="n">b</span> <span class="o">=</span> <span class="n">new</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="n">c</span><span class="p">,</span><span class="n">d</span> <span class="o">=</span> <span class="n">old</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">line</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">),(</span><span class="n">c</span><span class="p">,</span><span class="n">d</span><span class="p">),</span> <span class="n">color</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">frame</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">circle</span><span class="p">(</span><span class="n">frame</span><span class="p">,(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">),</span><span class="mi">5</span><span class="p">,</span><span class="n">color</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span><span class="n">mask</span><span class="p">)</span>

    <span class="n">cv2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s1">&#39;frame&#39;</span><span class="p">,</span><span class="n">img</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">30</span><span class="p">)</span> <span class="o">&amp;</span> <span class="mh">0xff</span>
    <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">27</span><span class="p">:</span>
        <span class="k">break</span>

    <span class="c1"># Now update the previous frame and previous points</span>
    <span class="n">old_gray</span> <span class="o">=</span> <span class="n">frame_gray</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">p0</span> <span class="o">=</span> <span class="n">good_new</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>

<span class="n">cv2</span><span class="o">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>
<span class="n">cap</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>
</pre></div>
</div>
<p>(This code doesn&#8217;t check how correct are the next keypoints. So even if any feature point disappears in image, there is a chance that optical flow finds the next point which may look close to it. So actually for a robust tracking, corner points should be detected in particular intervals. OpenCV samples comes up with such a sample which finds the feature points at every 5 frames. It also run a backward-check of the optical flow points got to select only good ones. Check <code class="docutils literal"><span class="pre">samples/python2/lk_track.py</span></code>).</p>
<p>See the results we got:</p>
<blockquote>
<div><img alt="Lucas-Kanade method for optical flow" class="align-center" src="_images/opticalflow_lk.jpg" />
</div></blockquote>
</div>
<div class="section" id="dense-optical-flow-in-opencv">
<h5>Dense Optical Flow in OpenCV<a class="headerlink" href="#dense-optical-flow-in-opencv" title="Permalink to this headline">¶</a></h5>
<p>Lucas-Kanade method computes optical flow for a sparse feature set (in our example, corners detected using Shi-Tomasi algorithm). OpenCV provides another algorithm to find the dense optical flow. It computes the optical flow for all the points in the frame. It is based on Gunner Farneback&#8217;s algorithm which is explained in &#8220;Two-Frame Motion Estimation Based on Polynomial Expansion&#8221; by Gunner Farneback in 2003.</p>
<p>Below sample shows how to find the dense optical flow using above algorithm. We get a 2-channel array with optical flow vectors, <img class="math" src="_images/math/2d6bf630586c698ccb735d756e4a4f0674d5ff68.png" alt="(u,v)"/>. We find their magnitude and direction. We color code the result for better visualization. Direction corresponds to Hue value of the image. Magnitude corresponds to Value plane. See the code below:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">cap</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">VideoCapture</span><span class="p">(</span><span class="s2">&quot;vtest.avi&quot;</span><span class="p">)</span>

<span class="n">ret</span><span class="p">,</span> <span class="n">frame1</span> <span class="o">=</span> <span class="n">cap</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
<span class="n">prvs</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">frame1</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2GRAY</span><span class="p">)</span>
<span class="n">hsv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">frame1</span><span class="p">)</span>
<span class="n">hsv</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">255</span>

<span class="k">while</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">ret</span><span class="p">,</span> <span class="n">frame2</span> <span class="o">=</span> <span class="n">cap</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
    <span class="nb">next</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">frame2</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2GRAY</span><span class="p">)</span>

    <span class="n">flow</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">calcOpticalFlowFarneback</span><span class="p">(</span><span class="n">prvs</span><span class="p">,</span><span class="nb">next</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="n">mag</span><span class="p">,</span> <span class="n">ang</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cartToPolar</span><span class="p">(</span><span class="n">flow</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">flow</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">hsv</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">ang</span><span class="o">*</span><span class="mi">180</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">2</span>
    <span class="n">hsv</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">mag</span><span class="p">,</span><span class="kc">None</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">NORM_MINMAX</span><span class="p">)</span>
    <span class="n">rgb</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">hsv</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_HSV2BGR</span><span class="p">)</span>

    <span class="n">cv2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s1">&#39;frame2&#39;</span><span class="p">,</span><span class="n">rgb</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">30</span><span class="p">)</span> <span class="o">&amp;</span> <span class="mh">0xff</span>
    <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">27</span><span class="p">:</span>
        <span class="k">break</span>
    <span class="k">elif</span> <span class="n">k</span> <span class="o">==</span> <span class="nb">ord</span><span class="p">(</span><span class="s1">&#39;s&#39;</span><span class="p">):</span>
        <span class="n">cv2</span><span class="o">.</span><span class="n">imwrite</span><span class="p">(</span><span class="s1">&#39;opticalfb.png&#39;</span><span class="p">,</span><span class="n">frame2</span><span class="p">)</span>
        <span class="n">cv2</span><span class="o">.</span><span class="n">imwrite</span><span class="p">(</span><span class="s1">&#39;opticalhsv.png&#39;</span><span class="p">,</span><span class="n">rgb</span><span class="p">)</span>
    <span class="n">prvs</span> <span class="o">=</span> <span class="nb">next</span>

<span class="n">cap</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>
</pre></div>
</div>
<p>See the result below:</p>
<blockquote>
<div><img alt="Dense Optical Flow" class="align-center" src="_images/opticalfb.jpg" />
</div></blockquote>
<p>OpenCV comes with a more advanced sample on dense optical flow, please see <code class="docutils literal"><span class="pre">samples/python2/opt_flow.py</span></code>.</p>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
<ol class="arabic simple">
<li>Check the code in <code class="docutils literal"><span class="pre">samples/python2/lk_track.py</span></code>. Try to understand the code.</li>
<li>Check the code in <code class="docutils literal"><span class="pre">samples/python2/opt_flow.py</span></code>. Try to understand the code.</li>
</ol>
</div>
</div>
<span id="document-py_tutorials/py_video/py_bg_subtraction/py_bg_subtraction"></span><div class="section" id="background-subtraction">
<span id="id1"></span><h4>Background Subtraction<a class="headerlink" href="#background-subtraction" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goal">
<h5>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h5>
<p>In this chapter,</p>
<blockquote>
<div><ul class="simple">
<li>We will familiarize with the background subtraction methods available in OpenCV.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="basics">
<h5>Basics<a class="headerlink" href="#basics" title="Permalink to this headline">¶</a></h5>
<p>Background subtraction is a major preprocessing steps in many vision based applications. For example, consider the cases like visitor counter where a static camera takes the number of visitors entering or leaving the room, or a traffic camera extracting information about the vehicles etc. In all these cases, first you need to extract the person or vehicles alone. Technically, you need to extract the moving foreground from static background.</p>
<p>If you have an image of background alone, like image of the room without visitors, image of the road without vehicles etc, it is an easy job. Just subtract the new image from the background. You get the foreground objects alone. But in most of the cases, you may not have such an image, so we need to extract the background from whatever images we have. It become more complicated when there is shadow of the vehicles. Since shadow is also moving, simple subtraction will mark that also as foreground. It complicates things.</p>
<p>Several algorithms were introduced for this purpose. OpenCV has implemented three such algorithms which is very easy to use. We will see them one-by-one.</p>
<div class="section" id="backgroundsubtractormog">
<h6>BackgroundSubtractorMOG<a class="headerlink" href="#backgroundsubtractormog" title="Permalink to this headline">¶</a></h6>
<p>It is a Gaussian Mixture-based Background/Foreground Segmentation Algorithm. It was introduced in the paper &#8220;An improved adaptive background mixture model for real-time tracking with shadow detection&#8221; by P. KadewTraKuPong and R. Bowden in 2001. It uses a method to model each background pixel by a mixture of K Gaussian distributions (K = 3 to 5). The weights of the mixture represent the time proportions that those colours stay in the scene. The probable background colours are the ones which stay longer and more static.</p>
<p>While coding, we need to create a background object using the function, <strong>cv2.createBackgroundSubtractorMOG()</strong>. It has some optional parameters like length of history, number of gaussian mixtures, threshold etc. It is all set to some default values. Then inside the video loop, use <code class="docutils literal"><span class="pre">backgroundsubtractor.apply()</span></code> method to get the foreground mask.</p>
<p>See a simple example below:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cv2</span>

<span class="n">cap</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">VideoCapture</span><span class="p">(</span><span class="s1">&#39;vtest.avi&#39;</span><span class="p">)</span>

<span class="n">fgbg</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">createBackgroundSubtractorMOG</span><span class="p">()</span>

<span class="k">while</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">ret</span><span class="p">,</span> <span class="n">frame</span> <span class="o">=</span> <span class="n">cap</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>

    <span class="n">fgmask</span> <span class="o">=</span> <span class="n">fgbg</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>

    <span class="n">cv2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s1">&#39;frame&#39;</span><span class="p">,</span><span class="n">fgmask</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">30</span><span class="p">)</span> <span class="o">&amp;</span> <span class="mh">0xff</span>
    <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">27</span><span class="p">:</span>
        <span class="k">break</span>

<span class="n">cap</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>
</pre></div>
</div>
<p>( All the results are shown at the end for comparison).</p>
</div>
<div class="section" id="backgroundsubtractormog2">
<h6>BackgroundSubtractorMOG2<a class="headerlink" href="#backgroundsubtractormog2" title="Permalink to this headline">¶</a></h6>
<p>It is also a Gaussian Mixture-based Background/Foreground Segmentation Algorithm. It is based on two papers by Z.Zivkovic, &#8220;Improved adaptive Gausian mixture model for background subtraction&#8221; in 2004 and &#8220;Efficient Adaptive Density Estimation per Image Pixel for the Task of Background Subtraction&#8221; in 2006. One important feature of this algorithm is that it selects the appropriate number of gaussian distribution for each pixel. (Remember, in last case, we took a K gaussian distributions throughout the algorithm). It provides better adaptibility to varying scenes due illumination changes etc.</p>
<p>As in previous case, we have to create a background subtractor object. Here, you have an option of selecting whether shadow to be detected or not. If <code class="docutils literal"><span class="pre">detectShadows</span> <span class="pre">=</span> <span class="pre">True</span></code> (which is so by default), it detects and marks shadows, but decreases the speed. Shadows will be marked in gray color.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cv2</span>

<span class="n">cap</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">VideoCapture</span><span class="p">(</span><span class="s1">&#39;vtest.avi&#39;</span><span class="p">)</span>

<span class="n">fgbg</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">createBackgroundSubtractorMOG2</span><span class="p">()</span>

<span class="k">while</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">ret</span><span class="p">,</span> <span class="n">frame</span> <span class="o">=</span> <span class="n">cap</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>

    <span class="n">fgmask</span> <span class="o">=</span> <span class="n">fgbg</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>

    <span class="n">cv2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s1">&#39;frame&#39;</span><span class="p">,</span><span class="n">fgmask</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">30</span><span class="p">)</span> <span class="o">&amp;</span> <span class="mh">0xff</span>
    <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">27</span><span class="p">:</span>
        <span class="k">break</span>

<span class="n">cap</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>
</pre></div>
</div>
<p>(Results given at the end)</p>
</div>
<div class="section" id="backgroundsubtractorgmg">
<h6>BackgroundSubtractorGMG<a class="headerlink" href="#backgroundsubtractorgmg" title="Permalink to this headline">¶</a></h6>
<p>This algorithm combines statistical background image estimation and per-pixel Bayesian segmentation. It was introduced by Andrew B. Godbehere, Akihiro Matsukawa, Ken Goldberg in their paper &#8220;Visual Tracking of Human Visitors under Variable-Lighting Conditions for a Responsive Audio Art Installation&#8221; in 2012. As per the paper, the system ran a successful interactive audio art installation called “Are We There Yet?” from March 31 - July 31 2011 at the Contemporary Jewish Museum in San Francisco, California.</p>
<p>It uses first few (120 by default) frames for background modelling. It employs probabilistic foreground segmentation algorithm that identifies possible foreground objects using Bayesian inference. The estimates are adaptive; newer observations are more heavily weighted than old observations to accommodate variable illumination. Several morphological filtering operations like closing and opening are done to remove unwanted noise. You will get a black window during first few frames.</p>
<p>It would be better to apply morphological opening to the result to remove the noises.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cv2</span>

<span class="n">cap</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">VideoCapture</span><span class="p">(</span><span class="s1">&#39;vtest.avi&#39;</span><span class="p">)</span>

<span class="n">kernel</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">getStructuringElement</span><span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">MORPH_ELLIPSE</span><span class="p">,(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">fgbg</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">createBackgroundSubtractorGMG</span><span class="p">()</span>

<span class="k">while</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">ret</span><span class="p">,</span> <span class="n">frame</span> <span class="o">=</span> <span class="n">cap</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>

    <span class="n">fgmask</span> <span class="o">=</span> <span class="n">fgbg</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>
    <span class="n">fgmask</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">morphologyEx</span><span class="p">(</span><span class="n">fgmask</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">MORPH_OPEN</span><span class="p">,</span> <span class="n">kernel</span><span class="p">)</span>

    <span class="n">cv2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s1">&#39;frame&#39;</span><span class="p">,</span><span class="n">fgmask</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">30</span><span class="p">)</span> <span class="o">&amp;</span> <span class="mh">0xff</span>
    <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">27</span><span class="p">:</span>
        <span class="k">break</span>

<span class="n">cap</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="results">
<h5>Results<a class="headerlink" href="#results" title="Permalink to this headline">¶</a></h5>
<p><strong>Original Frame</strong></p>
<p>Below image shows the 200th frame of a video</p>
<blockquote>
<div><img alt="Original frame" class="align-center" src="_images/resframe.jpg" />
</div></blockquote>
<p><strong>Result of BackgroundSubtractorMOG</strong></p>
<blockquote>
<div><img alt="Result of BackgroundSubtractorMOG" class="align-center" src="_images/resmog.jpg" />
</div></blockquote>
<p><strong>Result of BackgroundSubtractorMOG2</strong></p>
<p>Gray color region shows shadow region.</p>
<blockquote>
<div><img alt="Result of BackgroundSubtractorMOG2" class="align-center" src="_images/resmog2.jpg" />
</div></blockquote>
<p><strong>Result of BackgroundSubtractorGMG</strong></p>
<p>Noise is removed with morphological opening.</p>
<blockquote>
<div><img alt="Result of BackgroundSubtractorGMG" class="align-center" src="_images/resgmg.jpg" />
</div></blockquote>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
</div>
</div>
</div>
</div>
<span id="document-py_tutorials/py_calib3d/py_table_of_contents_calib3d/py_table_of_contents_calib3d"></span><div class="section" id="camera-calibration-and-3d-reconstruction">
<span id="py-table-of-content-calib"></span><h3>Camera Calibration and 3D Reconstruction<a class="headerlink" href="#camera-calibration-and-3d-reconstruction" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference internal" href="index.html#calibration"><span class="std std-ref">Camera Calibration</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="9%" />
<col width="91%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/calibration_icon.jpg"><img alt="calib_1" src="_images/calibration_icon.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Let&#8217;s find how good is our camera. Is there any distortion in images taken with it? If so how to correct it?</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#pose-estimation"><span class="std std-ref">Pose Estimation</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="11%" />
<col width="89%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/pose_icon.jpg"><img alt="calib_2" src="_images/pose_icon.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>This is a small section which will help you to create some cool 3D effects with calib module.</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#epipolar-geometry"><span class="std std-ref">Epipolar Geometry</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="16%" />
<col width="84%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/epipolar_icon.jpg"><img alt="calib_3" src="_images/epipolar_icon.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Let&#8217;s understand epipolar geometry and epipolar constraint.</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#py-depthmap"><span class="std std-ref">Depth Map from Stereo Images</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="17%" />
<col width="83%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/depthmap_icon.jpg"><img alt="calib_4" src="_images/depthmap_icon.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Extract depth information from 2D images.</td>
</tr>
</tbody>
</table>
</div></blockquote>
<div class="toctree-wrapper compound">
<span id="document-py_tutorials/py_calib3d/py_calibration/py_calibration"></span><div class="section" id="camera-calibration">
<span id="calibration"></span><h4>Camera Calibration<a class="headerlink" href="#camera-calibration" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goal">
<h5>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h5>
<dl class="docutils">
<dt>In this section,</dt>
<dd><ul class="first last simple">
<li>We will learn about distortions in camera, intrinsic and extrinsic parameters of camera etc.</li>
<li>We will learn to find these parameters, undistort images etc.</li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="basics">
<h5>Basics<a class="headerlink" href="#basics" title="Permalink to this headline">¶</a></h5>
<p>Today&#8217;s cheap pinhole cameras introduces a lot of distortion to images. Two major distortions are radial distortion and tangential distortion.</p>
<p>Due to radial distortion, straight lines will appear curved. Its effect is more as we move away from the center of image. For example, one image is shown below, where two edges of a chess board are marked with red lines. But you can see that border is not a straight line and doesn&#8217;t match with the red line. All the expected straight lines are bulged out. Visit <a class="reference external" href="http://en.wikipedia.org/wiki/Distortion_%28optics%29">Distortion (optics)</a> for more details.</p>
<blockquote>
<div><img alt="Radial Distortion" class="align-center" src="_images/calib_radial.jpg" />
</div></blockquote>
<p>This distortion is solved as follows:</p>
<div class="math">
<p><img src="_images/math/fc9f12344ac840bfdcb53f61df91cf853ddd6af8.png" alt="x_{corrected} = x( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6) \\
y_{corrected} = y( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6)"/></p>
</div><p>Similarly, another distortion is the tangential distortion which occurs because image taking lense is not aligned perfectly parallel to the imaging plane. So some areas in image may look nearer than expected. It is solved as below:</p>
<div class="math">
<p><img src="_images/math/7817dd9b0769c056c23aeefc392cf06907a990a9.png" alt="x_{corrected} = x + [ 2p_1xy + p_2(r^2+2x^2)] \\
y_{corrected} = y + [ p_1(r^2+ 2y^2)+ 2p_2xy]"/></p>
</div><p>In short, we need to find five parameters, known as distortion coefficients given by:</p>
<div class="math">
<p><img src="_images/math/26ad926921974fb12443390fe4f8e91c9c2a9e38.png" alt="Distortion \; coefficients=(k_1 \hspace{10pt} k_2 \hspace{10pt} p_1 \hspace{10pt} p_2 \hspace{10pt} k_3)"/></p>
</div><p>In addition to this, we need to find a few more information, like intrinsic and extrinsic parameters of a camera. Intrinsic parameters are specific to a camera. It includes information like focal length (<img class="math" src="_images/math/7e7676a33200adb748a20857dfcc35d3e400a55f.png" alt="f_x,f_y"/>), optical centers (<img class="math" src="_images/math/c17fedc7253a535ef58793a77030ce9481c32e7b.png" alt="c_x, c_y"/>) etc. It is also called camera matrix. It depends on the camera only, so once calculated, it can be stored for future purposes. It is expressed as a 3x3 matrix:</p>
<div class="math">
<p><img src="_images/math/5e9d2db8b3561f68fa0688992fa45c13766a382c.png" alt="camera \; matrix = \left [ \begin{matrix}   f_x &amp; 0 &amp; c_x \\  0 &amp; f_y &amp; c_y \\   0 &amp; 0 &amp; 1 \end{matrix} \right ]"/></p>
</div><p>Extrinsic parameters corresponds to rotation and translation vectors which translates a coordinates of a 3D point to a coordinate system.</p>
<p>For stereo applications, these distortions need to be corrected first. To find all these parameters, what we have to do is to provide some sample images of a well defined pattern (eg, chess board). We find some specific points in it ( square corners in chess board). We know its coordinates in real world space and we know its coordinates in image. With these data, some mathematical problem is solved in background to get the distortion coefficients. That is the summary of the whole story. For better results, we need atleast 10 test patterns.</p>
</div>
<div class="section" id="code">
<h5>Code<a class="headerlink" href="#code" title="Permalink to this headline">¶</a></h5>
<p>As mentioned above, we need atleast 10 test patterns for camera calibration. OpenCV comes with some images of chess board (see <code class="docutils literal"><span class="pre">samples/cpp/left01.jpg</span> <span class="pre">--</span> <span class="pre">left14.jpg</span></code>), so we will utilize it. For sake of understanding, consider just one image of a chess board. Important input datas needed for camera calibration is a set of 3D real world points and its corresponding 2D image points. 2D image points are OK which we can easily find from the image. (These image points are locations where two black squares touch each other in chess boards)</p>
<p>What about the 3D points from real world space? Those images are taken from a static camera and chess boards are placed at different locations and orientations. So we need to know <img class="math" src="_images/math/512ffb563b5757e9704055fa55a7fcab41e30a31.png" alt="(X,Y,Z)"/> values. But for simplicity, we can say chess board was kept stationary at XY plane, (so Z=0 always) and camera was moved accordingly. This consideration helps us to find only X,Y values. Now for X,Y values, we can simply pass the points as (0,0), (1,0), (2,0), ... which denotes the location of points. In this case, the results we get will be in the scale of size of chess board square. But if we know the square size, (say 30 mm), and we can pass the values as (0,0),(30,0),(60,0),..., we get the results in mm. (In this case, we don&#8217;t know square size since we didn&#8217;t take those images, so we pass in terms of square size).</p>
<p>3D points are called <strong>object points</strong> and 2D image points are called <strong>image points.</strong></p>
<div class="section" id="setup">
<h6>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h6>
<p>So to find pattern in chess board, we use the function, <strong>cv2.findChessboardCorners()</strong>. We also need to pass what kind of pattern we are looking, like 8x8 grid, 5x5 grid etc. In this example, we use 7x6 grid. (Normally a chess board has 8x8 squares and 7x7 internal corners). It returns the corner points and retval which will be True if pattern is obtained. These corners will be placed in an order (from left-to-right, top-to-bottom)</p>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last">This function may not be able to find the required pattern in all the images. So one good option is to write the code such that, it starts the camera and check each frame for required pattern. Once pattern is obtained, find the corners and store it in a list. Also provides some interval before reading next frame so that we can adjust our chess board in different direction. Continue this process until required number of good patterns are obtained. Even in the example provided here, we are not sure out of 14 images given, how many are good. So we read all the images and take the good ones.</p>
</div>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last">Instead of chess board, we can use some circular grid, but then use the function <strong>cv2.findCirclesGrid()</strong> to find the pattern. It is said that less number of images are enough when using circular grid.</p>
</div>
<p>Once we find the corners, we can increase their accuracy using <strong>cv2.cornerSubPix()</strong>. We can also draw the pattern using <strong>cv2.drawChessboardCorners()</strong>. All these steps are included in below code:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">glob</span>

<span class="c1"># termination criteria</span>
<span class="n">criteria</span> <span class="o">=</span> <span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">TERM_CRITERIA_EPS</span> <span class="o">+</span> <span class="n">cv2</span><span class="o">.</span><span class="n">TERM_CRITERIA_MAX_ITER</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">)</span>

<span class="c1"># prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)</span>
<span class="n">objp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">6</span><span class="o">*</span><span class="mi">7</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">objp</span><span class="p">[:,:</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mgrid</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">7</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="mi">6</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Arrays to store object points and image points from all the images.</span>
<span class="n">objpoints</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># 3d point in real world space</span>
<span class="n">imgpoints</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># 2d points in image plane.</span>

<span class="n">images</span> <span class="o">=</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s1">&#39;*.jpg&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">fname</span> <span class="ow">in</span> <span class="n">images</span><span class="p">:</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">fname</span><span class="p">)</span>
    <span class="n">gray</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2GRAY</span><span class="p">)</span>

    <span class="c1"># Find the chess board corners</span>
    <span class="n">ret</span><span class="p">,</span> <span class="n">corners</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">findChessboardCorners</span><span class="p">(</span><span class="n">gray</span><span class="p">,</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">6</span><span class="p">),</span><span class="kc">None</span><span class="p">)</span>

    <span class="c1"># If found, add object points, image points (after refining them)</span>
    <span class="k">if</span> <span class="n">ret</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">objpoints</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">objp</span><span class="p">)</span>

        <span class="n">corners2</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cornerSubPix</span><span class="p">(</span><span class="n">gray</span><span class="p">,</span><span class="n">corners</span><span class="p">,(</span><span class="mi">11</span><span class="p">,</span><span class="mi">11</span><span class="p">),(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span><span class="n">criteria</span><span class="p">)</span>
        <span class="n">imgpoints</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">corners2</span><span class="p">)</span>

        <span class="c1"># Draw and display the corners</span>
        <span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">drawChessboardCorners</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">6</span><span class="p">),</span> <span class="n">corners2</span><span class="p">,</span><span class="n">ret</span><span class="p">)</span>
        <span class="n">cv2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s1">&#39;img&#39;</span><span class="p">,</span><span class="n">img</span><span class="p">)</span>
        <span class="n">cv2</span><span class="o">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">500</span><span class="p">)</span>

<span class="n">cv2</span><span class="o">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>
</pre></div>
</div>
<p>One image with pattern drawn on it is shown below:</p>
<blockquote>
<div><img alt="Calibration Pattern" class="align-center" src="_images/calib_pattern.jpg" />
</div></blockquote>
</div>
<div class="section" id="id1">
<h6>Calibration<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h6>
<p>So now we have our object points and image points we are ready to go for calibration. For that we use the function, <strong>cv2.calibrateCamera()</strong>. It returns the camera matrix, distortion coefficients, rotation and translation vectors etc.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">ret</span><span class="p">,</span> <span class="n">mtx</span><span class="p">,</span> <span class="n">dist</span><span class="p">,</span> <span class="n">rvecs</span><span class="p">,</span> <span class="n">tvecs</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">calibrateCamera</span><span class="p">(</span><span class="n">objpoints</span><span class="p">,</span> <span class="n">imgpoints</span><span class="p">,</span> <span class="n">gray</span><span class="o">.</span><span class="n">shape</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="kc">None</span><span class="p">,</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="undistortion">
<h6>Undistortion<a class="headerlink" href="#undistortion" title="Permalink to this headline">¶</a></h6>
<p>We have got what we were trying. Now we can take an image and undistort it. OpenCV comes with two methods, we will see both. But before that, we can refine the camera matrix based on a free scaling parameter using <strong>cv2.getOptimalNewCameraMatrix()</strong>. If the scaling parameter <code class="docutils literal"><span class="pre">alpha=0</span></code>, it returns undistorted image with minimum unwanted pixels. So it may even remove some pixels at image corners. If <code class="docutils literal"><span class="pre">alpha=1</span></code>, all pixels are retained with some extra black images. It also returns an image ROI which can be used to crop the result.</p>
<p>So we take a new image (<code class="docutils literal"><span class="pre">left12.jpg</span></code> in this case. That is the first image in this chapter)</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;left12.jpg&#39;</span><span class="p">)</span>
<span class="n">h</span><span class="p">,</span>  <span class="n">w</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
<span class="n">newcameramtx</span><span class="p">,</span> <span class="n">roi</span><span class="o">=</span><span class="n">cv2</span><span class="o">.</span><span class="n">getOptimalNewCameraMatrix</span><span class="p">(</span><span class="n">mtx</span><span class="p">,</span><span class="n">dist</span><span class="p">,(</span><span class="n">w</span><span class="p">,</span><span class="n">h</span><span class="p">),</span><span class="mi">1</span><span class="p">,(</span><span class="n">w</span><span class="p">,</span><span class="n">h</span><span class="p">))</span>
</pre></div>
</div>
<div class="section" id="using-cv2-undistort">
<h7>1. Using <strong>cv2.undistort()</strong><a class="headerlink" href="#using-cv2-undistort" title="Permalink to this headline">¶</a></h7>
<p>This is the shortest path. Just call the function and use ROI obtained above to crop the result.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="c1"># undistort</span>
<span class="n">dst</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">undistort</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">mtx</span><span class="p">,</span> <span class="n">dist</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">newcameramtx</span><span class="p">)</span>

<span class="c1"># crop the image</span>
<span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="n">h</span> <span class="o">=</span> <span class="n">roi</span>
<span class="n">dst</span> <span class="o">=</span> <span class="n">dst</span><span class="p">[</span><span class="n">y</span><span class="p">:</span><span class="n">y</span><span class="o">+</span><span class="n">h</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span><span class="n">x</span><span class="o">+</span><span class="n">w</span><span class="p">]</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">imwrite</span><span class="p">(</span><span class="s1">&#39;calibresult.png&#39;</span><span class="p">,</span><span class="n">dst</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="using-remapping">
<h7>2. Using <strong>remapping</strong><a class="headerlink" href="#using-remapping" title="Permalink to this headline">¶</a></h7>
<p>This is curved path. First find a mapping function from distorted image to undistorted image. Then use the remap function.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="c1"># undistort</span>
<span class="n">mapx</span><span class="p">,</span><span class="n">mapy</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">initUndistortRectifyMap</span><span class="p">(</span><span class="n">mtx</span><span class="p">,</span><span class="n">dist</span><span class="p">,</span><span class="kc">None</span><span class="p">,</span><span class="n">newcameramtx</span><span class="p">,(</span><span class="n">w</span><span class="p">,</span><span class="n">h</span><span class="p">),</span><span class="mi">5</span><span class="p">)</span>
<span class="n">dst</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">mapx</span><span class="p">,</span><span class="n">mapy</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">INTER_LINEAR</span><span class="p">)</span>

<span class="c1"># crop the image</span>
<span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="n">h</span> <span class="o">=</span> <span class="n">roi</span>
<span class="n">dst</span> <span class="o">=</span> <span class="n">dst</span><span class="p">[</span><span class="n">y</span><span class="p">:</span><span class="n">y</span><span class="o">+</span><span class="n">h</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span><span class="n">x</span><span class="o">+</span><span class="n">w</span><span class="p">]</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">imwrite</span><span class="p">(</span><span class="s1">&#39;calibresult.png&#39;</span><span class="p">,</span><span class="n">dst</span><span class="p">)</span>
</pre></div>
</div>
<p>Both the methods give the same result. See the result below:</p>
<blockquote>
<div><img alt="Calibration Result" class="align-center" src="_images/calib_result.jpg" />
</div></blockquote>
<p>You can see in the result that all the edges are straight.</p>
<p>Now you can store the camera matrix and distortion coefficients using write functions in Numpy (np.savez, np.savetxt etc) for future uses.</p>
</div>
</div>
</div>
<div class="section" id="re-projection-error">
<h5>Re-projection Error<a class="headerlink" href="#re-projection-error" title="Permalink to this headline">¶</a></h5>
<p>Re-projection error gives a good estimation of just how exact is the found parameters. This should be as close to zero as possible. Given the intrinsic, distortion, rotation and translation matrices, we first transform the object point to image point using <strong>cv2.projectPoints()</strong>. Then we calculate the absolute norm between what we got with our transformation and the corner finding algorithm. To find the average error we calculate the arithmetical mean of the errors calculate for all the calibration images.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">mean_error</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">objpoints</span><span class="p">)):</span>
    <span class="n">imgpoints2</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">projectPoints</span><span class="p">(</span><span class="n">objpoints</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">rvecs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">tvecs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">mtx</span><span class="p">,</span> <span class="n">dist</span><span class="p">)</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">imgpoints</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">imgpoints2</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">NORM_L2</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">imgpoints2</span><span class="p">)</span>
    <span class="n">tot_error</span> <span class="o">+=</span> <span class="n">error</span>

<span class="nb">print</span> <span class="s2">&quot;total error: &quot;</span><span class="p">,</span> <span class="n">mean_error</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">objpoints</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
<ol class="arabic simple">
<li>Try camera calibration with circular grid.</li>
</ol>
</div>
</div>
<span id="document-py_tutorials/py_calib3d/py_pose/py_pose"></span><div class="section" id="pose-estimation">
<span id="id1"></span><h4>Pose Estimation<a class="headerlink" href="#pose-estimation" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goal">
<h5>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h5>
<dl class="docutils">
<dt>In this section,</dt>
<dd><ul class="first last simple">
<li>We will learn to exploit calib3d module to create some 3D effects in images.</li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="basics">
<h5>Basics<a class="headerlink" href="#basics" title="Permalink to this headline">¶</a></h5>
<p>This is going to be a small section. During the last session on camera calibration, you have found the camera matrix, distortion coefficients etc. Given a pattern image, we can utilize the above information to calculate its pose, or how the object is situated in space, like how it is rotated, how it is displaced etc. For a planar object, we can assume Z=0, such that, the problem now becomes how camera is placed in space to see our pattern image. So, if we know how the object lies in the space, we can draw some 2D diagrams in it to simulate the 3D effect. Let&#8217;s see how to do it.</p>
<p>Our problem is, we want to draw our 3D coordinate axis (X, Y, Z axes) on our chessboard&#8217;s first corner. X axis in blue color, Y axis in green color and Z axis in red color. So in-effect, Z axis should feel like it is perpendicular to our chessboard plane.</p>
<p>First, let&#8217;s load the camera matrix and distortion coefficients from the previous calibration result.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">glob</span>

<span class="c1"># Load previously saved data</span>
<span class="k">with</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;B.npz&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">X</span><span class="p">:</span>
    <span class="n">mtx</span><span class="p">,</span> <span class="n">dist</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="p">[</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;mtx&#39;</span><span class="p">,</span><span class="s1">&#39;dist&#39;</span><span class="p">,</span><span class="s1">&#39;rvecs&#39;</span><span class="p">,</span><span class="s1">&#39;tvecs&#39;</span><span class="p">)]</span>
</pre></div>
</div>
<p>Now let&#8217;s create a function, <code class="docutils literal"><span class="pre">draw</span></code> which takes the corners in the chessboard (obtained using <strong>cv2.findChessboardCorners()</strong>) and <strong>axis points</strong> to draw a 3D axis.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">draw</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">corners</span><span class="p">,</span> <span class="n">imgpts</span><span class="p">):</span>
    <span class="n">corner</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">corners</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">line</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">corner</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">imgpts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">ravel</span><span class="p">()),</span> <span class="p">(</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="mi">5</span><span class="p">)</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">line</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">corner</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">imgpts</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">ravel</span><span class="p">()),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="mi">5</span><span class="p">)</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">line</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">corner</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">imgpts</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">ravel</span><span class="p">()),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">),</span> <span class="mi">5</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">img</span>
</pre></div>
</div>
<p>Then as in previous case, we create termination criteria, object points (3D points of corners in chessboard) and axis points. Axis points are points in 3D space for drawing the axis. We draw axis of length 3 (units will be in terms of chess square size since we calibrated based on that size). So our X axis is drawn from (0,0,0) to (3,0,0), so for Y axis. For Z axis, it is drawn from (0,0,0) to (0,0,-3). Negative denotes it is drawn towards the camera.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">criteria</span> <span class="o">=</span> <span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">TERM_CRITERIA_EPS</span> <span class="o">+</span> <span class="n">cv2</span><span class="o">.</span><span class="n">TERM_CRITERIA_MAX_ITER</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">)</span>
<span class="n">objp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">6</span><span class="o">*</span><span class="mi">7</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">objp</span><span class="p">[:,:</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mgrid</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">7</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="mi">6</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>

<span class="n">axis</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">]])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
<p>Now, as usual, we load each image. Search for 7x6 grid. If found, we refine it with subcorner pixels. Then to calculate the rotation and translation, we use the function, <strong>cv2.solvePnPRansac()</strong>. Once we those transformation matrices, we use them to project our <strong>axis points</strong> to the image plane. In simple words, we find the points on image plane corresponding to each of (3,0,0),(0,3,0),(0,0,3) in 3D space. Once we get them, we draw lines from the first corner to each of these points using our <code class="docutils literal"><span class="pre">draw()</span></code> function. Done !!!</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">fname</span> <span class="ow">in</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s1">&#39;left*.jpg&#39;</span><span class="p">):</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">fname</span><span class="p">)</span>
    <span class="n">gray</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2GRAY</span><span class="p">)</span>
    <span class="n">ret</span><span class="p">,</span> <span class="n">corners</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">findChessboardCorners</span><span class="p">(</span><span class="n">gray</span><span class="p">,</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">6</span><span class="p">),</span><span class="kc">None</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">ret</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">corners2</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cornerSubPix</span><span class="p">(</span><span class="n">gray</span><span class="p">,</span><span class="n">corners</span><span class="p">,(</span><span class="mi">11</span><span class="p">,</span><span class="mi">11</span><span class="p">),(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span><span class="n">criteria</span><span class="p">)</span>

        <span class="c1"># Find the rotation and translation vectors.</span>
        <span class="n">rvecs</span><span class="p">,</span> <span class="n">tvecs</span><span class="p">,</span> <span class="n">inliers</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">solvePnPRansac</span><span class="p">(</span><span class="n">objp</span><span class="p">,</span> <span class="n">corners2</span><span class="p">,</span> <span class="n">mtx</span><span class="p">,</span> <span class="n">dist</span><span class="p">)</span>

        <span class="c1"># project 3D points to image plane</span>
        <span class="n">imgpts</span><span class="p">,</span> <span class="n">jac</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">projectPoints</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">rvecs</span><span class="p">,</span> <span class="n">tvecs</span><span class="p">,</span> <span class="n">mtx</span><span class="p">,</span> <span class="n">dist</span><span class="p">)</span>

        <span class="n">img</span> <span class="o">=</span> <span class="n">draw</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">corners2</span><span class="p">,</span><span class="n">imgpts</span><span class="p">)</span>
        <span class="n">cv2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s1">&#39;img&#39;</span><span class="p">,</span><span class="n">img</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">&amp;</span> <span class="mh">0xff</span>
        <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="s1">&#39;s&#39;</span><span class="p">:</span>
            <span class="n">cv2</span><span class="o">.</span><span class="n">imwrite</span><span class="p">(</span><span class="n">fname</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span><span class="o">+</span><span class="s1">&#39;.png&#39;</span><span class="p">,</span> <span class="n">img</span><span class="p">)</span>

<span class="n">cv2</span><span class="o">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>
</pre></div>
</div>
<p>See some results below. Notice that each axis is 3 squares long.:</p>
<blockquote>
<div><img alt="Pose Estimation" class="align-center" src="_images/pose_1.jpg" />
</div></blockquote>
<div class="section" id="render-a-cube">
<h6>Render a Cube<a class="headerlink" href="#render-a-cube" title="Permalink to this headline">¶</a></h6>
<p>If you want to draw a cube, modify the draw() function and axis points as follows.</p>
<p>Modified draw() function:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">draw</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">corners</span><span class="p">,</span> <span class="n">imgpts</span><span class="p">):</span>
    <span class="n">imgpts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">(</span><span class="n">imgpts</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>

    <span class="c1"># draw ground floor in green</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">drawContours</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="p">[</span><span class="n">imgpts</span><span class="p">[:</span><span class="mi">4</span><span class="p">]],</span><span class="o">-</span><span class="mi">1</span><span class="p">,(</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span>

    <span class="c1"># draw pillars in blue color</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">j</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span><span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">8</span><span class="p">)):</span>
        <span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">line</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">imgpts</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">imgpts</span><span class="p">[</span><span class="n">j</span><span class="p">]),(</span><span class="mi">255</span><span class="p">),</span><span class="mi">3</span><span class="p">)</span>

    <span class="c1"># draw top layer in red color</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">drawContours</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="p">[</span><span class="n">imgpts</span><span class="p">[</span><span class="mi">4</span><span class="p">:]],</span><span class="o">-</span><span class="mi">1</span><span class="p">,(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">),</span><span class="mi">3</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">img</span>
</pre></div>
</div>
<p>Modified axis points. They are the 8 corners of a cube in 3D space:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">axis</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
                   <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span> <span class="p">])</span>
</pre></div>
</div>
<p>And look at the result below:</p>
<blockquote>
<div><img alt="Pose Estimation" class="align-center" src="_images/pose_2.jpg" />
</div></blockquote>
<p>If you are interested in graphics, augmented reality etc, you can use OpenGL to render more complicated figures.</p>
</div>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
</div>
</div>
<span id="document-py_tutorials/py_calib3d/py_epipolar_geometry/py_epipolar_geometry"></span><div class="section" id="epipolar-geometry">
<span id="id1"></span><h4>Epipolar Geometry<a class="headerlink" href="#epipolar-geometry" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goal">
<h5>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h5>
<p>In this section,</p>
<blockquote>
<div><ul class="simple">
<li>We will learn about the basics of multiview geometry</li>
<li>We will see what is epipole, epipolar lines, epipolar constraint etc.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="basic-concepts">
<h5>Basic Concepts<a class="headerlink" href="#basic-concepts" title="Permalink to this headline">¶</a></h5>
<p>When we take an image using pin-hole camera, we loose an important information, ie depth of the image. Or how far is each point in the image from the camera because it is a 3D-to-2D conversion. So it is an important question whether we can find the depth information using these cameras. And the answer is to use more than one camera. Our eyes works in similar way where we use two cameras (two eyes) which is called stereo vision. So let&#8217;s see what OpenCV provides in this field.</p>
<p>(<em>Learning OpenCV</em> by Gary Bradsky has a lot of information in this field.)</p>
<p>Before going to depth images, let&#8217;s first understand some basic concepts in multiview geometry. In this section we will deal with epipolar geometry. See the image below which shows a basic setup with two cameras taking the image of same scene.</p>
<blockquote>
<div><img alt="Epipolar geometry" class="align-center" src="_images/epipolar.jpg" />
</div></blockquote>
<p>If we are using only the left camera, we can&#8217;t find the 3D point corresponding to the point <img class="math" src="_images/math/188c175aac0a8a9c22499336711b5d7256407254.png" alt="x"/> in image because every point on the line <img class="math" src="_images/math/9a0ff8b70b7fa76da01ea6282285e57aec1626bc.png" alt="OX"/> projects to the same point on the image plane. But consider the right image also. Now different points on the line <img class="math" src="_images/math/9a0ff8b70b7fa76da01ea6282285e57aec1626bc.png" alt="OX"/> projects to different points (<img class="math" src="_images/math/1ba6dff92a6534953b79b494fd0418c4ea6e7be3.png" alt="x'"/>) in right plane. So with these two images, we can triangulate the correct 3D point. This is the whole idea.</p>
<p>The projection of the different points on <img class="math" src="_images/math/9a0ff8b70b7fa76da01ea6282285e57aec1626bc.png" alt="OX"/> form a line on right plane (line <img class="math" src="_images/math/220366b154fb9d45a01498f2a977f43c93837dc2.png" alt="l'"/>). We call it <strong>epiline</strong> corresponding to the point <img class="math" src="_images/math/188c175aac0a8a9c22499336711b5d7256407254.png" alt="x"/>. It means, to find the point <img class="math" src="_images/math/188c175aac0a8a9c22499336711b5d7256407254.png" alt="x"/> on the right image, search along this epiline. It should be somewhere on this line (Think of it this way, to find the matching point in other image, you need not search the whole image, just search along the epiline. So it provides better performance and accuracy). This is called <strong>Epipolar Constraint</strong>. Similarly all points will have its corresponding epilines in the other image. The plane <img class="math" src="_images/math/bd543d0c80e5f63ddac60bd9537f335c6e86ceec.png" alt="XOO'"/> is called <strong>Epipolar Plane</strong>.</p>
<p><img class="math" src="_images/math/f3f716e1c56d5ebdca97bb1546662b9e96f5be07.png" alt="O"/> and <img class="math" src="_images/math/15ed528486df5661242a68d834bc09bc01827ad5.png" alt="O'"/> are the camera centers. From the setup given above, you can see that projection of right camera <img class="math" src="_images/math/15ed528486df5661242a68d834bc09bc01827ad5.png" alt="O'"/> is seen on the left image at the point, <img class="math" src="_images/math/630e3a780577ea7921e81ee2ac5237dd1802ec8d.png" alt="e"/>. It is called the <strong>epipole</strong>. Epipole is the point of intersection of line through camera centers and the image planes. Similarly <img class="math" src="_images/math/3fbcda0564c7f386345cb7a8c7ef20a44e552f89.png" alt="e'"/> is the epipole of the left camera. In some cases, you won&#8217;t be able to locate the epipole in the image, they may be outside the image (which means, one camera doesn&#8217;t see the other).</p>
<p>All the epilines pass through its epipole. So to find the location of epipole, we can find many epilines and find their intersection point.</p>
<p>So in this session, we focus on finding epipolar lines and epipoles. But to find them, we need two more ingredients, <strong>Fundamental Matrix (F)</strong> and <strong>Essential Matrix (E)</strong>. Essential Matrix contains the information about translation and rotation, which describe the location of the second camera relative to the first in global coordinates. See the image below (Image courtesy: Learning OpenCV by Gary Bradsky):</p>
<blockquote>
<div><img alt="Essential Matrix" class="align-center" src="_images/essential_matrix.jpg" />
</div></blockquote>
<p>But we prefer measurements to be done in pixel coordinates, right? Fundamental Matrix  contains the same information as Essential Matrix in addition to the information about the intrinsics of both cameras so that we can relate the two cameras in pixel coordinates. (If we are using rectified images and normalize the point by dividing by the focal lengths, <img class="math" src="_images/math/3cc166a528044f2dc2e7938096d0709aa0153975.png" alt="F=E"/>). In simple words, Fundamental Matrix F, maps a point in one image to a line (epiline) in the other image. This is calculated from matching points from both the images. A minimum of 8 such points are required to find the fundamental matrix (while using 8-point algorithm). More points are preferred and use RANSAC to get a more robust result.</p>
</div>
<div class="section" id="code">
<h5>Code<a class="headerlink" href="#code" title="Permalink to this headline">¶</a></h5>
<p>So first we need to find as many possible matches between two images to find the fundamental matrix. For this, we use SIFT descriptors with FLANN based matcher and ratio test.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">img1</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;myleft.jpg&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>  <span class="c1">#queryimage # left image</span>
<span class="n">img2</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;myright.jpg&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span> <span class="c1">#trainimage # right image</span>

<span class="n">sift</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">SIFT</span><span class="p">()</span>

<span class="c1"># find the keypoints and descriptors with SIFT</span>
<span class="n">kp1</span><span class="p">,</span> <span class="n">des1</span> <span class="o">=</span> <span class="n">sift</span><span class="o">.</span><span class="n">detectAndCompute</span><span class="p">(</span><span class="n">img1</span><span class="p">,</span><span class="kc">None</span><span class="p">)</span>
<span class="n">kp2</span><span class="p">,</span> <span class="n">des2</span> <span class="o">=</span> <span class="n">sift</span><span class="o">.</span><span class="n">detectAndCompute</span><span class="p">(</span><span class="n">img2</span><span class="p">,</span><span class="kc">None</span><span class="p">)</span>

<span class="c1"># FLANN parameters</span>
<span class="n">FLANN_INDEX_KDTREE</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">index_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">algorithm</span> <span class="o">=</span> <span class="n">FLANN_INDEX_KDTREE</span><span class="p">,</span> <span class="n">trees</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">search_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">checks</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>

<span class="n">flann</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">FlannBasedMatcher</span><span class="p">(</span><span class="n">index_params</span><span class="p">,</span><span class="n">search_params</span><span class="p">)</span>
<span class="n">matches</span> <span class="o">=</span> <span class="n">flann</span><span class="o">.</span><span class="n">knnMatch</span><span class="p">(</span><span class="n">des1</span><span class="p">,</span><span class="n">des2</span><span class="p">,</span><span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">good</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">pts1</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">pts2</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># ratio test as per Lowe&#39;s paper</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,(</span><span class="n">m</span><span class="p">,</span><span class="n">n</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">matches</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">distance</span> <span class="o">&lt;</span> <span class="mf">0.8</span><span class="o">*</span><span class="n">n</span><span class="o">.</span><span class="n">distance</span><span class="p">:</span>
        <span class="n">good</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
        <span class="n">pts2</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">kp2</span><span class="p">[</span><span class="n">m</span><span class="o">.</span><span class="n">trainIdx</span><span class="p">]</span><span class="o">.</span><span class="n">pt</span><span class="p">)</span>
        <span class="n">pts1</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">kp1</span><span class="p">[</span><span class="n">m</span><span class="o">.</span><span class="n">queryIdx</span><span class="p">]</span><span class="o">.</span><span class="n">pt</span><span class="p">)</span>
</pre></div>
</div>
<p>Now we have the list of best matches from both the images. Let&#8217;s find the Fundamental Matrix.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">pts1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">(</span><span class="n">pts1</span><span class="p">)</span>
<span class="n">pts2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">(</span><span class="n">pts2</span><span class="p">)</span>
<span class="n">F</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">findFundamentalMat</span><span class="p">(</span><span class="n">pts1</span><span class="p">,</span><span class="n">pts2</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">FM_LMEDS</span><span class="p">)</span>

<span class="c1"># We select only inlier points</span>
<span class="n">pts1</span> <span class="o">=</span> <span class="n">pts1</span><span class="p">[</span><span class="n">mask</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span><span class="o">==</span><span class="mi">1</span><span class="p">]</span>
<span class="n">pts2</span> <span class="o">=</span> <span class="n">pts2</span><span class="p">[</span><span class="n">mask</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span><span class="o">==</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
<p>Next we find the epilines. Epilines corresponding to the points in first image is drawn on second image. So mentioning of correct images are important here. We get an array of lines. So we define a new function to draw these lines on the images.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">drawlines</span><span class="p">(</span><span class="n">img1</span><span class="p">,</span><span class="n">img2</span><span class="p">,</span><span class="n">lines</span><span class="p">,</span><span class="n">pts1</span><span class="p">,</span><span class="n">pts2</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39; img1 - image on which we draw the epilines for the points in img2</span>
<span class="sd">        lines - corresponding epilines &#39;&#39;&#39;</span>
    <span class="n">r</span><span class="p">,</span><span class="n">c</span> <span class="o">=</span> <span class="n">img1</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">img1</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">img1</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_GRAY2BGR</span><span class="p">)</span>
    <span class="n">img2</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">img2</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_GRAY2BGR</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">r</span><span class="p">,</span><span class="n">pt1</span><span class="p">,</span><span class="n">pt2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">lines</span><span class="p">,</span><span class="n">pts1</span><span class="p">,</span><span class="n">pts2</span><span class="p">):</span>
        <span class="n">color</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
        <span class="n">x0</span><span class="p">,</span><span class="n">y0</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="n">r</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">/</span><span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">])</span>
        <span class="n">x1</span><span class="p">,</span><span class="n">y1</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="p">[</span><span class="n">c</span><span class="p">,</span> <span class="o">-</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">+</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">c</span><span class="p">)</span><span class="o">/</span><span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">])</span>
        <span class="n">img1</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">line</span><span class="p">(</span><span class="n">img1</span><span class="p">,</span> <span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">y0</span><span class="p">),</span> <span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">y1</span><span class="p">),</span> <span class="n">color</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">img1</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">circle</span><span class="p">(</span><span class="n">img1</span><span class="p">,</span><span class="nb">tuple</span><span class="p">(</span><span class="n">pt1</span><span class="p">),</span><span class="mi">5</span><span class="p">,</span><span class="n">color</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">img2</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">circle</span><span class="p">(</span><span class="n">img2</span><span class="p">,</span><span class="nb">tuple</span><span class="p">(</span><span class="n">pt2</span><span class="p">),</span><span class="mi">5</span><span class="p">,</span><span class="n">color</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">img1</span><span class="p">,</span><span class="n">img2</span>
</pre></div>
</div>
<p>Now we find the epilines in both the images and draw them.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="c1"># Find epilines corresponding to points in right image (second image) and</span>
<span class="c1"># drawing its lines on left image</span>
<span class="n">lines1</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">computeCorrespondEpilines</span><span class="p">(</span><span class="n">pts2</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span><span class="n">F</span><span class="p">)</span>
<span class="n">lines1</span> <span class="o">=</span> <span class="n">lines1</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="n">img5</span><span class="p">,</span><span class="n">img6</span> <span class="o">=</span> <span class="n">drawlines</span><span class="p">(</span><span class="n">img1</span><span class="p">,</span><span class="n">img2</span><span class="p">,</span><span class="n">lines1</span><span class="p">,</span><span class="n">pts1</span><span class="p">,</span><span class="n">pts2</span><span class="p">)</span>

<span class="c1"># Find epilines corresponding to points in left image (first image) and</span>
<span class="c1"># drawing its lines on right image</span>
<span class="n">lines2</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">computeCorrespondEpilines</span><span class="p">(</span><span class="n">pts1</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span><span class="n">F</span><span class="p">)</span>
<span class="n">lines2</span> <span class="o">=</span> <span class="n">lines2</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="n">img3</span><span class="p">,</span><span class="n">img4</span> <span class="o">=</span> <span class="n">drawlines</span><span class="p">(</span><span class="n">img2</span><span class="p">,</span><span class="n">img1</span><span class="p">,</span><span class="n">lines2</span><span class="p">,</span><span class="n">pts2</span><span class="p">,</span><span class="n">pts1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Below is the result we get:</p>
<blockquote>
<div><img alt="Epilines" class="align-center" src="_images/epiresult.jpg" />
</div></blockquote>
<p>You can see in the left image that all epilines are converging at a point outside the image at right side. That meeting point is the epipole.</p>
<p>For better results, images with good resolution and many non-planar points should be used.</p>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
<ol class="arabic simple">
<li>One important topic is the forward movement of camera. Then epipoles will be seen at the same locations in both with epilines emerging from a fixed point. <a class="reference external" href="http://answers.opencv.org/question/17912/location-of-epipole/">See this discussion</a>.</li>
<li>Fundamental Matrix estimation is sensitive to quality of matches, outliers etc. It becomes worse when all selected matches lie on the same plane. <a class="reference external" href="http://answers.opencv.org/question/18125/epilines-not-correct/">Check this discussion</a>.</li>
</ol>
</div>
</div>
<span id="document-py_tutorials/py_calib3d/py_depthmap/py_depthmap"></span><div class="section" id="depth-map-from-stereo-images">
<span id="py-depthmap"></span><h4>Depth Map from Stereo Images<a class="headerlink" href="#depth-map-from-stereo-images" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goal">
<h5>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h5>
<dl class="docutils">
<dt>In this session,</dt>
<dd><ul class="first last simple">
<li>We will learn to create depth map from stereo images.</li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="basics">
<h5>Basics<a class="headerlink" href="#basics" title="Permalink to this headline">¶</a></h5>
<p>In last session, we saw basic concepts like epipolar constraints and other related terms. We also saw that if we have two images of same scene, we can get depth information from that in an intuitive way. Below is an image and some simple mathematical formulas which proves that intuition. (Image Courtesy :</p>
<blockquote>
<div><img alt="Calculating depth" class="align-center" src="_images/stereo_depth.jpg" />
</div></blockquote>
<p>The above diagram contains equivalent triangles. Writing their equivalent equations will yield us following result:</p>
<div class="math">
<p><img src="_images/math/4415d16afe56fe75cab0d4d9763e448311fd6720.png" alt="disparity = x - x' = \frac{Bf}{Z}"/></p>
</div><p><img class="math" src="_images/math/188c175aac0a8a9c22499336711b5d7256407254.png" alt="x"/> and <img class="math" src="_images/math/1ba6dff92a6534953b79b494fd0418c4ea6e7be3.png" alt="x'"/> are the distance between points in image plane corresponding to the scene point 3D and their camera center. <img class="math" src="_images/math/83956e92fcc80dee17fce864543216939a3c9da7.png" alt="B"/> is the distance between two cameras (which we know) and <img class="math" src="_images/math/0001d02b63ede2fe3219e05a7cd09c82ae6298b6.png" alt="f"/> is the focal length of camera (already known). So in short, above equation says that the depth of a point in a scene is inversely proportional to the difference in distance of corresponding image points and their camera centers. So with this information, we can derive the depth of all pixels in an image.</p>
<p>So it finds corresponding matches between two images. We have already seen how epiline constraint make this operation faster and accurate. Once it finds matches, it finds the disparity. Let&#8217;s see how we can do it with OpenCV.</p>
</div>
<div class="section" id="code">
<h5>Code<a class="headerlink" href="#code" title="Permalink to this headline">¶</a></h5>
<p>Below code snippet shows a simple procedure to create disparity map.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">imgL</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;tsukuba_l.png&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">imgR</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;tsukuba_r.png&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>

<span class="n">stereo</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">createStereoBM</span><span class="p">(</span><span class="n">numDisparities</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">blockSize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">disparity</span> <span class="o">=</span> <span class="n">stereo</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">imgL</span><span class="p">,</span><span class="n">imgR</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">disparity</span><span class="p">,</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Below image contains the original image (left) and its disparity map (right). As you can see, result is contaminated with high degree of noise. By adjusting the values of numDisparities and blockSize, you can get better results.</p>
<blockquote>
<div><img alt="Disparity Map" class="align-center" src="_images/disparity_map.jpg" />
</div></blockquote>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">More details to be added</p>
</div>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
<ol class="arabic simple">
<li>OpenCV samples contain an example of generating disparity map and its 3D reconstruction. Check <code class="docutils literal"><span class="pre">stereo_match.py</span></code> in OpenCV-Python samples.</li>
</ol>
</div>
</div>
</div>
</div>
<span id="document-py_tutorials/py_ml/py_table_of_contents_ml/py_table_of_contents_ml"></span><div class="section" id="machine-learning">
<span id="py-table-of-content-ml"></span><h3>Machine Learning<a class="headerlink" href="#machine-learning" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference internal" href="index.html#knn"><span class="std std-ref">K-Nearest Neighbour</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="14%" />
<col width="86%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/knnicon.png"><img alt="ML_KNN" src="_images/knnicon.png" style="width: 90pt; height: 90pt;" /></a></td>
<td>Learn to use kNN for classification
Plus learn about handwritten digit recognition using kNN</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#svm"><span class="std std-ref">Support Vector Machines (SVM)</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="14%" />
<col width="86%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/svmicon.png"><img alt="ML_SVM" src="_images/svmicon.png" style="width: 90pt; height: 90pt;" /></a></td>
<td>Understand concepts of SVM</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#kmeans-clustering"><span class="std std-ref">K-Means Clustering</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="14%" />
<col width="86%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/kmeansicon.jpg"><img alt="ML_KM" src="_images/kmeansicon.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Learn to use K-Means Clustering to group data to a number of clusters.
Plus learn to do color quantization using K-Means Clustering</td>
</tr>
</tbody>
</table>
</div></blockquote>
<div class="toctree-wrapper compound">
<span id="document-py_tutorials/py_ml/py_knn/py_knn_index"></span><div class="section" id="k-nearest-neighbour">
<span id="knn"></span><h4>K-Nearest Neighbour<a class="headerlink" href="#k-nearest-neighbour" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><a class="reference internal" href="index.html#knn-understanding"><span class="std std-ref">Understanding k-Nearest Neighbour</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="14%" />
<col width="86%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/knn_icon1.jpg"><img alt="KNN_1" src="_images/knn_icon1.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Get a basic understanding of what kNN is</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#knn-opencv"><span class="std std-ref">OCR of Hand-written Data using kNN</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="14%" />
<col width="86%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/knn_icon2.jpg"><img alt="KNN_2" src="_images/knn_icon2.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Now let&#8217;s use kNN in OpenCV for digit recognition OCR</td>
</tr>
</tbody>
</table>
</div></blockquote>
<div class="toctree-wrapper compound">
<span id="document-py_tutorials/py_ml/py_knn/py_knn_understanding/py_knn_understanding"></span><div class="section" id="understanding-k-nearest-neighbour">
<span id="knn-understanding"></span><h5>Understanding k-Nearest Neighbour<a class="headerlink" href="#understanding-k-nearest-neighbour" title="Permalink to this headline">¶</a></h5>
<div class="section" id="goal">
<h6>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h6>
<p>In this chapter, we will understand the concepts of k-Nearest Neighbour (kNN) algorithm.</p>
</div>
<div class="section" id="theory">
<h6>Theory<a class="headerlink" href="#theory" title="Permalink to this headline">¶</a></h6>
<p>kNN is one of the simplest of classification algorithms available for supervised learning. The idea is to search for closest match of the test data in feature space. We will look into it with below image.</p>
<blockquote>
<div><img alt="Understanding kNN" class="align-center" src="_images/knn_theory.png" />
</div></blockquote>
<p>In the image, there are two families, <cite>Blue Squares and Red Triangles</cite>. We call each family as <strong>Class</strong>. Their houses are shown in their town map which we call <cite>feature space</cite>. <em>(You can consider a feature space as a space where all datas are projected. For example, consider a 2D coordinate space. Each data has two features, x and y coordinates. You can represent this data in your 2D coordinate space, right? Now imagine if there are three features, you need 3D space. Now consider N features, where you need N-dimensional space, right? This N-dimensional space is its feature space. In our image, you can consider it as a 2D case with two features)</em>.</p>
<p>Now a new member comes into the town and creates a new home, which is shown as green circle. He should be added to one of these Blue/Red families. We call that process, <strong>Classification</strong>. What we do? Since we are dealing with kNN, let us apply this algorithm.</p>
<p>One method is to check who is his nearest neighbour. From the image, it is clear it is the Red Triangle family. So he is also added into Red Triangle. This method is called simply <strong>Nearest Neighbour</strong>, because classification depends only on the nearest neighbour.</p>
<p>But there is a problem with that. Red Triangle may be the nearest. But what if there are lot of Blue Squares near to him? Then Blue Squares have more strength in that locality than Red Triangle. So just checking nearest one is not sufficient. Instead we check some <cite>k</cite> nearest families. Then whoever is majority in them, the new guy belongs to that family. In our image, let&#8217;s take <cite>k=3</cite>, ie 3 nearest families. He has two Red and one Blue (there are two Blues equidistant, but since k=3, we take only one of them), so again he should be added to Red family. But what if we take <cite>k=7</cite>? Then he has 5 Blue families and 2 Red families. Great!! Now he should be added to Blue family. So it all changes with value of k. More funny thing is, what if <cite>k = 4</cite>? He has 2 Red and 2 Blue neighbours. It is a tie !!! So better take k as an odd number. So this method is called <strong>k-Nearest Neighbour</strong> since classification depends on k nearest neighbours.</p>
<p>Again, in kNN, it is true we are considering k neighbours, but we are giving equal importance to all, right? Is it justice? For example, take the case of <cite>k=4</cite>. We told it is a tie. But see, the 2 Red families are more closer to him than the other 2 Blue families. So he is more eligible to be added to Red. So how do we mathematically explain that? We give some weights to each family depending on their distance to the new-comer. For those who are near to him get higher weights while those are far away get lower weights. Then we add total weights of each family separately. Whoever gets highest total weights, new-comer goes to that family. This is called <strong>modified kNN</strong>.</p>
<p>So what are some important things you see here?</p>
<blockquote>
<div><ul class="simple">
<li>You need to have information about all the houses in town, right? Because, we have to check the distance from new-comer to all the existing houses to find the nearest neighbour. If there are plenty of houses and families, it takes lots of memory, and more time for calculation also.</li>
<li>There is almost zero time for any kind of training or preparation.</li>
</ul>
</div></blockquote>
<p>Now let&#8217;s see it in OpenCV.</p>
</div>
<div class="section" id="knn-in-opencv">
<h6>kNN in OpenCV<a class="headerlink" href="#knn-in-opencv" title="Permalink to this headline">¶</a></h6>
<p>We will do a simple example here, with two families (classes), just like above. Then in the next chapter, we will do much more better example.</p>
<p>So here, we label the Red family as <strong>Class-0</strong> (so denoted by 0) and Blue family as <strong>Class-1</strong> (denoted by 1). We create 25 families or 25 training data, and label them either Class-0 or Class-1. We do all these with the help of Random Number Generator in Numpy.</p>
<p>Then we plot it with the help of Matplotlib. Red families are shown as Red Triangles and Blue families are shown as Blue Squares.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Feature set containing (x,y) values of 25 known/training data</span>
<span class="n">trainData</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">100</span><span class="p">,(</span><span class="mi">25</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># Labels each one either Red or Blue with numbers 0 and 1</span>
<span class="n">responses</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,(</span><span class="mi">25</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># Take Red families and plot them</span>
<span class="n">red</span> <span class="o">=</span> <span class="n">trainData</span><span class="p">[</span><span class="n">responses</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span><span class="o">==</span><span class="mi">0</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">red</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">red</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="mi">80</span><span class="p">,</span><span class="s1">&#39;r&#39;</span><span class="p">,</span><span class="s1">&#39;^&#39;</span><span class="p">)</span>

<span class="c1"># Take Blue families and plot them</span>
<span class="n">blue</span> <span class="o">=</span> <span class="n">trainData</span><span class="p">[</span><span class="n">responses</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span><span class="o">==</span><span class="mi">1</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">blue</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">blue</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="mi">80</span><span class="p">,</span><span class="s1">&#39;b&#39;</span><span class="p">,</span><span class="s1">&#39;s&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>You will get something similar to our first image. Since you are using random number generator, you will be getting different data each time you run the code.</p>
<p>Next initiate the kNN algorithm and pass the <cite>trainData</cite> and <cite>responses</cite> to train the kNN (It constructs a search tree).</p>
<p>Then we will bring one new-comer and classify him to a family with the help of kNN in OpenCV. Before going to kNN, we need to know something on our test data (data of new comers). Our data should be a floating point array with size <img class="math" src="_images/math/fa91386d1c9236d3c8533f4d92d83c15d15a3ab1.png" alt="number \; of \; testdata \times number \; of \; features"/>. Then we find the nearest neighbours of new-comer. We can specify how many neighbours we want. It returns:</p>
<blockquote>
<div><ol class="arabic simple">
<li>The label given to new-comer depending upon the kNN theory we saw earlier. If you want Nearest Neighbour algorithm, just specify <cite>k=1</cite> where k is the number of neighbours.</li>
<li>The labels of k-Nearest Neighbours.</li>
<li>Corresponding distances from new-comer to each nearest neighbour.</li>
</ol>
</div></blockquote>
<p>So let&#8217;s see how it works. New comer is marked in green color.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">newcomer</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">100</span><span class="p">,(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">newcomer</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">newcomer</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="mi">80</span><span class="p">,</span><span class="s1">&#39;g&#39;</span><span class="p">,</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>

<span class="n">knn</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">KNearest</span><span class="p">()</span>
<span class="n">knn</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">trainData</span><span class="p">,</span><span class="n">responses</span><span class="p">)</span>
<span class="n">ret</span><span class="p">,</span> <span class="n">results</span><span class="p">,</span> <span class="n">neighbours</span> <span class="p">,</span><span class="n">dist</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">find_nearest</span><span class="p">(</span><span class="n">newcomer</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="nb">print</span> <span class="s2">&quot;result: &quot;</span><span class="p">,</span> <span class="n">results</span><span class="p">,</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
<span class="nb">print</span> <span class="s2">&quot;neighbours: &quot;</span><span class="p">,</span> <span class="n">neighbours</span><span class="p">,</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
<span class="nb">print</span> <span class="s2">&quot;distance: &quot;</span><span class="p">,</span> <span class="n">dist</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>I got the result as follows:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">result</span><span class="p">:</span>  <span class="p">[[</span> <span class="mf">1.</span><span class="p">]]</span>
<span class="n">neighbours</span><span class="p">:</span>  <span class="p">[[</span> <span class="mf">1.</span>  <span class="mf">1.</span>  <span class="mf">1.</span><span class="p">]]</span>
<span class="n">distance</span><span class="p">:</span>  <span class="p">[[</span> <span class="mf">53.</span>  <span class="mf">58.</span>  <span class="mf">61.</span><span class="p">]]</span>
</pre></div>
</div>
<p>It says our new-comer got 3 neighbours, all from Blue family. Therefore, he is labelled as Blue family. It is obvious from plot below:</p>
<blockquote>
<div><img alt="kNN Demo" class="align-center" src="_images/knn_simple.png" />
</div></blockquote>
<p>If you have large number of data, you can just pass it as array. Corresponding results are also obtained as arrays.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="c1"># 10 new comers</span>
<span class="n">newcomers</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">100</span><span class="p">,(</span><span class="mi">10</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">ret</span><span class="p">,</span> <span class="n">results</span><span class="p">,</span><span class="n">neighbours</span><span class="p">,</span><span class="n">dist</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">find_nearest</span><span class="p">(</span><span class="n">newcomer</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="c1"># The results also will contain 10 labels.</span>
</pre></div>
</div>
</div>
<div class="section" id="additional-resources">
<h6>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h6>
<ol class="arabic simple">
<li><a class="reference external" href="http://www.nptel.iitm.ac.in/courses/106108057/12">NPTEL notes on Pattern Recognition, Chapter 11</a></li>
</ol>
</div>
<div class="section" id="exercises">
<h6>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h6>
</div>
</div>
<span id="document-py_tutorials/py_ml/py_knn/py_knn_opencv/py_knn_opencv"></span><div class="section" id="ocr-of-hand-written-data-using-knn">
<span id="knn-opencv"></span><h5>OCR of Hand-written Data using kNN<a class="headerlink" href="#ocr-of-hand-written-data-using-knn" title="Permalink to this headline">¶</a></h5>
<div class="section" id="goal">
<h6>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h6>
<dl class="docutils">
<dt>In this chapter</dt>
<dd><ul class="first last simple">
<li>We will use our knowledge on kNN to build a basic OCR application.</li>
<li>We will try with Digits and Alphabets data available that comes with OpenCV.</li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="ocr-of-hand-written-digits">
<h6>OCR of Hand-written Digits<a class="headerlink" href="#ocr-of-hand-written-digits" title="Permalink to this headline">¶</a></h6>
<p>Our goal is to build an application which can read the handwritten digits. For this we need some train_data and test_data. OpenCV comes with an image <cite>digits.png</cite> (in the folder <code class="docutils literal"><span class="pre">opencv/samples/python2/data/</span></code>) which has 5000 handwritten digits (500 for each digit). Each digit is a 20x20 image. So our first step is to split this image into 5000 different digits. For each digit, we flatten it into a single row with 400 pixels. That is our feature set, ie intensity values of all pixels. It is the simplest feature set we can create. We use first 250 samples of each digit as train_data, and next 250 samples as test_data. So let&#8217;s prepare them first.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;digits.png&#39;</span><span class="p">)</span>
<span class="n">gray</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2GRAY</span><span class="p">)</span>

<span class="c1"># Now we split the image to 5000 cells, each 20x20 size</span>
<span class="n">cells</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">hsplit</span><span class="p">(</span><span class="n">row</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">vsplit</span><span class="p">(</span><span class="n">gray</span><span class="p">,</span><span class="mi">50</span><span class="p">)]</span>

<span class="c1"># Make it into a Numpy array. It size will be (50,100,20,20)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">cells</span><span class="p">)</span>

<span class="c1"># Now we prepare train_data and test_data.</span>
<span class="n">train</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,:</span><span class="mi">50</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">400</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="c1"># Size = (2500,400)</span>
<span class="n">test</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span><span class="mi">50</span><span class="p">:</span><span class="mi">100</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">400</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="c1"># Size = (2500,400)</span>

<span class="c1"># Create labels for train and test data</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">train_labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">k</span><span class="p">,</span><span class="mi">250</span><span class="p">)[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
<span class="n">test_labels</span> <span class="o">=</span> <span class="n">train_labels</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="c1"># Initiate kNN, train the data, then test it with test data for k=1</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">KNearest</span><span class="p">()</span>
<span class="n">knn</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">train</span><span class="p">,</span><span class="n">train_labels</span><span class="p">)</span>
<span class="n">ret</span><span class="p">,</span><span class="n">result</span><span class="p">,</span><span class="n">neighbours</span><span class="p">,</span><span class="n">dist</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">find_nearest</span><span class="p">(</span><span class="n">test</span><span class="p">,</span><span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># Now we check the accuracy of classification</span>
<span class="c1"># For that, compare the result with test_labels and check which are wrong</span>
<span class="n">matches</span> <span class="o">=</span> <span class="n">result</span><span class="o">==</span><span class="n">test_labels</span>
<span class="n">correct</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">count_nonzero</span><span class="p">(</span><span class="n">matches</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">correct</span><span class="o">*</span><span class="mf">100.0</span><span class="o">/</span><span class="n">result</span><span class="o">.</span><span class="n">size</span>
<span class="nb">print</span> <span class="n">accuracy</span>
</pre></div>
</div>
<p>So our basic OCR app is ready. This particular example gave me an accuracy of 91%. One option improve accuracy is to add more data for training, especially the wrong ones. So instead of finding this training data everytime I start application, I better save it, so that next time, I directly read this data from a file and start classification. You can do it with the help of some Numpy functions like np.savetxt, np.savez, np.load etc. Please check their docs for more details.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="c1"># save the data</span>
<span class="n">np</span><span class="o">.</span><span class="n">savez</span><span class="p">(</span><span class="s1">&#39;knn_data.npz&#39;</span><span class="p">,</span><span class="n">train</span><span class="o">=</span><span class="n">train</span><span class="p">,</span> <span class="n">train_labels</span><span class="o">=</span><span class="n">train_labels</span><span class="p">)</span>

<span class="c1"># Now load the data</span>
<span class="k">with</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;knn_data.npz&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">data</span><span class="p">:</span>
    <span class="nb">print</span> <span class="n">data</span><span class="o">.</span><span class="n">files</span>
    <span class="n">train</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span>
    <span class="n">train_labels</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;train_labels&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>In my system, it takes around 4.4 MB of memory. Since we are using intensity values (uint8 data) as features, it would be better to convert the data to np.uint8 first and then save it. It takes only 1.1 MB in this case. Then while loading, you can convert back into float32.</p>
</div>
<div class="section" id="ocr-of-english-alphabets">
<h6>OCR of English Alphabets<a class="headerlink" href="#ocr-of-english-alphabets" title="Permalink to this headline">¶</a></h6>
<p>Next we will do the same for English alphabets, but there is a slight change in data and feature set. Here, instead of images, OpenCV comes with a data file, <code class="docutils literal"><span class="pre">letter-recognition.data</span></code> in <code class="docutils literal"><span class="pre">opencv/samples/cpp/</span></code> folder. If you open it, you will see 20000 lines which may, on first sight, look like garbage. Actually, in each row, first column is an alphabet which is our label. Next 16 numbers following it are its different features. These features are obtained from <a class="reference external" href="http://archive.ics.uci.edu/ml/">UCI Machine Learning Repository</a>. You can find the details of these features in <a class="reference external" href="http://archive.ics.uci.edu/ml/datasets/Letter+Recognition">this page</a>.</p>
<p>There are 20000 samples available, so we take first 10000 data as training samples and remaining 10000 as test samples. We should change the alphabets to ascii characters because we can&#8217;t work with alphabets directly.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Load the data, converters convert the letter to a number</span>
<span class="n">data</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s1">&#39;letter-recognition.data&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span> <span class="s1">&#39;float32&#39;</span><span class="p">,</span> <span class="n">delimiter</span> <span class="o">=</span> <span class="s1">&#39;,&#39;</span><span class="p">,</span>
                    <span class="n">converters</span><span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">ch</span><span class="p">:</span> <span class="nb">ord</span><span class="p">(</span><span class="n">ch</span><span class="p">)</span><span class="o">-</span><span class="nb">ord</span><span class="p">(</span><span class="s1">&#39;A&#39;</span><span class="p">)})</span>

<span class="c1"># split the data to two, 10000 each for train and test</span>
<span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vsplit</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># split trainData and testData to features and responses</span>
<span class="n">responses</span><span class="p">,</span> <span class="n">trainData</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hsplit</span><span class="p">(</span><span class="n">train</span><span class="p">,[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">labels</span><span class="p">,</span> <span class="n">testData</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hsplit</span><span class="p">(</span><span class="n">test</span><span class="p">,[</span><span class="mi">1</span><span class="p">])</span>

<span class="c1"># Initiate the kNN, classify, measure accuracy.</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">KNearest</span><span class="p">()</span>
<span class="n">knn</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">trainData</span><span class="p">,</span> <span class="n">responses</span><span class="p">)</span>
<span class="n">ret</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span> <span class="n">neighbours</span><span class="p">,</span> <span class="n">dist</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">find_nearest</span><span class="p">(</span><span class="n">testData</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="n">correct</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">count_nonzero</span><span class="p">(</span><span class="n">result</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">correct</span><span class="o">*</span><span class="mf">100.0</span><span class="o">/</span><span class="mi">10000</span>
<span class="nb">print</span> <span class="n">accuracy</span>
</pre></div>
</div>
<p>It gives me an accuracy of 93.22%. Again, if you want to increase accuracy, you can iteratively add error data in each level.</p>
</div>
<div class="section" id="additional-resources">
<h6>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h6>
</div>
<div class="section" id="exercises">
<h6>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h6>
</div>
</div>
</div>
</div>
<span id="document-py_tutorials/py_ml/py_svm/py_svm_index"></span><div class="section" id="support-vector-machines-svm">
<span id="svm"></span><h4>Support Vector Machines (SVM)<a class="headerlink" href="#support-vector-machines-svm" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><a class="reference internal" href="index.html#svm-understanding"><span class="std std-ref">Understanding SVM</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="14%" />
<col width="86%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/svm_icon1.jpg"><img alt="SVM_1" src="_images/svm_icon1.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Get a basic understanding of what SVM is</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#svm-opencv"><span class="std std-ref">OCR of Hand-written Data using SVM</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="14%" />
<col width="86%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/svm_icon2.jpg"><img alt="SVM_2" src="_images/svm_icon2.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Let&#8217;s use SVM functionalities in OpenCV</td>
</tr>
</tbody>
</table>
</div></blockquote>
<div class="toctree-wrapper compound">
<span id="document-py_tutorials/py_ml/py_svm/py_svm_basics/py_svm_basics"></span><div class="section" id="understanding-svm">
<span id="svm-understanding"></span><h5>Understanding SVM<a class="headerlink" href="#understanding-svm" title="Permalink to this headline">¶</a></h5>
<div class="section" id="goal">
<h6>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h6>
<dl class="docutils">
<dt>In this chapter</dt>
<dd><ul class="first last simple">
<li>We will see an intuitive understanding of SVM</li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="theory">
<h6>Theory<a class="headerlink" href="#theory" title="Permalink to this headline">¶</a></h6>
<div class="section" id="linearly-separable-data">
<h7>Linearly Separable Data<a class="headerlink" href="#linearly-separable-data" title="Permalink to this headline">¶</a></h7>
<p>Consider the image below which has two types of data, red and blue. In kNN, for a test data, we used to measure its distance to all the training samples and take the one with minimum distance. It takes plenty of time to measure all the distances and plenty of memory to store all the training-samples. But considering the data given in image, should we need that much?</p>
<blockquote>
<div><img alt="Test Data" class="align-center" src="_images/svm_basics1.png" />
</div></blockquote>
<p>Consider another idea. We find a line, <img class="math" src="_images/math/defd2cca427cca8e7e530aee7d8d9a195ed1443c.png" alt="f(x)=ax_1+bx_2+c"/> which divides both the data to two regions. When we get a new test_data <img class="math" src="_images/math/f026aecf11ec7f6141ab863f260d395f94b10f51.png" alt="X"/>, just substitute it in <img class="math" src="_images/math/14546c27a7b929642f7840acca5f851c503ea109.png" alt="f(x)"/>. If <img class="math" src="_images/math/fd263053d15fad3cfbfa097f59dc9784f96ef2cc.png" alt="f(X) &gt; 0"/>, it belongs to blue group, else it belongs to red group. We can call this line as <strong>Decision Boundary</strong>. It is very simple and memory-efficient. Such data which can be divided into two with a straight line (or hyperplanes in higher dimensions) is called <strong>Linear Separable</strong>.</p>
<p>So in above image, you can see plenty of such lines are possible. Which one we will take? Very intuitively we can say that the line should be passing as far as possible from all the points. Why? Because there can be noise in the incoming data. This data should not affect the classification accuracy. So taking a farthest line will provide more immunity against noise. So what SVM does is to find a straight line (or hyperplane) with largest minimum distance to the training samples. See the bold line in below image passing through the center.</p>
<blockquote>
<div><img alt="Decision Boundary" class="align-center" src="_images/svm_basics2.png" />
</div></blockquote>
<p>So to find this Decision Boundary, you need training data. Do you need all? NO. Just the ones which are close to the opposite group are sufficient. In our image, they are the one blue filled circle and two red filled squares. We can call them <strong>Support Vectors</strong> and the lines passing through them are called <strong>Support Planes</strong>. They are adequate for finding our decision boundary. We need not worry about all the data. It helps in data reduction.</p>
<p>What happened is, first two hyperplanes are found which best represents the data. For eg, blue data is represented by <img class="math" src="_images/math/0a2b5a00960d79e8aedff79d0599eb3971c0634c.png" alt="w^Tx+b_0 &gt; 1"/> while red data is represented by <img class="math" src="_images/math/0bc08366b183df95be0149bf3c39744c0414772c.png" alt="w^Tx+b_0 &lt; -1"/> where <img class="math" src="_images/math/8659700e6646cd91bc02c32affaa5ec046ee9935.png" alt="w"/> is <strong>weight vector</strong> ( <img class="math" src="_images/math/3f85eb04600da2130fdd6b9a08a01437b9e9646a.png" alt="w=[w_1, w_2,..., w_n]"/>) and <img class="math" src="_images/math/188c175aac0a8a9c22499336711b5d7256407254.png" alt="x"/> is the feature vector (<img class="math" src="_images/math/2191d7168dbb9daf9d46b22e43b81bb2e5f9c401.png" alt="x = [x_1,x_2,..., x_n]"/>). <img class="math" src="_images/math/7941b619fd3246c1e7c3956d0ae9e9613ecc1786.png" alt="b_0"/> is the <strong>bias</strong>. Weight vector decides the orientation of decision boundary while bias point decides its location. Now decision boundary is defined to be midway between these hyperplanes, so expressed as <img class="math" src="_images/math/46e677f9dfa973ad57c5935369089d66e20ff45b.png" alt="w^Tx+b_0 = 0"/>. The minimum distance from support vector to the decision boundary is given by, <img class="math" src="_images/math/d4823e1e4a4cad05aef9868fad023cf3fa0c9b3d.png" alt="distance_{support \, vectors}=\frac{1}{||w||}"/>. Margin is twice this distance, and we need to maximize this margin. i.e. we need to minimize a new function <img class="math" src="_images/math/93e93448145c658da9cd5365532d3ea55b59b959.png" alt="L(w, b_0)"/> with some constraints which can expressed below:</p>
<div class="math">
<p><img src="_images/math/d214595aa2f88c7bd72d9c20507d598427d51842.png" alt="\min_{w, b_0} L(w, b_0) = \frac{1}{2}||w||^2 \; \text{subject to} \; t_i(w^Tx+b_0) \geq 1 \; \forall i"/></p>
</div><p>where <img class="math" src="_images/math/64283d3d8caf93093b91aba9a61dbe26b5c18562.png" alt="t_i"/> is the label of each class, <img class="math" src="_images/math/6513fc85b1daf439bb5a4ab1ea313ae91358bb41.png" alt="t_i \in [-1,1]"/>.</p>
</div>
<div class="section" id="non-linearly-separable-data">
<h7>Non-Linearly Separable Data<a class="headerlink" href="#non-linearly-separable-data" title="Permalink to this headline">¶</a></h7>
<p>Consider some data which can&#8217;t be divided into two with a straight line. For example, consider an one-dimensional data where &#8216;X&#8217; is at -3 &amp; +3 and &#8216;O&#8217; is at -1 &amp; +1. Clearly it is not linearly separable. But there are methods to solve these kinds of problems. If we can map this data set with a function, <img class="math" src="_images/math/bbbb5b7f3b2d4f5c8b1ebb5bfa76910615b77b62.png" alt="f(x) = x^2"/>, we get &#8216;X&#8217; at 9 and &#8216;O&#8217; at 1 which are linear separable.</p>
<p>Otherwise we can convert this one-dimensional to two-dimensional data. We can use <img class="math" src="_images/math/0d3e894aee659a08fd696073243f70cdfee621e5.png" alt="f(x)=(x,x^2)"/> function to map this data. Then &#8216;X&#8217; becomes (-3,9) and (3,9) while &#8216;O&#8217; becomes (-1,1) and (1,1). This is also linear separable. In short, chance is more for a non-linear separable data in lower-dimensional space to become linear separable in higher-dimensional space.</p>
<p>In general, it is possible to map points in a d-dimensional space to some D-dimensional space <img class="math" src="_images/math/655ed998f6a937b74670c3c416a304dabe6f0a73.png" alt="(D&gt;d)"/> to check the possibility of linear separability. There is an idea which helps to compute the dot product in the high-dimensional (kernel) space by performing computations in the low-dimensional input (feature) space. We can illustrate with following example.</p>
<p>Consider two points in two-dimensional space, <img class="math" src="_images/math/5e41538f74ff4225571880938b0d1f45c236cbb9.png" alt="p=(p_1,p_2)"/> and <img class="math" src="_images/math/675cd1c9c577b20204dc3969b7e41e6bc89551ee.png" alt="q=(q_1,q_2)"/>. Let <img class="math" src="_images/math/10e009bdb83f96c5f47c58b34d5d4b12ef268d5b.png" alt="\phi"/> be a mapping function which maps a two-dimensional point to three-dimensional space as follows:</p>
<div class="math">
<p><img src="_images/math/477e92201a7a40c09c9d904068c027191266bda9.png" alt="\phi (p) = (p_{1}^2,p_{2}^2,\sqrt{2} p_1 p_2)
\phi (q) = (q_{1}^2,q_{2}^2,\sqrt{2} q_1 q_2)"/></p>
</div><p>Let us define a kernel function <img class="math" src="_images/math/e0f2fc8c7c4d6fec9bcd73656c0c1ebc34007475.png" alt="K(p,q)"/> which does a dot product between two points, shown below:</p>
<div class="math">
<p><img src="_images/math/9bac51fbba3a7bd6c1c32e3d0552bb2a8c4f9d67.png" alt="K(p,q)  = \phi(p).\phi(q) &amp;= \phi(p)^T , \phi(q) \\
                          &amp;= (p_{1}^2,p_{2}^2,\sqrt{2} p_1 p_2).(q_{1}^2,q_{2}^2,\sqrt{2} q_1 q_2) \\
                          &amp;= p_{1}^2 q_{1}^2 + p_{2}^2 q_{2}^2 + 2 p_1 q_1 p_2 q_2 \\
                          &amp;= (p_1 q_1 + p_2 q_2)^2 \\
          \phi(p).\phi(q) &amp;= (p.q)^2"/></p>
</div><p>It means, a dot product in three-dimensional space can be achieved using squared dot product in two-dimensional space. This can be applied to higher dimensional space. So we can calculate higher dimensional features from lower dimensions itself. Once we map them, we get a higher dimensional space.</p>
<p>In addition to all these concepts, there comes the problem of misclassification. So just finding decision boundary with maximum margin is not sufficient. We need to consider the problem of misclassification errors also. Sometimes, it may be possible to find a decision boundary with less margin, but with reduced misclassification. Anyway we need to modify our model such that it should find decision boundary with maximum margin, but with less misclassification. The minimization criteria is modified as:</p>
<div class="math">
<p><img src="_images/math/b9e565737fba6f232f2cfa8f4e18755457fdf8e6.png" alt="min \; ||w||^2 + C(distance \; of \; misclassified \; samples \; to \; their \; correct \; regions)"/></p>
</div><p>Below image shows this concept. For each sample of the training data a new parameter <img class="math" src="_images/math/8e2fad164dd6987a25bd3f9fe9ede7006e5f060d.png" alt="\xi_i"/> is defined. It is the distance from its corresponding training sample to their correct decision region. For those who are not misclassified, they fall on their corresponding support planes, so their distance is zero.</p>
<blockquote>
<div><img alt="Misclassification" class="align-center" src="_images/svm_basics3.png" />
</div></blockquote>
<p>So the new optimization problem is :</p>
<div class="math">
<p><img src="_images/math/a43b1daf45a2c36ab8cf60c66f1d5e50e4afbde5.png" alt="\min_{w, b_{0}} L(w,b_0) = ||w||^{2} + C \sum_{i} {\xi_{i}} \text{ subject to } y_{i}(w^{T} x_{i} + b_{0}) \geq 1 - \xi_{i} \text{ and } \xi_{i} \geq 0 \text{ } \forall i"/></p>
</div><p>How should the parameter C be chosen? It is obvious that the answer to this question depends on how the training data is distributed. Although there is no general answer, it is useful to take into account these rules:</p>
<blockquote>
<div><ul class="simple">
<li>Large values of C give solutions with less misclassification errors but a smaller margin. Consider that in this case it is expensive to make misclassification errors. Since the aim of the optimization is to minimize the argument, few misclassifications errors are allowed.</li>
<li>Small values of C give solutions with bigger margin and more classification errors. In this case the minimization does not consider that much the term of the sum so it focuses more on finding a hyperplane with big margin.</li>
</ul>
</div></blockquote>
</div>
</div>
<div class="section" id="additional-resources">
<h6>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h6>
<ol class="arabic simple">
<li><a class="reference external" href="http://www.nptel.iitm.ac.in/courses/106108057/26">NPTEL notes on Statistical Pattern Recognition, Chapters 25-29</a>.</li>
</ol>
</div>
<div class="section" id="exercises">
<h6>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h6>
</div>
</div>
<span id="document-py_tutorials/py_ml/py_svm/py_svm_opencv/py_svm_opencv"></span><div class="section" id="ocr-of-hand-written-data-using-svm">
<span id="svm-opencv"></span><h5>OCR of Hand-written Data using SVM<a class="headerlink" href="#ocr-of-hand-written-data-using-svm" title="Permalink to this headline">¶</a></h5>
<div class="section" id="goal">
<h6>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h6>
<p>In this chapter</p>
<blockquote>
<div><ul class="simple">
<li>We will revisit the hand-written data OCR, but, with SVM instead of kNN.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="ocr-of-hand-written-digits">
<h6>OCR of Hand-written Digits<a class="headerlink" href="#ocr-of-hand-written-digits" title="Permalink to this headline">¶</a></h6>
<p>In kNN, we directly used pixel intensity as the feature vector. This time we will use <a class="reference external" href="http://en.wikipedia.org/wiki/Histogram_of_oriented_gradients">Histogram of Oriented Gradients</a> (HOG) as feature vectors.</p>
<p>Here, before finding the HOG, we deskew the image using its second order moments. So we first define a function <strong>deskew()</strong> which takes a digit image and deskew it. Below is the deskew() function:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">deskew</span><span class="p">(</span><span class="n">img</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">moments</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">m</span><span class="p">[</span><span class="s1">&#39;mu02&#39;</span><span class="p">])</span> <span class="o">&lt;</span> <span class="mf">1e-2</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">img</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">skew</span> <span class="o">=</span> <span class="n">m</span><span class="p">[</span><span class="s1">&#39;mu11&#39;</span><span class="p">]</span><span class="o">/</span><span class="n">m</span><span class="p">[</span><span class="s1">&#39;mu02&#39;</span><span class="p">]</span>
    <span class="n">M</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="n">skew</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="o">*</span><span class="n">SZ</span><span class="o">*</span><span class="n">skew</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">warpAffine</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">M</span><span class="p">,(</span><span class="n">SZ</span><span class="p">,</span> <span class="n">SZ</span><span class="p">),</span><span class="n">flags</span><span class="o">=</span><span class="n">affine_flags</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">img</span>
</pre></div>
</div>
<p>Below image shows above deskew function applied to an image of zero. Left image is the original image and right image is the deskewed image.</p>
<blockquote>
<div><img alt="Deskew" class="align-center" src="_images/deskew.jpg" />
</div></blockquote>
<p>Next we have to find the HOG Descriptor of each cell. For that, we find Sobel derivatives of each cell in X and Y direction. Then find their magnitude and direction of gradient at each pixel. This gradient is quantized to 16 integer values. Divide this image to four sub-squares. For each sub-square, calculate the histogram of direction (16 bins) weighted with their magnitude. So each sub-square gives you a vector containing 16 values. Four such vectors (of four sub-squares) together gives us a feature vector containing 64 values. This is the feature vector we use to train our data.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">hog</span><span class="p">(</span><span class="n">img</span><span class="p">):</span>
    <span class="n">gx</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">Sobel</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">CV_32F</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">gy</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">Sobel</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">CV_32F</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">mag</span><span class="p">,</span> <span class="n">ang</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cartToPolar</span><span class="p">(</span><span class="n">gx</span><span class="p">,</span> <span class="n">gy</span><span class="p">)</span>

    <span class="c1"># quantizing binvalues in (0...16)</span>
    <span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">(</span><span class="n">bin_n</span><span class="o">*</span><span class="n">ang</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">))</span>

    <span class="c1"># Divide to 4 sub-squares</span>
    <span class="n">bin_cells</span> <span class="o">=</span> <span class="n">bins</span><span class="p">[:</span><span class="mi">10</span><span class="p">,:</span><span class="mi">10</span><span class="p">],</span> <span class="n">bins</span><span class="p">[</span><span class="mi">10</span><span class="p">:,:</span><span class="mi">10</span><span class="p">],</span> <span class="n">bins</span><span class="p">[:</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">:],</span> <span class="n">bins</span><span class="p">[</span><span class="mi">10</span><span class="p">:,</span><span class="mi">10</span><span class="p">:]</span>
    <span class="n">mag_cells</span> <span class="o">=</span> <span class="n">mag</span><span class="p">[:</span><span class="mi">10</span><span class="p">,:</span><span class="mi">10</span><span class="p">],</span> <span class="n">mag</span><span class="p">[</span><span class="mi">10</span><span class="p">:,:</span><span class="mi">10</span><span class="p">],</span> <span class="n">mag</span><span class="p">[:</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">:],</span> <span class="n">mag</span><span class="p">[</span><span class="mi">10</span><span class="p">:,</span><span class="mi">10</span><span class="p">:]</span>
    <span class="n">hists</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">m</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">bin_n</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">bin_cells</span><span class="p">,</span> <span class="n">mag_cells</span><span class="p">)]</span>
    <span class="n">hist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">(</span><span class="n">hists</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">hist</span>
</pre></div>
</div>
<p>Finally, as in the previous case, we start by splitting our big dataset into individual cells. For every digit, 250 cells are reserved for training data and remaining 250 data is reserved for testing. Full code is given below:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">SZ</span><span class="o">=</span><span class="mi">20</span>
<span class="n">bin_n</span> <span class="o">=</span> <span class="mi">16</span> <span class="c1"># Number of bins</span>

<span class="n">svm_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span> <span class="n">kernel_type</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">SVM_LINEAR</span><span class="p">,</span>
                    <span class="n">svm_type</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">SVM_C_SVC</span><span class="p">,</span>
                    <span class="n">C</span><span class="o">=</span><span class="mf">2.67</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">5.383</span> <span class="p">)</span>

<span class="n">affine_flags</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">WARP_INVERSE_MAP</span><span class="o">|</span><span class="n">cv2</span><span class="o">.</span><span class="n">INTER_LINEAR</span>

<span class="k">def</span> <span class="nf">deskew</span><span class="p">(</span><span class="n">img</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">moments</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">m</span><span class="p">[</span><span class="s1">&#39;mu02&#39;</span><span class="p">])</span> <span class="o">&lt;</span> <span class="mf">1e-2</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">img</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">skew</span> <span class="o">=</span> <span class="n">m</span><span class="p">[</span><span class="s1">&#39;mu11&#39;</span><span class="p">]</span><span class="o">/</span><span class="n">m</span><span class="p">[</span><span class="s1">&#39;mu02&#39;</span><span class="p">]</span>
    <span class="n">M</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="n">skew</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="o">*</span><span class="n">SZ</span><span class="o">*</span><span class="n">skew</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">warpAffine</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">M</span><span class="p">,(</span><span class="n">SZ</span><span class="p">,</span> <span class="n">SZ</span><span class="p">),</span><span class="n">flags</span><span class="o">=</span><span class="n">affine_flags</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">img</span>

<span class="k">def</span> <span class="nf">hog</span><span class="p">(</span><span class="n">img</span><span class="p">):</span>
    <span class="n">gx</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">Sobel</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">CV_32F</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">gy</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">Sobel</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">CV_32F</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">mag</span><span class="p">,</span> <span class="n">ang</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cartToPolar</span><span class="p">(</span><span class="n">gx</span><span class="p">,</span> <span class="n">gy</span><span class="p">)</span>
    <span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">(</span><span class="n">bin_n</span><span class="o">*</span><span class="n">ang</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">))</span>    <span class="c1"># quantizing binvalues in (0...16)</span>
    <span class="n">bin_cells</span> <span class="o">=</span> <span class="n">bins</span><span class="p">[:</span><span class="mi">10</span><span class="p">,:</span><span class="mi">10</span><span class="p">],</span> <span class="n">bins</span><span class="p">[</span><span class="mi">10</span><span class="p">:,:</span><span class="mi">10</span><span class="p">],</span> <span class="n">bins</span><span class="p">[:</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">:],</span> <span class="n">bins</span><span class="p">[</span><span class="mi">10</span><span class="p">:,</span><span class="mi">10</span><span class="p">:]</span>
    <span class="n">mag_cells</span> <span class="o">=</span> <span class="n">mag</span><span class="p">[:</span><span class="mi">10</span><span class="p">,:</span><span class="mi">10</span><span class="p">],</span> <span class="n">mag</span><span class="p">[</span><span class="mi">10</span><span class="p">:,:</span><span class="mi">10</span><span class="p">],</span> <span class="n">mag</span><span class="p">[:</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">:],</span> <span class="n">mag</span><span class="p">[</span><span class="mi">10</span><span class="p">:,</span><span class="mi">10</span><span class="p">:]</span>
    <span class="n">hists</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">m</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">bin_n</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">bin_cells</span><span class="p">,</span> <span class="n">mag_cells</span><span class="p">)]</span>
    <span class="n">hist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">(</span><span class="n">hists</span><span class="p">)</span>     <span class="c1"># hist is a 64 bit vector</span>
    <span class="k">return</span> <span class="n">hist</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;digits.png&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>

<span class="n">cells</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">hsplit</span><span class="p">(</span><span class="n">row</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">vsplit</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="mi">50</span><span class="p">)]</span>

<span class="c1"># First half is trainData, remaining is testData</span>
<span class="n">train_cells</span> <span class="o">=</span> <span class="p">[</span> <span class="n">i</span><span class="p">[:</span><span class="mi">50</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">cells</span> <span class="p">]</span>
<span class="n">test_cells</span> <span class="o">=</span> <span class="p">[</span> <span class="n">i</span><span class="p">[</span><span class="mi">50</span><span class="p">:]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">cells</span><span class="p">]</span>

<span class="c1">######     Now training      ########################</span>

<span class="n">deskewed</span> <span class="o">=</span> <span class="p">[</span><span class="nb">map</span><span class="p">(</span><span class="n">deskew</span><span class="p">,</span><span class="n">row</span><span class="p">)</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">train_cells</span><span class="p">]</span>
<span class="n">hogdata</span> <span class="o">=</span> <span class="p">[</span><span class="nb">map</span><span class="p">(</span><span class="n">hog</span><span class="p">,</span><span class="n">row</span><span class="p">)</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">deskewed</span><span class="p">]</span>
<span class="n">trainData</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">hogdata</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">64</span><span class="p">)</span>
<span class="n">responses</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span><span class="mi">250</span><span class="p">)[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])</span>

<span class="n">svm</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">SVM</span><span class="p">()</span>
<span class="n">svm</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">trainData</span><span class="p">,</span><span class="n">responses</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">svm_params</span><span class="p">)</span>
<span class="n">svm</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;svm_data.dat&#39;</span><span class="p">)</span>

<span class="c1">######     Now testing      ########################</span>

<span class="n">deskewed</span> <span class="o">=</span> <span class="p">[</span><span class="nb">map</span><span class="p">(</span><span class="n">deskew</span><span class="p">,</span><span class="n">row</span><span class="p">)</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">test_cells</span><span class="p">]</span>
<span class="n">hogdata</span> <span class="o">=</span> <span class="p">[</span><span class="nb">map</span><span class="p">(</span><span class="n">hog</span><span class="p">,</span><span class="n">row</span><span class="p">)</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">deskewed</span><span class="p">]</span>
<span class="n">testData</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">hogdata</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">bin_n</span><span class="o">*</span><span class="mi">4</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">predict_all</span><span class="p">(</span><span class="n">testData</span><span class="p">)</span>

<span class="c1">#######   Check Accuracy   ########################</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">result</span><span class="o">==</span><span class="n">responses</span>
<span class="n">correct</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">count_nonzero</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>
<span class="nb">print</span> <span class="n">correct</span><span class="o">*</span><span class="mf">100.0</span><span class="o">/</span><span class="n">result</span><span class="o">.</span><span class="n">size</span>
</pre></div>
</div>
<p>This particular technique gave me nearly 94% accuracy. You can try different values for various parameters of SVM to check if higher accuracy is possible. Or you can read technical papers on this area and try to implement them.</p>
</div>
<div class="section" id="additional-resources">
<h6>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h6>
<ol class="arabic simple">
<li><a class="reference external" href="www.youtube.com/watch?v=0Zib1YEE4LU‎">Histograms of Oriented Gradients Video</a></li>
</ol>
</div>
<div class="section" id="exercises">
<h6>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h6>
<ol class="arabic simple">
<li>OpenCV samples contain <code class="docutils literal"><span class="pre">digits.py</span></code> which applies a slight improvement of the above method to get improved result. It also contains the reference. Check it and understand it.</li>
</ol>
</div>
</div>
</div>
</div>
<span id="document-py_tutorials/py_ml/py_kmeans/py_kmeans_index"></span><div class="section" id="k-means-clustering">
<span id="kmeans-clustering"></span><h4>K-Means Clustering<a class="headerlink" href="#k-means-clustering" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><a class="reference internal" href="index.html#kmeans-clustering-understanding"><span class="std std-ref">Understanding K-Means Clustering</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="14%" />
<col width="86%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/kmeans_begin.jpg"><img alt="KM_1" src="_images/kmeans_begin.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Read to get an intuitive understanding of K-Means Clustering</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#kmeans-opencv"><span class="std std-ref">K-Means Clustering in OpenCV</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="14%" />
<col width="86%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/kmeans_demo.jpg"><img alt="KM_2" src="_images/kmeans_demo.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Now let&#8217;s try K-Means functions in OpenCV</td>
</tr>
</tbody>
</table>
</div></blockquote>
<div class="toctree-wrapper compound">
<span id="document-py_tutorials/py_ml/py_kmeans/py_kmeans_understanding/py_kmeans_understanding"></span><div class="section" id="understanding-k-means-clustering">
<span id="kmeans-clustering-understanding"></span><h5>Understanding K-Means Clustering<a class="headerlink" href="#understanding-k-means-clustering" title="Permalink to this headline">¶</a></h5>
<div class="section" id="goal">
<h6>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h6>
<p>In this chapter, we will understand the concepts of K-Means Clustering, how it works etc.</p>
</div>
<div class="section" id="theory">
<h6>Theory<a class="headerlink" href="#theory" title="Permalink to this headline">¶</a></h6>
<p>We will deal this with an example which is commonly used.</p>
<div class="section" id="t-shirt-size-problem">
<h7>T-shirt size problem<a class="headerlink" href="#t-shirt-size-problem" title="Permalink to this headline">¶</a></h7>
<p>Consider a company, which is going to release a new model of T-shirt to market. Obviously they will have to manufacture models in different sizes to satisfy people of all sizes. So the company make a data of people&#8217;s height and weight, and plot them on to a graph, as below:</p>
<blockquote>
<div><img alt="T-shirt Problem" class="align-center" src="_images/tshirt.jpg" />
</div></blockquote>
<p>Company can&#8217;t create t-shirts with all the sizes. Instead, they divide people to Small, Medium and Large, and manufacture only these 3 models which will fit into all the people. This grouping of people into three groups can be done by k-means clustering, and algorithm provides us best 3 sizes, which will satisfy all the people. And if it doesn&#8217;t, company can divide people to more groups, may be five, and so on. Check image below :</p>
<blockquote>
<div><img alt="People Grouped into Different Sizes" class="align-center" src="_images/tshirt_grouped.jpg" />
</div></blockquote>
</div>
<div class="section" id="how-does-it-work">
<h7>How does it work ?<a class="headerlink" href="#how-does-it-work" title="Permalink to this headline">¶</a></h7>
<p>This algorithm is an iterative process. We will explain it step-by-step with the help of images.</p>
<p>Consider a set of data as below ( You can consider it as t-shirt problem). We need to cluster this data into two groups.</p>
<blockquote>
<div><img alt="Test Data" class="align-center" src="_images/testdata.jpg" />
</div></blockquote>
<p><strong>Step : 1</strong> - Algorithm randomly chooses two centroids, <img class="math" src="_images/math/a2be04c0711ea6ab40ba82694215e7452404f4c9.png" alt="C1"/> and <img class="math" src="_images/math/15c1c6aad983580ad3f7491227410bb0065e4260.png" alt="C2"/> (sometimes, any two data are taken as the centroids).</p>
<p><strong>Step : 2</strong> - It calculates the distance from each point to both centroids. If a test data is more closer to <img class="math" src="_images/math/a2be04c0711ea6ab40ba82694215e7452404f4c9.png" alt="C1"/>, then that data is labelled with &#8216;0&#8217;. If it is closer to <img class="math" src="_images/math/15c1c6aad983580ad3f7491227410bb0065e4260.png" alt="C2"/>, then labelled as &#8216;1&#8217; (If more centroids are there, labelled as &#8216;2&#8217;,&#8216;3&#8217; etc).</p>
<p>In our case, we will color all &#8216;0&#8217; labelled with red, and &#8216;1&#8217; labelled with blue. So we get following image after above operations.</p>
<blockquote>
<div><img alt="Initial Centroid Selection and Data Collection" class="align-center" src="_images/initial_labelling.jpg" />
</div></blockquote>
<p><strong>Step : 3</strong> - Next we calculate the average of all blue points and red points separately and that will be our new centroids. That is <img class="math" src="_images/math/a2be04c0711ea6ab40ba82694215e7452404f4c9.png" alt="C1"/> and <img class="math" src="_images/math/15c1c6aad983580ad3f7491227410bb0065e4260.png" alt="C2"/> shift to newly calculated centroids. (Remember, the images shown are not true values and not to true scale, it is just for demonstration only).</p>
<p>And again, perform step 2 with new centroids and label data to &#8216;0&#8217; and &#8216;1&#8217;.</p>
<p>So we get result as below :</p>
<blockquote>
<div><img alt="New Centroid Calculated and Data Re-laballed" class="align-center" src="_images/update_centroid.jpg" />
</div></blockquote>
<p>Now <strong>Step - 2</strong> and <strong>Step - 3</strong> are iterated until both centroids are converged to fixed points. <em>(Or it may be stopped depending on the criteria we provide, like maximum number of iterations, or a specific accuracy is reached etc.)</em> <strong>These points are such that sum of distances between test data and their corresponding centroids are minimum</strong>. Or simply, sum of distances between <img class="math" src="_images/math/9eb53b4275a27231387b1ccb6afa3a1134890f89.png" alt="C1 \leftrightarrow Red\_Points"/> and <img class="math" src="_images/math/ee8e6d64f935c4d580439d128fb0817e1aa73db3.png" alt="C2 \leftrightarrow Blue\_Points"/> is minimum.</p>
<div class="math">
<p><img src="_images/math/9b80d05ad90341c7c41b2a3da583f6c532465a9d.png" alt="minimize \;\bigg[J = \sum_{All\: Red_Points}distance(C1,Red\_Point) + \sum_{All\: Blue\_Points}distance(C2,Blue\_Point)\bigg]"/></p>
</div><p>Final result almost looks like below :</p>
<blockquote>
<div><img alt="Final Result" class="align-center" src="_images/final_clusters.jpg" />
</div></blockquote>
<p>So this is just an intuitive understanding of K-Means Clustering. For more details and mathematical explanation, please read any standard machine learning textbooks or check links in additional resources. It is just a top layer of K-Means clustering. There are a lot of modifications to this algorithm like, how to choose the initial centroids, how to speed up the iteration process etc.</p>
</div>
</div>
<div class="section" id="additional-resources">
<h6>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h6>
<ol class="arabic simple">
<li><a class="reference external" href="https://www.coursera.org/course/ml">Machine Learning Course</a>, Video lectures by Prof. Andrew Ng (Some of the images are taken from this)</li>
</ol>
</div>
<div class="section" id="exercises">
<h6>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h6>
</div>
</div>
<span id="document-py_tutorials/py_ml/py_kmeans/py_kmeans_opencv/py_kmeans_opencv"></span><div class="section" id="k-means-clustering-in-opencv">
<span id="kmeans-opencv"></span><h5>K-Means Clustering in OpenCV<a class="headerlink" href="#k-means-clustering-in-opencv" title="Permalink to this headline">¶</a></h5>
<div class="section" id="goal">
<h6>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h6>
<blockquote>
<div><ul class="simple">
<li>Learn to use <strong>cv2.kmeans()</strong> function in OpenCV for data clustering</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="understanding-parameters">
<h6>Understanding Parameters<a class="headerlink" href="#understanding-parameters" title="Permalink to this headline">¶</a></h6>
<div class="section" id="input-parameters">
<h7>Input parameters<a class="headerlink" href="#input-parameters" title="Permalink to this headline">¶</a></h7>
<blockquote>
<div><ol class="arabic">
<li><p class="first"><strong>samples</strong> : It should be of <strong>np.float32</strong> data type, and each feature should be put in a single column.</p>
</li>
<li><p class="first"><strong>nclusters(K)</strong> : Number of clusters required at end</p>
</li>
<li><dl class="first docutils">
<dt><strong>criteria</strong> <span class="classifier-delimiter">:</span> <span class="classifier">It is the iteration termination criteria. When this criteria is satisfied, algorithm iteration stops. Actually, it should be a tuple of 3 parameters. They are <code class="docutils literal"><span class="pre">(</span> <span class="pre">type,</span> <span class="pre">max_iter,</span> <span class="pre">epsilon</span> <span class="pre">)</span></code>:</span></dt>
<dd><ul class="first last simple">
<li><dl class="first docutils">
<dt>3.a - type of termination criteria <span class="classifier-delimiter">:</span> <span class="classifier">It has 3 flags as below:</span></dt>
<dd><strong>cv2.TERM_CRITERIA_EPS</strong> - stop the algorithm iteration if specified accuracy, <em>epsilon</em>, is reached.
<strong>cv2.TERM_CRITERIA_MAX_ITER</strong> - stop the algorithm after the specified number of iterations, <em>max_iter</em>.
<strong>cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER</strong> - stop the iteration when any of the above condition is met.</dd>
</dl>
</li>
<li>3.b - max_iter - An integer specifying maximum number of iterations.</li>
<li>3.c - epsilon - Required accuracy</li>
</ul>
</dd>
</dl>
</li>
<li><p class="first"><strong>attempts</strong> : Flag to specify the number of times the algorithm is executed using different initial labellings. The algorithm returns the labels that yield the best compactness. This compactness is returned as output.</p>
</li>
<li><p class="first"><strong>flags</strong> : This flag is used to specify how initial centers are taken. Normally two flags are used for this : <strong>cv2.KMEANS_PP_CENTERS</strong> and <strong>cv2.KMEANS_RANDOM_CENTERS</strong>.</p>
</li>
</ol>
</div></blockquote>
</div>
<div class="section" id="output-parameters">
<h7>Output parameters<a class="headerlink" href="#output-parameters" title="Permalink to this headline">¶</a></h7>
<blockquote>
<div><ol class="arabic simple">
<li><strong>compactness</strong> : It is the sum of squared distance from each point to their corresponding centers.</li>
<li><strong>labels</strong> : This is the label array (same as &#8216;code&#8217; in previous article) where each element marked &#8216;0&#8217;, &#8216;1&#8217;.....</li>
<li><strong>centers</strong> : This is array of centers of clusters.</li>
</ol>
</div></blockquote>
<p>Now we will see how to apply K-Means algorithm with three examples.</p>
</div>
</div>
<div class="section" id="data-with-only-one-feature">
<h6>1. Data with Only One Feature<a class="headerlink" href="#data-with-only-one-feature" title="Permalink to this headline">¶</a></h6>
<p>Consider, you have a set of data with only one feature, ie one-dimensional. For eg, we can take our t-shirt problem where you use only height of people to decide the size of t-shirt.</p>
<p>So we start by creating data and plot it in Matplotlib</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">25</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">175</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="mi">25</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">))</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">z</span><span class="p">,</span><span class="mi">256</span><span class="p">,[</span><span class="mi">0</span><span class="p">,</span><span class="mi">256</span><span class="p">]),</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>So we have &#8216;z&#8217; which is an array of size 50, and values ranging from 0 to 255. I have reshaped &#8216;z&#8217; to a column vector. It will be more useful when more than one features are present. Then I made data of np.float32 type.</p>
<p>We get following image :</p>
<blockquote>
<div><img alt="Test Data" class="align-center" src="_images/oc_1d_testdata.png" />
</div></blockquote>
<p>Now we apply the KMeans function. Before that we need to specify the <cite>criteria</cite>. My criteria is such that, whenever 10 iterations of algorithm is ran, or an accuracy of <code class="docutils literal"><span class="pre">epsilon</span> <span class="pre">=</span> <span class="pre">1.0</span></code> is reached, stop the algorithm and return the answer.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="c1"># Define criteria = ( type, max_iter = 10 , epsilon = 1.0 )</span>
<span class="n">criteria</span> <span class="o">=</span> <span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">TERM_CRITERIA_EPS</span> <span class="o">+</span> <span class="n">cv2</span><span class="o">.</span><span class="n">TERM_CRITERIA_MAX_ITER</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>

<span class="c1"># Set flags (Just to avoid line break in the code)</span>
<span class="n">flags</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">KMEANS_RANDOM_CENTERS</span>

<span class="c1"># Apply KMeans</span>
<span class="n">compactness</span><span class="p">,</span><span class="n">labels</span><span class="p">,</span><span class="n">centers</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">kmeans</span><span class="p">(</span><span class="n">z</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="kc">None</span><span class="p">,</span><span class="n">criteria</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="n">flags</span><span class="p">)</span>
</pre></div>
</div>
<p>This gives us the compactness, labels and centers. In this case, I got centers as 60 and 207. Labels will have the same size as that of test data where each data will be labelled as &#8216;0&#8217;,&#8216;1&#8217;,&#8216;2&#8217; etc. depending on their centroids. Now we split the data to different clusters depending on their labels.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">z</span><span class="p">[</span><span class="n">labels</span><span class="o">==</span><span class="mi">0</span><span class="p">]</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">z</span><span class="p">[</span><span class="n">labels</span><span class="o">==</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
<p>Now we plot A in Red color and B in Blue color and their centroids in Yellow color.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="c1"># Now plot &#39;A&#39; in red, &#39;B&#39; in blue, &#39;centers&#39; in yellow</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="mi">256</span><span class="p">,[</span><span class="mi">0</span><span class="p">,</span><span class="mi">256</span><span class="p">],</span><span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">B</span><span class="p">,</span><span class="mi">256</span><span class="p">,[</span><span class="mi">0</span><span class="p">,</span><span class="mi">256</span><span class="p">],</span><span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;b&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">centers</span><span class="p">,</span><span class="mi">32</span><span class="p">,[</span><span class="mi">0</span><span class="p">,</span><span class="mi">256</span><span class="p">],</span><span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Below is the output we got:</p>
<blockquote>
<div><img alt="Result of KMeans Clustering" class="align-center" src="_images/oc_1d_clustered.png" />
</div></blockquote>
</div>
<div class="section" id="data-with-multiple-features">
<h6>2. Data with Multiple Features<a class="headerlink" href="#data-with-multiple-features" title="Permalink to this headline">¶</a></h6>
<p>In previous example, we took only height for t-shirt problem. Here, we will take both height and weight, ie two features.</p>
<p>Remember, in previous case, we made our data to a single column vector. Each feature is arranged in a column, while each row corresponds to an input test sample.</p>
<p>For example, in this case, we set a test data of size 50x2, which are heights and weights of 50 people. First column corresponds to height of all the 50 people and second column corresponds to their weights. First row contains two elements where first one is the height of first person and second one his weight. Similarly remaining rows corresponds to heights and weights of other people. Check image below:</p>
<blockquote>
<div><img alt="Feature Representation" class="align-center" src="_images/oc_feature_representation.jpg" />
</div></blockquote>
<p>Now I am directly moving to the code:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span><span class="mi">50</span><span class="p">,(</span><span class="mi">25</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">60</span><span class="p">,</span><span class="mi">85</span><span class="p">,(</span><span class="mi">25</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">))</span>

<span class="c1"># convert to np.float32</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>

<span class="c1"># define criteria and apply kmeans()</span>
<span class="n">criteria</span> <span class="o">=</span> <span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">TERM_CRITERIA_EPS</span> <span class="o">+</span> <span class="n">cv2</span><span class="o">.</span><span class="n">TERM_CRITERIA_MAX_ITER</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
<span class="n">ret</span><span class="p">,</span><span class="n">label</span><span class="p">,</span><span class="n">center</span><span class="o">=</span><span class="n">cv2</span><span class="o">.</span><span class="n">kmeans</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="kc">None</span><span class="p">,</span><span class="n">criteria</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">KMEANS_RANDOM_CENTERS</span><span class="p">)</span>

<span class="c1"># Now separate the data, Note the flatten()</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">Z</span><span class="p">[</span><span class="n">label</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span><span class="o">==</span><span class="mi">0</span><span class="p">]</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">Z</span><span class="p">[</span><span class="n">label</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span><span class="o">==</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Plot the data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">A</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">A</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">B</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">B</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">center</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">center</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">s</span> <span class="o">=</span> <span class="mi">80</span><span class="p">,</span><span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;s&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Height&#39;</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Weight&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Below is the output we get:</p>
<blockquote>
<div><img alt="Result of KMeans Clustering" class="align-center" src="_images/oc_2d_clustered.jpg" />
</div></blockquote>
</div>
<div class="section" id="color-quantization">
<h6>3. Color Quantization<a class="headerlink" href="#color-quantization" title="Permalink to this headline">¶</a></h6>
<p>Color Quantization is the process of reducing number of colors in an image. One reason to do so is to reduce the memory. Sometimes, some devices may have limitation such that it can produce only limited number of colors. In those cases also, color quantization is performed. Here we use k-means clustering for color quantization.</p>
<p>There is nothing new to be explained here. There are 3 features, say, R,G,B. So we need to reshape the image to an array of Mx3 size (M is number of pixels in image). And after the clustering, we apply centroid values (it is also R,G,B) to all pixels, such that resulting image will have specified number of colors. And again we need to reshape it back to the shape of original image. Below is the code:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cv2</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;home.jpg&#39;</span><span class="p">)</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>

<span class="c1"># convert to np.float32</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>

<span class="c1"># define criteria, number of clusters(K) and apply kmeans()</span>
<span class="n">criteria</span> <span class="o">=</span> <span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">TERM_CRITERIA_EPS</span> <span class="o">+</span> <span class="n">cv2</span><span class="o">.</span><span class="n">TERM_CRITERIA_MAX_ITER</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">ret</span><span class="p">,</span><span class="n">label</span><span class="p">,</span><span class="n">center</span><span class="o">=</span><span class="n">cv2</span><span class="o">.</span><span class="n">kmeans</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span><span class="n">K</span><span class="p">,</span><span class="kc">None</span><span class="p">,</span><span class="n">criteria</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">KMEANS_RANDOM_CENTERS</span><span class="p">)</span>

<span class="c1"># Now convert back into uint8, and make original image</span>
<span class="n">center</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">(</span><span class="n">center</span><span class="p">)</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">center</span><span class="p">[</span><span class="n">label</span><span class="o">.</span><span class="n">flatten</span><span class="p">()]</span>
<span class="n">res2</span> <span class="o">=</span> <span class="n">res</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>

<span class="n">cv2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s1">&#39;res2&#39;</span><span class="p">,</span><span class="n">res2</span><span class="p">)</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>
</pre></div>
</div>
<p>See the result below for K=8:</p>
<blockquote>
<div><img alt="Color Quantization" class="align-center" src="_images/oc_color_quantization.jpg" />
</div></blockquote>
</div>
<div class="section" id="additional-resources">
<h6>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h6>
</div>
<div class="section" id="exercises">
<h6>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h6>
</div>
</div>
</div>
</div>
</div>
</div>
<span id="document-py_tutorials/py_photo/py_table_of_contents_photo/py_table_of_contents_photo"></span><div class="section" id="computational-photography">
<span id="py-table-of-content-photo"></span><h3>Computational Photography<a class="headerlink" href="#computational-photography" title="Permalink to this headline">¶</a></h3>
<p>Here you will learn different OpenCV functionalities related to Computational Photography like image denoising etc.</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#non-local-means"><span class="std std-ref">Image Denoising</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="12%" />
<col width="88%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/nlm_icon1.jpg"><img alt="photo_1" src="_images/nlm_icon1.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>See a good technique to remove noises in images called Non-Local Means Denoising</td>
</tr>
</tbody>
</table>
</div></blockquote>
<ul class="simple">
<li><a class="reference internal" href="index.html#inpainting"><span class="std std-ref">Image Inpainting</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="7%" />
<col width="93%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/inpainticon.jpg"><img alt="photo_2" src="_images/inpainticon.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Do you have a old degraded photo with many black spots and strokes on it? Take it. Let&#8217;s try to restore them with a technique called image inpainting.</td>
</tr>
</tbody>
</table>
</div></blockquote>
<div class="toctree-wrapper compound">
<span id="document-py_tutorials/py_photo/py_non_local_means/py_non_local_means"></span><div class="section" id="image-denoising">
<span id="non-local-means"></span><h4>Image Denoising<a class="headerlink" href="#image-denoising" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goal">
<h5>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h5>
<p>In this chapter,</p>
<blockquote>
<div><ul class="simple">
<li>You will learn about Non-local Means Denoising algorithm to remove noise in the image.</li>
<li>You will see different functions like <strong>cv2.fastNlMeansDenoising()</strong>, <strong>cv2.fastNlMeansDenoisingColored()</strong> etc.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="theory">
<h5>Theory<a class="headerlink" href="#theory" title="Permalink to this headline">¶</a></h5>
<p>In earlier chapters, we have seen many image smoothing techniques like Gaussian Blurring, Median Blurring etc and they were good to some extent in removing small quantities of noise. In those techniques, we took a small neighbourhood around a pixel and did some operations like gaussian weighted average, median of the values etc to replace the central element. In short, noise removal at a pixel was local to its neighbourhood.</p>
<p>There is a property of noise. Noise is generally considered to be a random variable with zero mean. Consider a noisy pixel, <img class="math" src="_images/math/202a4c39e0ace29877f6ac2f0bd1e0e4f0a936bd.png" alt="p = p_0 + n"/> where <img class="math" src="_images/math/2aa528e29c4df6e3a7e44783f02f45e46012b948.png" alt="p_0"/> is the true value of pixel and <img class="math" src="_images/math/413f8a8e40062a9090d9d50b88bc7b551b314c26.png" alt="n"/> is the noise in that pixel. You can take large number of same pixels (say <img class="math" src="_images/math/75e27f04188974063be3230dca208cd495b77ce1.png" alt="N"/>) from different images and computes their average. Ideally, you should get <img class="math" src="_images/math/c96f201555dd4939095d26ecbfb68bf2e4fa1189.png" alt="p = p_0"/> since mean of noise is zero.</p>
<p>You can verify it yourself by a simple setup. Hold a static camera to a certain location for a couple of seconds. This will give you plenty of frames, or a lot of images of the same scene. Then write a piece of code to find the average of all the frames in the video (This should be too simple for you now ). Compare the final result and first frame. You can see reduction in noise. Unfortunately this simple method is not robust to camera and scene motions. Also often there is only one noisy image available.</p>
<p>So idea is simple, we need a set of similar images to average out the noise. Consider a small window (say 5x5 window) in the image. Chance is large that the same patch may be somewhere else in the image. Sometimes in a small neigbourhood around it. What about using these similar patches together and find their average? For that particular window, that is fine. See an example image below:</p>
<blockquote>
<div><img alt="Similar patches" class="align-center" src="_images/nlm_patch.jpg" />
</div></blockquote>
<p>The blue patches in the image looks the similar. Green patches looks similar. So we take a pixel, take small window around it, search for similar windows in the image, average all the windows and replace the pixel with the result we got. This method is Non-Local Means Denoising. It takes more time compared to blurring techniques we saw earlier, but its result is very good. More details and online demo can be found at first link in additional resources.</p>
<p>For color images, image is converted to CIELAB colorspace and then it separately denoise L and AB components.</p>
</div>
<div class="section" id="image-denoising-in-opencv">
<h5>Image Denoising in OpenCV<a class="headerlink" href="#image-denoising-in-opencv" title="Permalink to this headline">¶</a></h5>
<p>OpenCV provides four variations of this technique.</p>
<ol class="arabic simple">
<li><strong>cv2.fastNlMeansDenoising()</strong> - works with a single grayscale images</li>
<li><strong>cv2.fastNlMeansDenoisingColored()</strong> - works with a color image.</li>
<li><strong>cv2.fastNlMeansDenoisingMulti()</strong> - works with image sequence captured in short period of time (grayscale images)</li>
<li><strong>cv2.fastNlMeansDenoisingColoredMulti()</strong> - same as above, but for color images.</li>
</ol>
<dl class="docutils">
<dt>Common arguments are:</dt>
<dd><ul class="first last simple">
<li>h : parameter deciding filter strength. Higher h value removes noise better, but removes details of image also. (10 is ok)</li>
<li>hForColorComponents : same as h, but for color images only. (normally same as h)</li>
<li>templateWindowSize : should be odd. (recommended 7)</li>
<li>searchWindowSize : should be odd. (recommended 21)</li>
</ul>
</dd>
</dl>
<p>Please visit first link in additional resources for more details on these parameters.</p>
<p>We will demonstrate 2 and 3 here. Rest is left for you.</p>
<div class="section" id="cv2-fastnlmeansdenoisingcolored">
<h6>1. cv2.fastNlMeansDenoisingColored()<a class="headerlink" href="#cv2-fastnlmeansdenoisingcolored" title="Permalink to this headline">¶</a></h6>
<p>As mentioned above it is used to remove noise from color images. (Noise is expected to be gaussian). See the example below:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;die.png&#39;</span><span class="p">)</span>

<span class="n">dst</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">fastNlMeansDenoisingColored</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="kc">None</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">21</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">dst</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Below is a zoomed version of result. My input image has a gaussian noise of <img class="math" src="_images/math/99b4e153cea3d8433f307dab707a2c43c8f9f72d.png" alt="\sigma = 25"/>. See the result:</p>
<blockquote>
<div><img alt="Result of denoising" class="align-center" src="_images/nlm_result1.jpg" />
</div></blockquote>
</div>
<div class="section" id="cv2-fastnlmeansdenoisingmulti">
<h6>2. cv2.fastNlMeansDenoisingMulti()<a class="headerlink" href="#cv2-fastnlmeansdenoisingmulti" title="Permalink to this headline">¶</a></h6>
<p>Now we will apply the same method to a video. The first argument is the list of noisy frames. Second argument <cite>imgToDenoiseIndex</cite> specifies which frame we need to denoise, for that we pass the index of frame in our input list. Third is the <cite>temporalWindowSize</cite> which specifies the number of nearby frames to be used for denoising. It should be odd. In that case, a total of <cite>temporalWindowSize</cite> frames are used where central frame is the frame to be denoised. For example, you passed a list of 5 frames as input. Let <cite>imgToDenoiseIndex = 2</cite> and <cite>temporalWindowSize = 3</cite>. Then frame-1, frame-2 and frame-3 are used to denoise frame-2. Let&#8217;s see an example.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">cap</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">VideoCapture</span><span class="p">(</span><span class="s1">&#39;vtest.avi&#39;</span><span class="p">)</span>

<span class="c1"># create a list of first 5 frames</span>
<span class="n">img</span> <span class="o">=</span> <span class="p">[</span><span class="n">cap</span><span class="o">.</span><span class="n">read</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="mi">5</span><span class="p">)]</span>

<span class="c1"># convert all to grayscale</span>
<span class="n">gray</span> <span class="o">=</span> <span class="p">[</span><span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2GRAY</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">img</span><span class="p">]</span>

<span class="c1"># convert all to float64</span>
<span class="n">gray</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">gray</span><span class="p">]</span>

<span class="c1"># create a noise of variance 25</span>
<span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="n">gray</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">*</span><span class="mi">10</span>

<span class="c1"># Add this noise to images</span>
<span class="n">noisy</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="n">noise</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">gray</span><span class="p">]</span>

<span class="c1"># Convert back to uint8</span>
<span class="n">noisy</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">noisy</span><span class="p">]</span>

<span class="c1"># Denoise 3rd frame considering all the 5 frames</span>
<span class="n">dst</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">fastNlMeansDenoisingMulti</span><span class="p">(</span><span class="n">noisy</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">35</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">131</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">gray</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">132</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">noisy</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">),</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">dst</span><span class="p">,</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Below image shows a zoomed version of the result we got:</p>
<blockquote>
<div><img alt="Denoising a frame" class="align-center" src="_images/nlm_multi.jpg" />
</div></blockquote>
<p>It takes considerable amount of time for computation. In the result, first image is the original frame, second is the noisy one, third is the denoised image.</p>
</div>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
<ol class="arabic simple">
<li><a class="reference external" href="http://www.ipol.im/pub/art/2011/bcm_nlm/">http://www.ipol.im/pub/art/2011/bcm_nlm/</a> (It has the details, online demo etc. Highly recommended to visit. Our test image is generated from this link)</li>
<li><a class="reference external" href="https://www.coursera.org/course/images">Online course at coursera</a> (First image taken from here)</li>
</ol>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
</div>
</div>
<span id="document-py_tutorials/py_photo/py_inpainting/py_inpainting"></span><div class="section" id="image-inpainting">
<span id="inpainting"></span><h4>Image Inpainting<a class="headerlink" href="#image-inpainting" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goal">
<h5>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h5>
<dl class="docutils">
<dt>In this chapter,</dt>
<dd><ul class="first last simple">
<li>We will learn how to remove small noises, strokes etc in old photographs by a method called inpainting</li>
<li>We will see inpainting functionalities in OpenCV.</li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="basics">
<h5>Basics<a class="headerlink" href="#basics" title="Permalink to this headline">¶</a></h5>
<p>Most of you will have some old degraded photos at your home with some black spots, some strokes etc on it. Have you ever thought of restoring it back? We can&#8217;t simply erase them in a paint tool because it is will simply replace black structures with white structures which is of no use. In these cases, a technique called image inpainting is used. The basic idea is simple: Replace those bad marks with its neighbouring pixels so that it looks like the neigbourhood. Consider the image shown below (taken from <a class="reference external" href="http://en.wikipedia.org/wiki/Inpainting">Wikipedia</a>):</p>
<blockquote>
<div><img alt="Inpainting example" class="align-center" src="_images/inpaint_basics.jpg" />
</div></blockquote>
<p>Several algorithms were designed for this purpose and OpenCV provides two of them. Both can be accessed by the same function, <strong>cv2.inpaint()</strong></p>
<p>First algorithm is based on the paper <strong>&#8220;An Image Inpainting Technique Based on the Fast Marching Method&#8221;</strong> by Alexandru Telea in 2004. It is based on Fast Marching Method. Consider a region in the image to be inpainted. Algorithm starts from the boundary of this region and goes inside the region gradually filling everything in the boundary first. It takes a small neighbourhood around the pixel on the neigbourhood to be inpainted. This pixel is replaced by normalized weighted sum of all the known pixels in the neigbourhood. Selection of the weights is an important matter. More weightage is given to those pixels lying near to the point, near to the normal of the boundary and those lying on the boundary contours. Once a pixel is inpainted, it moves to next nearest pixel using Fast Marching Method. FMM ensures those pixels near the known pixels are inpainted first, so that it just works like a manual heuristic operation. This algorithm is enabled by using the flag, <code class="docutils literal"><span class="pre">cv2.INPAINT_TELEA</span></code>.</p>
<p>Second algorithm is based on the paper <strong>&#8220;Navier-Stokes, Fluid Dynamics, and Image and Video Inpainting&#8221;</strong> by Bertalmio, Marcelo, Andrea L. Bertozzi, and Guillermo Sapiro in 2001. This algorithm is based on fluid dynamics and utilizes partial differential equations. Basic principle is heurisitic. It first travels along the edges from known regions to unknown regions (because edges are meant to be continuous). It continues isophotes (lines joining points with same intensity, just like contours joins points with same elevation) while matching gradient vectors at the boundary of the inpainting region. For this, some methods from fluid dynamics are used. Once they are obtained, color is filled to reduce minimum variance in that area. This algorithm is enabled by using the flag, <code class="docutils literal"><span class="pre">cv2.INPAINT_NS</span></code>.</p>
</div>
<div class="section" id="code">
<h5>Code<a class="headerlink" href="#code" title="Permalink to this headline">¶</a></h5>
<p>We need to create a mask of same size as that of input image, where non-zero pixels corresponds to the area which is to be inpainted. Everything else is simple. My image is degraded with some black strokes (I added manually). I created a corresponding strokes with Paint tool.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cv2</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;messi_2.jpg&#39;</span><span class="p">)</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;mask2.png&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>

<span class="n">dst</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">inpaint</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">mask</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">INPAINT_TELEA</span><span class="p">)</span>

<span class="n">cv2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s1">&#39;dst&#39;</span><span class="p">,</span><span class="n">dst</span><span class="p">)</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>
</pre></div>
</div>
<p>See the result below. First image shows degraded input. Second image is the mask. Third image is the result of first algorithm and last image is the result of second algorithm.</p>
<blockquote>
<div><img alt="Inpainting result" class="align-center" src="_images/inpaint_result.jpg" />
</div></blockquote>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
<ol class="arabic simple">
<li>Bertalmio, Marcelo, Andrea L. Bertozzi, and Guillermo Sapiro. &#8220;Navier-stokes, fluid dynamics, and image and video inpainting.&#8221; In Computer Vision and Pattern Recognition, 2001. CVPR 2001. Proceedings of the 2001 IEEE Computer Society Conference on, vol. 1, pp. I-355. IEEE, 2001.</li>
<li>Telea, Alexandru. &#8220;An image inpainting technique based on the fast marching method.&#8221; Journal of graphics tools 9.1 (2004): 23-34.</li>
</ol>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
<ol class="arabic simple">
<li>OpenCV comes with an interactive sample on inpainting, <code class="docutils literal"><span class="pre">samples/python2/inpaint.py</span></code>, try it.</li>
<li>A few months ago, I watched a video on <a class="reference external" href="http://www.youtube.com/watch?v=ZtoUiplKa2A">Content-Aware Fill</a>, an advanced inpainting technique used in Adobe Photoshop. On further search, I was able to find that same technique is already there in GIMP with different name, &#8220;Resynthesizer&#8221; (You need to install separate plugin). I am sure you will enjoy the technique.</li>
</ol>
</div>
</div>
</div>
</div>
<span id="document-py_tutorials/py_objdetect/py_table_of_contents_objdetect/py_table_of_contents_objdetect"></span><div class="section" id="object-detection">
<span id="py-table-of-content-objdetection"></span><h3>Object Detection<a class="headerlink" href="#object-detection" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference internal" href="index.html#face-detection"><span class="std std-ref">Face Detection using Haar Cascades</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="17%" />
<col width="83%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/face_icon.jpg"><img alt="objdet_1" src="_images/face_icon.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Face detection using haar-cascades</td>
</tr>
</tbody>
</table>
</div></blockquote>
<div class="toctree-wrapper compound">
<span id="document-py_tutorials/py_objdetect/py_face_detection/py_face_detection"></span><div class="section" id="face-detection-using-haar-cascades">
<span id="face-detection"></span><h4>Face Detection using Haar Cascades<a class="headerlink" href="#face-detection-using-haar-cascades" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goal">
<h5>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h5>
<p>In this session,</p>
<blockquote>
<div><ul class="simple">
<li>We will see the basics of face detection using Haar Feature-based Cascade Classifiers</li>
<li>We will extend the same for eye detection etc.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="basics">
<h5>Basics<a class="headerlink" href="#basics" title="Permalink to this headline">¶</a></h5>
<p>Object Detection using Haar feature-based cascade classifiers is an effective object detection method proposed by Paul Viola and Michael Jones in their paper, &#8220;Rapid Object Detection using a Boosted Cascade of Simple Features&#8221; in 2001. It is a machine learning based approach where a cascade function is trained from a lot of positive and negative images. It is then used to detect objects in other images.</p>
<p>Here we will work with face detection. Initially, the algorithm needs a lot of positive images (images of faces) and negative images (images without faces) to train the classifier. Then we need to extract features from it. For this, haar features shown in below image are used. They are just like our convolutional kernel. Each feature is a single value obtained by subtracting sum of pixels under white rectangle from sum of pixels under black rectangle.</p>
<blockquote>
<div><img alt="Haar Features" class="align-center" src="_images/haar_features.jpg" />
</div></blockquote>
<p>Now all possible sizes and locations of each kernel is used to calculate plenty of features. (Just imagine how much computation it needs? Even a 24x24 window results over 160000 features). For each feature calculation, we need to find sum of pixels under white and black rectangles. To solve this, they introduced the integral images. It simplifies calculation of sum of pixels, how large may be the number of pixels, to an operation involving just four pixels. Nice, isn&#8217;t it? It makes things super-fast.</p>
<p>But among all these features we calculated, most of them are irrelevant. For example, consider the image below. Top row shows two good features. The first feature selected seems to focus on the property that the region of the eyes is often darker than the region of the nose and cheeks. The second feature selected relies on the property that the eyes are darker than the bridge of the nose. But the same windows applying on cheeks or any other place is irrelevant. So how do we select the best features out of 160000+ features? It is achieved by <strong>Adaboost</strong>.</p>
<blockquote>
<div><img alt="Face Detection" class="align-center" src="_images/haar.png" />
</div></blockquote>
<p>For this, we apply each and every feature on all the training images. For each feature, it finds the best threshold which will classify the faces to positive and negative. But obviously, there will be errors or misclassifications. We select the features with minimum error rate, which means they are the features that best classifies the face and non-face images. (The process is not as simple as this. Each image is given an equal weight in the beginning. After each classification, weights of misclassified images are increased. Then again same process is done. New error rates are calculated. Also new weights. The process is continued until required accuracy or error rate is achieved or required number of features are found).</p>
<p>Final classifier is a weighted sum of these weak classifiers. It is called weak because it alone can&#8217;t classify the image, but together with others forms a strong classifier. The paper says even 200 features provide detection with 95% accuracy. Their final setup had around 6000 features. (Imagine a reduction from 160000+ features to 6000 features. That is a big gain).</p>
<p>So now you take an image. Take each 24x24 window. Apply 6000 features to it. Check if it is face or not. Wow.. Wow.. Isn&#8217;t it a little inefficient and time consuming? Yes, it is. Authors have a good solution for that.</p>
<p>In an image, most of the image region is non-face region. So it is a better idea to have a simple method to check if a window is not a face region. If it is not, discard it in a single shot. Don&#8217;t process it again. Instead focus on region where there can be a face. This way, we can find more time to check a possible face region.</p>
<p>For this they introduced the concept of <strong>Cascade of Classifiers</strong>. Instead of applying all the 6000 features on a window, group the features into different stages of classifiers and apply one-by-one. (Normally first few stages will contain very less number of features). If a window fails the first stage, discard it. We don&#8217;t consider remaining features on it. If it passes, apply the second stage of features and continue the process. The window which passes all stages is a face region. How is the plan !!!</p>
<p>Authors&#8217; detector had 6000+ features with 38 stages with 1, 10, 25, 25 and 50 features in first five stages. (Two features in the above image is actually obtained as the best two features from Adaboost). According to authors, on an average, 10 features out of 6000+ are evaluated per sub-window.</p>
<p>So this is a simple intuitive explanation of how Viola-Jones face detection works. Read paper for more details or check out the references in Additional Resources section.</p>
</div>
<div class="section" id="haar-cascade-detection-in-opencv">
<h5>Haar-cascade Detection in OpenCV<a class="headerlink" href="#haar-cascade-detection-in-opencv" title="Permalink to this headline">¶</a></h5>
<p>OpenCV comes with a trainer as well as detector. If you want to train your own classifier for any object like car, planes etc. you can use OpenCV to create one. Its full details are given here: <a class="reference external" href="http://docs.opencv.org/doc/user_guide/ug_traincascade.html">Cascade Classifier Training.</a></p>
<p>Here we will deal with detection. OpenCV already contains many pre-trained classifiers for face, eyes, smile etc. Those XML files are stored in <code class="docutils literal"><span class="pre">opencv/data/haarcascades/</span></code> folder. Let&#8217;s create face and eye detector with OpenCV.</p>
<p>First we need to load the required XML classifiers. Then load our input image (or video) in grayscale mode.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cv2</span>

<span class="n">face_cascade</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">CascadeClassifier</span><span class="p">(</span><span class="s1">&#39;haarcascade_frontalface_default.xml&#39;</span><span class="p">)</span>
<span class="n">eye_cascade</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">CascadeClassifier</span><span class="p">(</span><span class="s1">&#39;haarcascade_eye.xml&#39;</span><span class="p">)</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;sachin.jpg&#39;</span><span class="p">)</span>
<span class="n">gray</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2GRAY</span><span class="p">)</span>
</pre></div>
</div>
<p>Now we find the faces in the image. If faces are found, it returns the positions of detected faces as Rect(x,y,w,h). Once we get these locations, we can create a ROI for the face and apply eye detection on this ROI (since eyes are always on the face !!! ).</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">faces</span> <span class="o">=</span> <span class="n">face_cascade</span><span class="o">.</span><span class="n">detectMultiScale</span><span class="p">(</span><span class="n">gray</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="k">for</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="n">h</span><span class="p">)</span> <span class="ow">in</span> <span class="n">faces</span><span class="p">:</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">rectangle</span><span class="p">(</span><span class="n">img</span><span class="p">,(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">),(</span><span class="n">x</span><span class="o">+</span><span class="n">w</span><span class="p">,</span><span class="n">y</span><span class="o">+</span><span class="n">h</span><span class="p">),(</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">roi_gray</span> <span class="o">=</span> <span class="n">gray</span><span class="p">[</span><span class="n">y</span><span class="p">:</span><span class="n">y</span><span class="o">+</span><span class="n">h</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span><span class="n">x</span><span class="o">+</span><span class="n">w</span><span class="p">]</span>
    <span class="n">roi_color</span> <span class="o">=</span> <span class="n">img</span><span class="p">[</span><span class="n">y</span><span class="p">:</span><span class="n">y</span><span class="o">+</span><span class="n">h</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span><span class="n">x</span><span class="o">+</span><span class="n">w</span><span class="p">]</span>
    <span class="n">eyes</span> <span class="o">=</span> <span class="n">eye_cascade</span><span class="o">.</span><span class="n">detectMultiScale</span><span class="p">(</span><span class="n">roi_gray</span><span class="p">)</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">ex</span><span class="p">,</span><span class="n">ey</span><span class="p">,</span><span class="n">ew</span><span class="p">,</span><span class="n">eh</span><span class="p">)</span> <span class="ow">in</span> <span class="n">eyes</span><span class="p">:</span>
        <span class="n">cv2</span><span class="o">.</span><span class="n">rectangle</span><span class="p">(</span><span class="n">roi_color</span><span class="p">,(</span><span class="n">ex</span><span class="p">,</span><span class="n">ey</span><span class="p">),(</span><span class="n">ex</span><span class="o">+</span><span class="n">ew</span><span class="p">,</span><span class="n">ey</span><span class="o">+</span><span class="n">eh</span><span class="p">),(</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span><span class="mi">2</span><span class="p">)</span>

<span class="n">cv2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s1">&#39;img&#39;</span><span class="p">,</span><span class="n">img</span><span class="p">)</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>
</pre></div>
</div>
<p>Result looks like below:</p>
<blockquote>
<div><img alt="Face Detection" class="align-center" src="_images/face.jpg" />
</div></blockquote>
</div>
<div class="section" id="additional-resources">
<h5>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline">¶</a></h5>
<ol class="arabic simple">
<li>Video Lecture on <a class="reference external" href="http://www.youtube.com/watch?v=WfdYYNamHZ8">Face Detection and Tracking</a></li>
<li>An interesting interview regarding Face Detection by <a class="reference external" href="http://www.makematics.com/research/viola-jones/">Adam Harvey</a></li>
</ol>
</div>
<div class="section" id="exercises">
<h5>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h5>
</div>
</div>
</div>
</div>
<span id="document-py_tutorials/py_bindings/py_table_of_contents_bindings/py_table_of_contents_bindings"></span><div class="section" id="opencv-python-bindings">
<span id="py-table-of-content-bindings"></span><h3>OpenCV-Python Bindings<a class="headerlink" href="#opencv-python-bindings" title="Permalink to this headline">¶</a></h3>
<p>Here, you will learn how OpenCV-Python bindings are generated.</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#bindings-basics"><span class="std std-ref">How OpenCV-Python Bindings Works?</span></a></li>
</ul>
<blockquote>
<div><table border="1" class="toctableopencv docutils">
<colgroup>
<col width="17%" />
<col width="83%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="_images/nlm_icon.jpg"><img alt="bind1" src="_images/nlm_icon.jpg" style="width: 90pt; height: 90pt;" /></a></td>
<td>Learn how OpenCV-Python bindings are generated.</td>
</tr>
</tbody>
</table>
</div></blockquote>
<div class="toctree-wrapper compound">
<span id="document-py_tutorials/py_bindings/py_bindings_basics/py_bindings_basics"></span><div class="section" id="how-opencv-python-bindings-works">
<span id="bindings-basics"></span><h4>How OpenCV-Python Bindings Works?<a class="headerlink" href="#how-opencv-python-bindings-works" title="Permalink to this headline">¶</a></h4>
<div class="section" id="goal">
<h5>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h5>
<p>Learn:</p>
<blockquote>
<div><ul class="simple">
<li>How OpenCV-Python bindings are generated?</li>
<li>How to extend new OpenCV modules to Python?</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="how-opencv-python-bindings-are-generated">
<h5>How OpenCV-Python bindings are generated?<a class="headerlink" href="#how-opencv-python-bindings-are-generated" title="Permalink to this headline">¶</a></h5>
<p>In OpenCV, all algorithms are implemented in C++. But these algorithms can be used from different languages like Python, Java etc. This is made possible by the bindings generators. These generators create a bridge between C++ and Python which enables users to call C++ functions from Python. To get a complete picture of what is happening in background, a good knowledge of Python/C API is required. A simple example on extending C++ functions to Python can be found in official Python documentation[1]. So extending all functions in OpenCV to Python by writing their wrapper functions manually is a time-consuming task. So OpenCV does it in a  more intelligent way. OpenCV generates these wrapper functions automatically from the C++ headers using some Python scripts which are located in <code class="docutils literal"><span class="pre">modules/python/src2</span></code>. We will look into what they do.</p>
<p>First, <code class="docutils literal"><span class="pre">modules/python/CMakeFiles.txt</span></code> is a CMake script which checks the modules to be extended to Python. It will automatically check all the modules to be extended and grab their header files. These header files contain list of all classes, functions, constants etc. for that particular modules.</p>
<p>Second, these header files are passed to a Python script, <code class="docutils literal"><span class="pre">modules/python/src2/gen2.py</span></code>. This is the Python bindings generator script. It calls another Python script <code class="docutils literal"><span class="pre">modules/python/src2/hdr_parser.py</span></code>. This is the header parser script. This header parser splits the complete header file into small Python lists. So these lists contain all details about a particular function, class etc. For example, a function will be parsed to get a list containing function name, return type, input arguments, argument types etc. Final list contains details of all the functions, structs, classes etc. in that header file.</p>
<p>But header parser doesn&#8217;t parse all the functions/classes in the header file. The developer has to specify which functions should be exported to Python. For that, there are certain macros added to the beginning of these declarations which enables the header parser to identify functions to be parsed. These macros are added by the developer who programs the particular function. In short, the developer decides which functions should be extended to Python and which are not. Details of those macros will be given in next session.</p>
<p>So header parser returns a final big list of parsed functions. Our generator script (gen2.py) will create wrapper functions for all the functions/classes/enums/structs parsed by header parser (You can find these header files during compilation in the <code class="docutils literal"><span class="pre">build/modules/python/</span></code> folder as <code class="docutils literal"><span class="pre">pyopencv_generated_*.h</span></code> files). But there may be some basic OpenCV datatypes like Mat, Vec4i, Size. They need to be extended manually. For example, a Mat type should be extended to Numpy array, Size should be extended to a tuple of two integers etc. Similarly, there may be some complex structs/classes/functions etc. which need to be extended manually. All such manual wrapper functions are placed in <code class="docutils literal"><span class="pre">modules/python/src2/pycv2.hpp</span></code>.</p>
<p>So now only thing left is the compilation of these wrapper files which gives us <strong>cv2</strong> module. So when you call a function, say <code class="docutils literal"><span class="pre">res</span> <span class="pre">=</span> <span class="pre">equalizeHist(img1,img2)</span></code> in Python, you pass two numpy arrays and you expect another numpy array as the output. So these numpy arrays are converted to <code class="docutils literal"><span class="pre">cv::Mat</span></code> and then calls the <code class="docutils literal"><span class="pre">equalizeHist()</span></code> function in C++. Final result, <code class="docutils literal"><span class="pre">res</span></code> will be converted back into a Numpy array. So in short, almost all operations are done in C++ which gives us almost same speed as that of C++.</p>
<p>So this is the basic version of how OpenCV-Python bindings are generated.</p>
</div>
<div class="section" id="how-to-extend-new-modules-to-python">
<h5>How to extend new modules to Python?<a class="headerlink" href="#how-to-extend-new-modules-to-python" title="Permalink to this headline">¶</a></h5>
<p>Header parser parse the header files based on some wrapper macros added to function declaration. Enumeration constants don&#8217;t need any wrapper macros. They are automatically wrapped. But remaining functions, classes etc. need wrapper macros.</p>
<p>Functions are extended using <code class="docutils literal"><span class="pre">CV_EXPORTS_W</span></code> macro. An example is shown below.</p>
<div class="highlight-cpp"><div class="highlight"><pre><span></span><span class="n">CV_EXPORTS_W</span> <span class="kt">void</span> <span class="nf">equalizeHist</span><span class="p">(</span> <span class="n">InputArray</span> <span class="n">src</span><span class="p">,</span> <span class="n">OutputArray</span> <span class="n">dst</span> <span class="p">);</span>
</pre></div>
</div>
<p>Header parser can understand the input and output arguments from keywords like <code class="docutils literal"><span class="pre">InputArray,</span> <span class="pre">OutputArray</span></code> etc. But sometimes, we may need to hardcode inputs and outputs. For that, macros like <code class="docutils literal"><span class="pre">CV_OUT,</span> <span class="pre">CV_IN_OUT</span></code> etc. are used.</p>
<div class="highlight-cpp"><div class="highlight"><pre><span></span><span class="n">CV_EXPORTS_W</span> <span class="kt">void</span> <span class="nf">minEnclosingCircle</span><span class="p">(</span> <span class="n">InputArray</span> <span class="n">points</span><span class="p">,</span>
                                     <span class="n">CV_OUT</span> <span class="n">Point2f</span><span class="o">&amp;</span> <span class="n">center</span><span class="p">,</span> <span class="n">CV_OUT</span> <span class="kt">float</span><span class="o">&amp;</span> <span class="n">radius</span> <span class="p">);</span>
</pre></div>
</div>
<p>For large classes also, <code class="docutils literal"><span class="pre">CV_EXPORTS_W</span></code> is used. To extend class methods, <code class="docutils literal"><span class="pre">CV_WRAP</span></code> is used. Similarly, <code class="docutils literal"><span class="pre">CV_PROP</span></code> is used for class fields.</p>
<div class="highlight-cpp"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">CV_EXPORTS_W</span> <span class="nl">CLAHE</span> <span class="p">:</span> <span class="k">public</span> <span class="n">Algorithm</span>
<span class="p">{</span>
<span class="k">public</span><span class="o">:</span>
    <span class="n">CV_WRAP</span> <span class="k">virtual</span> <span class="kt">void</span> <span class="n">apply</span><span class="p">(</span><span class="n">InputArray</span> <span class="n">src</span><span class="p">,</span> <span class="n">OutputArray</span> <span class="n">dst</span><span class="p">)</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>

    <span class="n">CV_WRAP</span> <span class="k">virtual</span> <span class="kt">void</span> <span class="nf">setClipLimit</span><span class="p">(</span><span class="kt">double</span> <span class="n">clipLimit</span><span class="p">)</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="n">CV_WRAP</span> <span class="k">virtual</span> <span class="kt">double</span> <span class="nf">getClipLimit</span><span class="p">()</span> <span class="k">const</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Overloaded functions can be extended using <code class="docutils literal"><span class="pre">CV_EXPORTS_AS</span></code>. But we need to pass a new name so that each function will be called by that name in Python. Take the case of integral function below. Three functions are available, so each one is named with a suffix in Python. Similarly <code class="docutils literal"><span class="pre">CV_WRAP_AS</span></code> can be used to wrap overloaded methods.</p>
<div class="highlight-cpp"><div class="highlight"><pre><span></span><span class="c1">//! computes the integral image</span>
<span class="n">CV_EXPORTS_W</span> <span class="kt">void</span> <span class="nf">integral</span><span class="p">(</span> <span class="n">InputArray</span> <span class="n">src</span><span class="p">,</span> <span class="n">OutputArray</span> <span class="n">sum</span><span class="p">,</span> <span class="kt">int</span> <span class="n">sdepth</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="p">);</span>

<span class="c1">//! computes the integral image and integral for the squared image</span>
<span class="n">CV_EXPORTS_AS</span><span class="p">(</span><span class="n">integral2</span><span class="p">)</span> <span class="kt">void</span> <span class="n">integral</span><span class="p">(</span> <span class="n">InputArray</span> <span class="n">src</span><span class="p">,</span> <span class="n">OutputArray</span> <span class="n">sum</span><span class="p">,</span>
                                        <span class="n">OutputArray</span> <span class="n">sqsum</span><span class="p">,</span> <span class="kt">int</span> <span class="n">sdepth</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="kt">int</span> <span class="n">sqdepth</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="p">);</span>

<span class="c1">//! computes the integral image, integral for the squared image and the tilted integral image</span>
<span class="n">CV_EXPORTS_AS</span><span class="p">(</span><span class="n">integral3</span><span class="p">)</span> <span class="kt">void</span> <span class="n">integral</span><span class="p">(</span> <span class="n">InputArray</span> <span class="n">src</span><span class="p">,</span> <span class="n">OutputArray</span> <span class="n">sum</span><span class="p">,</span>
                                        <span class="n">OutputArray</span> <span class="n">sqsum</span><span class="p">,</span> <span class="n">OutputArray</span> <span class="n">tilted</span><span class="p">,</span>
                                        <span class="kt">int</span> <span class="n">sdepth</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="kt">int</span> <span class="n">sqdepth</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="p">);</span>
</pre></div>
</div>
<p>Small classes/structs are extended using <code class="docutils literal"><span class="pre">CV_EXPORTS_W_SIMPLE</span></code>. These structs are passed by value to C++ functions. Examples are KeyPoint, Match etc. Their methods are extended by <code class="docutils literal"><span class="pre">CV_WRAP</span></code> and fields are extended by <code class="docutils literal"><span class="pre">CV_PROP_RW</span></code>.</p>
<div class="highlight-cpp"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">CV_EXPORTS_W_SIMPLE</span> <span class="n">DMatch</span>
<span class="p">{</span>
<span class="k">public</span><span class="o">:</span>
    <span class="n">CV_WRAP</span> <span class="n">DMatch</span><span class="p">();</span>
    <span class="n">CV_WRAP</span> <span class="nf">DMatch</span><span class="p">(</span><span class="kt">int</span> <span class="n">_queryIdx</span><span class="p">,</span> <span class="kt">int</span> <span class="n">_trainIdx</span><span class="p">,</span> <span class="kt">float</span> <span class="n">_distance</span><span class="p">);</span>
    <span class="n">CV_WRAP</span> <span class="nf">DMatch</span><span class="p">(</span><span class="kt">int</span> <span class="n">_queryIdx</span><span class="p">,</span> <span class="kt">int</span> <span class="n">_trainIdx</span><span class="p">,</span> <span class="kt">int</span> <span class="n">_imgIdx</span><span class="p">,</span> <span class="kt">float</span> <span class="n">_distance</span><span class="p">);</span>

    <span class="n">CV_PROP_RW</span> <span class="kt">int</span> <span class="n">queryIdx</span><span class="p">;</span> <span class="c1">// query descriptor index</span>
    <span class="n">CV_PROP_RW</span> <span class="kt">int</span> <span class="n">trainIdx</span><span class="p">;</span> <span class="c1">// train descriptor index</span>
    <span class="n">CV_PROP_RW</span> <span class="kt">int</span> <span class="n">imgIdx</span><span class="p">;</span>   <span class="c1">// train image index</span>

    <span class="n">CV_PROP_RW</span> <span class="kt">float</span> <span class="n">distance</span><span class="p">;</span>
<span class="p">};</span>
</pre></div>
</div>
<p>Some other small classes/structs can be exported using <code class="docutils literal"><span class="pre">CV_EXPORTS_W_MAP</span></code> where it is exported to a Python native dictionary. Moments() is an example of it.</p>
<div class="highlight-cpp"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">CV_EXPORTS_W_MAP</span> <span class="n">Moments</span>
<span class="p">{</span>
<span class="k">public</span><span class="o">:</span>
    <span class="c1">//! spatial moments</span>
    <span class="n">CV_PROP_RW</span> <span class="kt">double</span>  <span class="n">m00</span><span class="p">,</span> <span class="n">m10</span><span class="p">,</span> <span class="n">m01</span><span class="p">,</span> <span class="n">m20</span><span class="p">,</span> <span class="n">m11</span><span class="p">,</span> <span class="n">m02</span><span class="p">,</span> <span class="n">m30</span><span class="p">,</span> <span class="n">m21</span><span class="p">,</span> <span class="n">m12</span><span class="p">,</span> <span class="n">m03</span><span class="p">;</span>
    <span class="c1">//! central moments</span>
    <span class="n">CV_PROP_RW</span> <span class="kt">double</span>  <span class="n">mu20</span><span class="p">,</span> <span class="n">mu11</span><span class="p">,</span> <span class="n">mu02</span><span class="p">,</span> <span class="n">mu30</span><span class="p">,</span> <span class="n">mu21</span><span class="p">,</span> <span class="n">mu12</span><span class="p">,</span> <span class="n">mu03</span><span class="p">;</span>
    <span class="c1">//! central normalized moments</span>
    <span class="n">CV_PROP_RW</span> <span class="kt">double</span>  <span class="n">nu20</span><span class="p">,</span> <span class="n">nu11</span><span class="p">,</span> <span class="n">nu02</span><span class="p">,</span> <span class="n">nu30</span><span class="p">,</span> <span class="n">nu21</span><span class="p">,</span> <span class="n">nu12</span><span class="p">,</span> <span class="n">nu03</span><span class="p">;</span>
<span class="p">};</span>
</pre></div>
</div>
<p>So these are the major extension macros available in OpenCV. Typically, a developer has to put proper macros in their appropriate positions. Rest is done by generator scripts. Sometimes, there may be an exceptional cases where generator scripts cannot create the wrappers. Such functions need to be handled manually. But most of the time, a code written according to OpenCV coding guidelines will be automatically wrapped by generator scripts.</p>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="indices-and-tables">
<h1>Indices and tables<a class="headerlink" href="#indices-and-tables" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a></li>
<li><a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a></li>
<li><a class="reference internal" href="search.html"><span class="std std-ref">Search Page</span></a></li>
</ul>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2013, Alexander Mordvintsev &amp; Abid K.
      
        <span class="commit">
          Revision <code>43532856</code>.
        </span>
      

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      <span class="fa fa-book"> Read the Docs</span>
      v: latest
      <span class="fa fa-caret-down"></span>
    </span>
    <div class="rst-other-versions">
      <dl>
        <dt>Versions</dt>
        
          <dd><a href="/en/latest/">latest</a></dd>
        
          <dd><a href="/en/stable/">stable</a></dd>
        
      </dl>
      <dl>
        <dt>Downloads</dt>
        
          <dd><a href="//readthedocs.org/projects/opencv-python-tutroals/downloads/pdf/latest/">pdf</a></dd>
        
          <dd><a href="//readthedocs.org/projects/opencv-python-tutroals/downloads/htmlzip/latest/">htmlzip</a></dd>
        
          <dd><a href="//readthedocs.org/projects/opencv-python-tutroals/downloads/epub/latest/">epub</a></dd>
        
      </dl>
      <dl>
        <dt>On Read the Docs</dt>
          <dd>
            <a href="//readthedocs.org/projects/opencv-python-tutroals/?fromdocs=opencv-python-tutroals">Project Home</a>
          </dd>
          <dd>
            <a href="//readthedocs.org/builds/opencv-python-tutroals/?fromdocs=opencv-python-tutroals">Builds</a>
          </dd>
      </dl>
      <hr/>
      Free document hosting provided by <a href="http://www.readthedocs.org">Read the Docs</a>.

    </div>
  </div>



  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'1',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>

  

  
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>